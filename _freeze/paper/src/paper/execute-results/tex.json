{
  "hash": "739571d35469575db33f022f6756a955",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Effortless Bayesian Deep Learning in Julia through Laplace\nabstract: |\n  Treating deep neural networks probabilistically comes with numerous advantages including improved robustness and greater interpretability. These factors are key to building Artificial Intelligence (AI) that is trustworthy. A drawback commonly associated with existing Bayesian methods is that they increase computational costs. Recent work has shown that Bayesian deep learning can be effortless through Laplace approximation. We propose a light-weight Julia package, `LaplaceRedux.jl` that implements this new approach for deep neural networks trained in `Flux.jl`.\nkeywords:\n  - Julia\n  - Probabilistic Machine Learning\n  - Laplace Approximation\n  - Deep Learning\n  - Artificial Intelligence\nauthors:\n  - name: Patrick Altmeyer\n    orcid: 0000-0003-4726-8613\n    affiliation:\n      - name: Delft University of Technology\ndate: 17 November 2023\nbibliography: ref.bib\nformat:\n  juliacon-proceedings-pdf:\n    keep-tex: true\nexecute:\n  eval: false\n  echo: false\n---\n\n\n\n# Background {#sec-intro}\n\nOver the past decade, Deep Learning (DL) has arguably been one of the dominating subdisciplines of Artificial Intelligence. Despite the tremendous success of deep neural networks, practitioners and researchers have also pointed to a vast number of pitfalls that have so far inhibited the use of DL in safety-critical applications. Among other things, these pitfalls include a lack of adversarial robustness \\cite{goodfellow2014explaining} and an inherent opaqueness of deep neural networks, often described as the black-box problem. \n\nIn deep learning, the number of parameters relative to the size of the available data is generally huge \\cite{wilson2020case}: \"[...] deep neural networks are typically very underspecified by the available data, and [...] parameters [therefore] correspond to a diverse variety of compelling explanations for the data.\" A scenario like this very much calls for treating model predictions probabilistically \\cite{wilson2020case}, to capture the inherent uncertainty in the model's predictions. This has been shown to improve robustness \\cite{daxberger2021laplace}, inherent interpretability \\cite{ish-horowicz2019interpreting} and post-hoc explainability \\cite{schut2021generating,altmeyer2023faithful}. \n\nIt is therefore not surprising that interest in Bayesian deep learning has grown in recent years as researchers have tackled the problem from a wide range of angles including MCMC (see [Turing.jl](https://turing.ml/dev/tutorials/03-bayesian-neural-network/)), Mean Field Variational Inference \\cite{blundell2015weight}, Monte Carlo Dropout \\cite{gal2016dropout} and Deep Ensembles \\cite{lakshminarayanan2016simple}. Laplace Redux \\cite{immer2020improving,daxberger2021laplace} is one of the most recent and promising approaches to Bayesian neural networks (BNN). \n\n# Laplace Approximation for Deep Learning {#sec-body}\n\nLet $\\mathcal{D}=\\{x,y\\}_{n=1}^N$ denote our feature-label pairs and let $f(x;\\theta)=y$ denote some deep neural network specified by its parameters $\\theta$. We are interested in estimating the posterior predictive distribution given by the following Bayesian model average (BMA):\n\n$$\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n$$ {#eq-bma}\n\nTo do so we first need to compute the weight posterior $p(\\theta|\\mathcal{D})$. Laplace approximation (LA) relies on the fact that the second-order Taylor expansion of this posterior amounts to a multivariate Gaussian $q(\\theta)=\\mathcal{N}(\\hat\\mu,\\hat\\Sigma)$ centred around the maximum a posteriori (MAP) estimate $\\hat\\mu=\\hat{\\theta}=\\arg\\max_{\\theta}p(\\theta|\\mathcal{D})$ with covariance equal to the negative inverse Hessian of our loss function evaluated at the mode $\\hat{\\Sigma}=-(\\hat{\\mathcal{H}}|_{\\hat{\\theta}})^{-1}$. \n\nTo apply LA in the context of deep learning, we can train our network in the standard way by minimizing the negative log-likelihood $\\ell(\\theta)=-\\log p(y|x,\\mathcal{D})$. To obtain the Gaussian LA weight posterior we then only need to compute the Hessian evaluated at the obtained MAP estimate. \n\nLaplace approximation dates back to the 18th century but despite its simplicity, it has not been widely adopted or studied by the deep learning community until recently. One reason for this may be that for large neural networks with many parameters, the exact Hessian computation is prohibitive. One can rely on linearized approximations of the Hessian but those still scale quadratically in the number of parameters. Recent work has shown that block-diagonal factorizations can help in this context \\cite{martens2015optimizing}. \n\nAnother more compelling reason why LA had been mostly neglected until recently is that early attempts at using it for deep learning failed: simply sampling from the Laplace posterior to compute the exact Bayesian neural network (BNN) posterior predictive distribution in @eq-bma does not work when using approximations for the Hessian \\cite{lawrence2001variational}. Instead, we can use a linear expansion of the predictive around the mode as demonstrated by Immer et al. (2020) \\cite{immer2020improving}. Formally, we locally linearize our network,\n\n$$\nf^{\\hat{\\theta}}_{\\mbox{lin}}(x;\\theta)=f(x;\\hat{\\theta}) + \\mathcal{J}_{\\hat{\\theta}}(\\theta-\\hat{\\theta})\n$$ {#eq-glm}\n \n\nwhich turns the BNN into a Bayesian generalized linear model (GLM) where $\\hat{\\theta}$ corresponds to the MAP estimate as before and $\\mathcal{J}_{\\hat{\\theta}}$ is the network *Jacobian* \\cite{immer2020improving} evaluated at the mode. The corresponding GLM predictive, \n\n$$\np(y|x,\\mathcal{D}) = \\mathbb{E} \\left[ p(y|f^{\\hat{\\theta}}_{\\mbox{lin}}(x;\\theta_n)) \\right], \\ \\ \\ \\theta_n \\sim q(\\theta)\n$$ {#eq-glm-predictive}\n\nhas a closed-form solution for regression problems. For classification problems it can be approximated using (extended) probit approximation \\cite{daxberger2021laplace}. \n\nImmer et al. (2020) \\cite{immer2020improving} provide a much more detailed exposition of the above with a focus on theoretical underpinnings and intuition. Daxberger et al. (2021) \\cite{daxberger2021laplace} introduce Laplace Redux from more of an applied perspective and present a comprehensive Python implementation: [laplace](https://aleximmer.github.io/Laplace/).\n\n# `LaplaceRedux.jl` --- a Julia implementation\n\nThe `LaplaceRedux.jl` package makes this new methodological framework available to the Julia community. It is interfaced with the popular deep learning library, [`Flux.jl`](https://fluxml.ai/). \n\n\n\nUsing just a few lines of code the package enables users to compute and apply LA to their pre-trained neural networks. A basic usage example is shown in Code \\ref{lst:fit}: the `Laplace` constructor takes a pre-trained Flux neural network `nn` as its only positional argument. Since the underlying model is a classifier, we need to specify the likelihood accordingly using the corresponding keyword argument. Additional keyword arguments can be used to define the Bayesian priors and select the backend used to approximate the Hessian among other things. The returned instance is then fitted to the data using the generic `fit!` method. Note that the `fit!` method also accepts a `Flux.DataLoader` as its second positional argument and mini-batch training is supported.\n\n\n\n\\begin{lstlisting}[language=Julia, escapechar=@, numbers=left, label={lst:fit}, caption={Fitting a pre-trained neural network to data using Laplace Redux.}]\nla = Laplace(nn; likelihood=:classification)\nfit!(la, data)\n\\end{lstlisting}\n\nThe `la` object is a mutable and callable struct that wraps the pre-trained neural networks along with hyperparameters relevant to the Laplace approximation. Simply calling the instance with new data as in Code \\ref{lst:predict} will generate GLM predictions according to @eq-glm-predictive. In the classification case, softmax outputs are returned by default following the convention in the Python implementation, but this can be changed using the `predict_proba` keyword argument. It is also possible to generate conventional plug-in predictions using the original MAP estimate by setting the `link_approx` keyword argument to `:plugin`. \n\n\\begin{lstlisting}[language=Julia, escapechar=@, numbers=left, label={lst:predict}, caption={Predictions using the fitted Laplace Redux instance.}]\nla(X)                       # GLM predictions\nla(X; predict_proba=false)  # no softmax\nla(X; link_approx=:plugin)  # MAP predictions\n\\end{lstlisting}\n\nAdditional methods can be used to optimize the prior precision $\\lambda$ and to visualize the predictions (Code \\ref{lst:other}). The `optimize_prior!` method optimizes the prior precision $\\lambda$ through Empirical Bayes \\cite{daxberger2021laplace}. The `plot` method visualizes the predictions of the fitted instance using [Plots.jl](https://docs.juliaplots.org/stable/). The method extension is provided through the [TaijaPlotting.jl](https://github.com/JuliaTrustworthyAI/TaijaPlotting.jl) meta package.\n\n\\begin{lstlisting}[language=Julia, escapechar=@, numbers=left, label={lst:other}, caption={Prior optimization and visualization of the predictive distribution.}]\noptimize_prior!(la)         # optimize Î»\nusing TaijaPlotting\nplot(la, X, y)              # plot predictions\n\\end{lstlisting}\n\n\n\n@fig-class shows an example involving a synthetic data set consisting of two classes. Contours indicate the predicted probabilities using the plugin estimator (left), untuned Laplace approximation (center) and finally optimized LA (right). For the latter two, the respective choices for the prior precision parameter $\\lambda$ are indicated in the title. Relying solely on the MAP estimate, the plugin estimator produces overly confident predictions. Conversely, the GLM predictions account for predictive uncertainty as captured by the Laplace posterior.\n\n@fig-reg presents a regression example with optimized LA. Wide regions of the confidence interval (shaded area) indicate high predictive uncertainty. Intuitively, the estimated predictive uncertainty increases significantly in regions characterized by high epistemic uncertainty: epistemic uncertainty arises in regions of the domain that have not been observed by the classifier, so regions that are free of training samples. \n\n\n\n![Posterior predictive distribution for binary classifier: plugin estimate (left), untuned LA (center) and optimized LA (right). The colour of the contour indicates the predicted class probabilities: the more yellow a region, the more confident the classifier that samples belong to the orange class.](www/posterior_predictive_mlp.png){#fig-class width=\"20pc\" height=\"6.7pc\"}\n\n![Posterior predictive distribution for regressor: wide regions of the confidence interval (shaded area) indicate high predictive uncertainty.](www/regression.png){#fig-reg width=\"20pc\" height=\"10pc\"}\n\n# Scaling Up {#sec-scale}\n\nAs mentioned in @sec-body, Laplace Redux hinges on linear approximations of the Hessian, which scale quadratically in the number of network parameters \\cite{daxberger2021laplace}. Our package currently supports two broad approaches to address this issue: the first approach is to compute LA over a subnetwork with explicit control over the number of parameters; the second approach is to use more scalable approximations of the Hessians. For the second approach, the package currently offers support for Kronecker-factored approximate curvature (KFAC) estimation \\cite{martens2020optimizing}. A third approach is to use sample-based linearised Laplace \\cite{antoran2023samplingbased}, which is not yet supported.\n\n# Discussion and Outlook {#sec-con}\n\nLaplace Redux is an exciting and promising recent development in Bayesian deep learning. The package `LaplaceRedux.jl` brings this framework to the Julia ecosystem. Future developments are planned and contributions are very much welcome. At the time of writing, we are particularly interested in streamlining the package's interface to the larger [Taija](https://github.com/JuliaTrustworthyAI) ecosystem and improving our support for scalable LA.\n\n# Acknowledgements {#sec-ack}\n\nI am grateful to my PhD supervisors Cynthia C. S. Liem and Arie van Deursen for being so supportive of my work on open-source developments. Furthermore, I would like to thank the group of students who contributed to this package through a course project: Mark Ardman, Severin Bratus, Adelina Cazacu, Andrei Ionescu and Ivan Makarov.\n\n",
    "supporting": [
      "paper_files/figure-pdf"
    ],
    "filters": []
  }
}
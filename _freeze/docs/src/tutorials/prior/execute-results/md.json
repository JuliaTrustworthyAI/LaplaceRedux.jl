{
  "hash": "0847f90ab1207be76da5d4abba8a93fb",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n## Libraries\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux, LinearAlgebra\n```\n:::\n\n\n| !!! note \\\"In Progress\\\"\n|     This documentation is still incomplete. \n\n## A quick note on the prior \n\n### General Effect\n\nHigh prior precision $\\rightarrow$ only observation noise. Low prior precision $\\rightarrow$ high posterior uncertainty.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 150       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 10\nD = size(X,1)\nΛ = [1e5, nothing, 1e-5]\nplts = []\nnns = []\nopt=Flux.Adam(1e-3)\nfor λ ∈ Λ\n    nn = Chain(\n        Dense(D, n_hidden, tanh),\n        Dense(n_hidden, 1)\n    )  \n    loss(x, y) = Flux.Losses.mse(nn(x), y)\n    # train\n    epochs = 1000\n    for epoch = 1:epochs\n        for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        Flux.update!(opt, Flux.params(nn), gs)\n        end\n    end\n    # laplace\n    if !isnothing(λ)\n        la = Laplace(nn; likelihood=:regression, λ=λ)\n        fit!(la, data)  \n    else\n        la = Laplace(nn; likelihood=:regression)\n        fit!(la, data)  \n        optimize_prior!(la)\n    end\n    \n    _suffix = isnothing(λ) ? \" (optimal)\" : \"\"\n    λ = unique(diag(la.prior.P₀))[1]\n    title = \"λ=$(round(λ,digits=2))$(_suffix)\"\n\n    # plot \n    plt = plot(la, X, y; title=title, zoom=-5)\n    plts = vcat(plts..., plt)\n    nns = vcat(nns..., nn)\nend\nplot(plts..., layout=(1,3), size=(1200,300))\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](prior_files/figure-commonmark/cell-4-output-1.svg){}\n:::\n:::\n\n\n### Effect of Model Size on Optimal Choice\n\nFor larger models, the optimal prior precision $\\lambda$ as evaluated through Empirical Bayes tends to be smaller.\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hiddens = [5, 10, 50]\nD = size(X,1)\nplts = []\nnns = []\nopt=Flux.Adam(1e-3)\nfor n_hidden ∈ n_hiddens\n    nn = Chain(\n        Dense(D, n_hidden, tanh),\n        Dense(n_hidden, 1)\n    )  \n    loss(x, y) = Flux.Losses.mse(nn(x), y)\n    # train\n    epochs = 1000\n    for epoch = 1:epochs\n        for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        Flux.update!(opt, Flux.params(nn), gs)\n        end\n    end\n    # laplace\n    la = Laplace(nn; likelihood=:regression)\n    fit!(la, data)  \n    optimize_prior!(la)\n    \n    λ = unique(diag(la.prior.P₀))[1]\n    title = \"n_params=$(LaplaceRedux.n_params(la)),λ=$(round(λ,digits=2))\"\n\n    # plot \n    plt = plot(la, X, y; title=title, zoom=-5)\n    plts = vcat(plts..., plt)\n    nns = vcat(nns..., nn)\nend\nplot(plts..., layout=(1,3), size=(1200,300))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](prior_files/figure-commonmark/cell-5-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\n# Number of points to generate.\nxs, ys = LaplaceRedux.Data.toy_data_non_linear(200)\nX = hcat(xs...) # bring into tabular format\ndata = zip(xs,ys)\n\nn_hiddens = [5, 10, 50]\nD = size(X,1)\nplts = []\nnns = []\nopt=Flux.Adam(1e-3)\nfor n_hidden ∈ n_hiddens\n    nn = Chain(\n        Dense(D, n_hidden, σ),\n        Dense(n_hidden, 1)\n    )  \n    loss(x, y) = Flux.Losses.mse(nn(x), y)\n    # train\n    epochs = 100\n    for epoch = 1:epochs\n        for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        Flux.update!(opt, Flux.params(nn), gs)\n        end\n    end\n    # laplace\n    la = Laplace(nn; likelihood=:classification)\n    fit!(la, data)  \n    optimize_prior!(la)\n    \n    λ = unique(diag(la.prior.P₀))[1]\n    title = \"n_params=$(LaplaceRedux.n_params(la)),λ=$(round(λ,digits=2))\"\n\n    # plot \n    plt = plot(la, X, ys; title=title, zoom=-1, clim=(0,1))\n    plts = vcat(plts..., plt)\n    nns = vcat(nns..., nn)\nend\nplot(plts..., layout=(1,3), size=(1200,300))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](prior_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "prior_files\\figure-commonmark"
    ],
    "filters": []
  }
}
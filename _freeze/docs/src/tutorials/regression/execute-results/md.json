{
  "hash": "c88bae090b3f2a5aef13489a868e344b",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n## Libraries\n\nImport the libraries required to run this example\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:wong)\n```\n:::\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 300       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](regression_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 104.78561546028183\nLog likelihood: -70.48742092717352\nLog det ratio: 41.1390695290454\nScatter: 27.45731953717124\nloss(exp.(logP₀), exp.(logσ)) = 104.9736282327825\nLog likelihood: -74.85481357633174\nLog det ratio: 46.59827618892447\nScatter: 13.639353123977058\nloss(exp.(logP₀), exp.(logσ)) = 84.38222356291794\nLog likelihood: -54.86985627702764\nLog det ratio: 49.92347667032635\nScatter: 9.101257901454279\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 84.53493863039972\nLog likelihood: -55.013137224636\nLog det ratio: 51.43622180356522\nScatter: 7.607381007962245\nloss(exp.(logP₀), exp.(logσ)) = 83.95921598606084\nLog likelihood: -54.41492266831395\nLog det ratio: 51.794520967146354\nScatter: 7.294065668347427\nloss(exp.(logP₀), exp.(logσ)) = 83.03505059021086\nLog likelihood: -53.50540374805591\nLog det ratio: 51.574749787874794\nScatter: 7.484543896435117\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 82.97840036025443\nLog likelihood: -53.468475394115416\nLog det ratio: 51.17273666609066\nScatter: 7.847113266187348\nloss(exp.(logP₀), exp.(logσ)) = 82.98550025321256\nLog likelihood: -53.48508828283467\nLog det ratio: 50.81442045868749\nScatter: 8.186403482068298\nloss(exp.(logP₀), exp.(logσ)) = 82.9584040552644\nLog likelihood: -53.45989630330948\nLog det ratio: 50.59063282947659\nScatter: 8.406382674433235\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nloss(exp.(logP₀), exp.(logσ)) = 82.94465052328141\nLog likelihood: -53.44600301956443\nLog det ratio: 50.500079294094405\nScatter: 8.497215713339543\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-5.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files\\figure-commonmark"
    ],
    "filters": []
  }
}
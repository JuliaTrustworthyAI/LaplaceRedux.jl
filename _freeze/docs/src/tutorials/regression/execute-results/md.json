{
  "hash": "98c1ab1e86df8b4609b352140d0361d5",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 300       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](regression_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 124.30892828437838\nLog likelihood: -92.90051444935894\nLog det ratio: 40.658383861905975\nScatter: 22.15844380813291\nloss(exp.(logP₀), exp.(logσ)) = 130.81658756749727\nLog likelihood: -102.38911604652068\nLog det ratio: 45.69949273706189\nScatter: 11.155450304891295\nloss(exp.(logP₀), exp.(logσ)) = 117.38076793615893\nLog likelihood: -89.25592255384686\nLog det ratio: 48.59462871227015\nScatter: 7.655062052354006\nloss(exp.(logP₀), exp.(logσ)) = 114.88202732095019\nLog likelihood: -86.70548060362589\nLog det ratio: 49.71436271446572\nScatter: 6.638730720182862\nloss(exp.(logP₀), exp.(logσ)) = 115.26415029719183\nLog likelihood: -87.08424763856297\nLog det ratio: 49.75555914247877\nScatter: 6.60424617477896\nloss(exp.(logP₀), exp.(logσ)) = 114.61842761360316\nLog likelihood: -86.46860147471742\nLog det ratio: 49.32651563215006\nScatter: 6.973136645621423\nloss(exp.(logP₀), exp.(logσ)) = 114.39008804031867\nLog likelihood: -86.26082328115449\nLog det ratio: 48.8367441519502\nScatter: 7.421785366378184\nloss(exp.(logP₀), exp.(logσ)) = 114.38876489620353\nLog likelihood: -86.26458532181827\nLog det ratio: 48.49569624521385\nScatter: 7.752662903556662\nloss(exp.(logP₀), exp.(logσ)) = 114.3921409222427\nLog likelihood: -86.26770107709343\nLog det ratio: 48.349001843084864\nScatter: 7.899877847213686\nloss(exp.(logP₀), exp.(logσ)) = 114.38668455910438\nLog likelihood: -86.26219437624869\nLog det ratio: 48.34197553950503\nScatter: 7.907004826206364\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-2.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files"
    ],
    "filters": []
  }
}
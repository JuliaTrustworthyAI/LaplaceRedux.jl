{
  "hash": "53aecba375c27439bafa9ae3a81f9bf6",
  "result": {
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 150       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](regression_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 10: P₀=0.3895817899073436, σ=0.3743155211807906\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 57.82862051462456\nIteration 20: P₀=0.19740377732969688, σ=0.22232904647966711\nloss(exp.(logP₀), exp.(logσ)) = 56.9828631315647\nIteration 30: P₀=0.13730732318486746, σ=0.30407118191989013\nloss(exp.(logP₀), exp.(logσ)) = 46.453150536156386\nIteration 40: P₀=0.12115811067346993, σ=0.3080945046886427\nloss(exp.(logP₀), exp.(logσ)) = 46.770263799396396\nIteration 50: P₀=0.1225243267122822, σ=0.26950400534294194\nloss(exp.(logP₀), exp.(logσ)) = 46.367032506875915\nIteration 60: P₀=0.1308469880921181, σ=0.2884000501866077\nloss(exp.(logP₀), exp.(logσ)) = 45.87856486201052\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 70: P₀=0.13983814051116947, σ=0.28798753052555986\nloss(exp.(logP₀), exp.(logσ)) = 45.85626199765708\nIteration 80: P₀=0.14573256667723103, σ=0.28137446446795905\nloss(exp.(logP₀), exp.(logσ)) = 45.85903167427847\nIteration 90: P₀=0.1477073935557212, σ=0.2869757911722043\nloss(exp.(logP₀), exp.(logσ)) = 45.844604952740006\nIteration 100: P₀=0.1471549041911406, σ=0.2840891068076814\nloss(exp.(logP₀), exp.(logσ)) = 45.83742360639423\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-4.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files"
    ],
    "filters": []
  }
}
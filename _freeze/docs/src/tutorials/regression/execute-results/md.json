{
  "hash": "53aecba375c27439bafa9ae3a81f9bf6",
  "result": {
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 150       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](regression_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 10: P₀=0.3884183915097719, σ=0.37462791983413435\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 62.286501135197625\nIteration 20: P₀=0.19286897058427785, σ=0.22662547463745616\nloss(exp.(logP₀), exp.(logσ)) = 60.95395063398549\nIteration 30: P₀=0.12850167531313283, σ=0.31089662895570863\nloss(exp.(logP₀), exp.(logσ)) = 50.466530344502566\nIteration 40: P₀=0.10713367763315505, σ=0.3117173325625265\nloss(exp.(logP₀), exp.(logσ)) = 50.52282762877773\nIteration 50: P₀=0.10242099028762099, σ=0.27382604745869443\nloss(exp.(logP₀), exp.(logσ)) = 50.25213793017164\nIteration 60: P₀=0.10482842243715489, σ=0.29384717721411097\nloss(exp.(logP₀), exp.(logσ)) = 49.78683248122465\nIteration 70: P₀=0.10972567294088902, σ=0.2922018292211539\nloss(exp.(logP₀), exp.(logσ)) = 49.75103929588618\nIteration 80: P₀=0.11440323090214488, σ=0.2861879031236091\nloss(exp.(logP₀), exp.(logσ)) = 49.75147574978594\nIteration 90: P₀=0.11750906238748272, σ=0.2917479215594753\nloss(exp.(logP₀), exp.(logσ)) = 49.73732964241694\nIteration 100: P₀=0.1188587829052049, σ=0.28865384391024296\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nloss(exp.(logP₀), exp.(logσ)) = 49.730466973202795\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-4.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files"
    ],
    "filters": []
  }
}
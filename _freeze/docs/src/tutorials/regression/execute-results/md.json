{
  "hash": "79cacba9fa0817151af46c5aa05860ad",
  "result": {
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 300       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](regression_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 117.66664854620934\nLog likelihood: -85.92178189333164\nLog det ratio: 35.71426712713756\nScatter: 27.77546617861784\nloss(exp.(logP₀), exp.(logσ)) = 121.87231511858798\nLog likelihood: -94.6377872523882\nLog det ratio: 40.7731858243348\nScatter: 13.695869908064768\nloss(exp.(logP₀), exp.(logσ)) = 105.74544738246172\nLog likelihood: -79.26976568466756\nLog det ratio: 43.950740004518195\nScatter: 9.000623391070134\nloss(exp.(logP₀), exp.(logσ)) = 103.88142914896216\nLog likelihood: -77.44149990175734\nLog det ratio: 45.50530524149423\nScatter: 7.374553252915389\nloss(exp.(logP₀), exp.(logσ)) = 104.09479293926637\nLog likelihood: -77.63140243238486\nLog det ratio: 45.99843510214248\nScatter: 6.928345911620529\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 103.3468498181464\nLog likelihood: -76.88770063543016\nLog det ratio: 45.92861066847996\nScatter: 6.989687696952491\nloss(exp.(logP₀), exp.(logσ)) = 103.16246637027459\nLog likelihood: -76.71757535070529\nLog det ratio: 45.64189954216795\nScatter: 7.247882496970637\nloss(exp.(logP₀), exp.(logσ)) = 103.16978958495622\nLog likelihood: -76.7341717145966\nLog det ratio: 45.34308830278303\nScatter: 7.528147437936199\nloss(exp.(logP₀), exp.(logσ)) = 103.16445388078068\nLog likelihood: -76.73183970804324\nLog det ratio: 45.1273375561218\nScatter: 7.737890789353098\nloss(exp.(logP₀), exp.(logσ)) = 103.15526534959244\nLog likelihood: -76.72294852715255\nLog det ratio: 45.01394675305761\nScatter: 7.850686891822177\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-3.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files"
    ],
    "filters": []
  }
}
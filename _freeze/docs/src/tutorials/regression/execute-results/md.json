{
  "hash": "b564be7edfe0ecc539a995c2e4ff5c7c",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n## Libraries\n\nImport the libraries required to run this example\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:wong)\n```\n:::\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 3000       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\nand split them in a training set  and a test set\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# Shuffle the data\nRandom.seed!(1234)  # Set a seed for reproducibility\nshuffle_indices = shuffle(1:n)\n\n# Define split ratios\ntrain_ratio = 0.8\ntest_ratio = 0.2\n\n# Calculate split indices\ntrain_end = Int(floor(train_ratio * n))\n\n# Split the data\ntrain_indices = shuffle_indices[1:train_end]\ntest_indices = shuffle_indices[train_end+1:end]\n\n# Create the splits\nx_train, y_train = x[train_indices], y[train_indices]\nx_test, y_test = x[test_indices], y[test_indices]\n\n# Optional: Convert to desired format\nxs_train = [[x] for x in x_train]\nxs_test = [[x] for x in x_test]\nX_train = permutedims(x_train)\nX_test = permutedims(x_test)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\ntrain_data = zip(xs_train,y_train)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(train_data) = mean(map(d -> loss(d[1],d[2]), train_data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in train_data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(train_data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, train_data)\nplot(la, X_train, y_train; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X_train, y_train; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 699.4614071578686\nLog likelihood: -650.5288616722523\nLog det ratio: 66.18989615253814\nScatter: 31.675194818694376\nloss(exp.(logP₀), exp.(logσ)) = 758.1137287674911\nLog likelihood: -713.3275341047802\nLog det ratio: 73.58411353906541\nScatter: 15.98827578635654\nloss(exp.(logP₀), exp.(logσ)) = 623.1232220351936\nLog likelihood: -578.7313039580287\nLog det ratio: 77.76098439481223\nScatter: 11.022851759517634\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 612.9000913546312\nLog likelihood: -568.4300289112797\nLog det ratio: 79.33032526731489\nScatter: 9.609799619388047\nloss(exp.(logP₀), exp.(logσ)) = 612.980624661215\nLog likelihood: -568.5099348866323\nLog det ratio: 79.33795872667935\nScatter: 9.603420822486008\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 606.4185627905605\nLog likelihood: -561.9934406760827\nLog det ratio: 78.67976558586915\nScatter: 10.17047864308627\nloss(exp.(logP₀), exp.(logσ)) = 605.2983263524043\nLog likelihood: -560.9025071929581\nLog det ratio: 77.9541387818727\nScatter: 10.83749953701965\nloss(exp.(logP₀), exp.(logσ)) = 605.464398184884\nLog likelihood: -561.0751506853986\nLog det ratio: 77.46246992569823\nScatter: 11.316025073272467\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nloss(exp.(logP₀), exp.(logσ)) = 605.4020214804258\nLog likelihood: -561.0122284359471\nLog det ratio: 77.26250383495702\nScatter: 11.517082254000224\nloss(exp.(logP₀), exp.(logσ)) = 605.3141710440409\nLog likelihood: -560.924396603449\nLog det ratio: 77.2650076170082\nScatter: 11.514541264175579\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](regression_files/figure-commonmark/cell-8-output-5.svg){}\n:::\n:::\n\n\n## Calibration Plot\nOnce the prior precision has been optimized it is possible to evaluate the quality of the predictive distribution \nobtained through a calibration plot and a test dataset (y_test, X_test). \n\nFirst, we apply the trained network on the test dataset (y_test, X_test) and collect the neural network's predicted distributions \n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\npredicted_distributions= predict(la, X_test,ret_distr=true)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n600×1 Matrix{Distributions.Normal{Float64}}:\n Distributions.Normal{Float64}(μ=-0.4196469187736511, σ=0.0679738666764263)\n Distributions.Normal{Float64}(μ=-0.39177197217941284, σ=0.06890169312970602)\n Distributions.Normal{Float64}(μ=0.776796281337738, σ=0.06074957252594663)\n Distributions.Normal{Float64}(μ=0.6933168172836304, σ=0.08344435466780176)\n Distributions.Normal{Float64}(μ=0.832196831703186, σ=0.08084460912585997)\n Distributions.Normal{Float64}(μ=-0.050286442041397095, σ=0.06884885797502903)\n Distributions.Normal{Float64}(μ=-0.5049006938934326, σ=0.06568541695736727)\n Distributions.Normal{Float64}(μ=-0.06951528787612915, σ=0.05514742556492769)\n Distributions.Normal{Float64}(μ=0.7513407468795776, σ=0.05183199458344559)\n Distributions.Normal{Float64}(μ=-0.3874087929725647, σ=0.06904606151969317)\n Distributions.Normal{Float64}(μ=-0.9113342761993408, σ=0.06564413451854467)\n Distributions.Normal{Float64}(μ=-0.24987715482711792, σ=0.05616188450864741)\n Distributions.Normal{Float64}(μ=0.9503012299537659, σ=0.05509250127866398)\n ⋮\n Distributions.Normal{Float64}(μ=-0.5406216979026794, σ=0.055769675132711756)\n Distributions.Normal{Float64}(μ=0.31152284145355225, σ=0.05454183352368205)\n Distributions.Normal{Float64}(μ=1.061593770980835, σ=0.07329565553777241)\n Distributions.Normal{Float64}(μ=0.12588687241077423, σ=0.06686624669227054)\n Distributions.Normal{Float64}(μ=0.058063820004463196, σ=0.11908083025214214)\n Distributions.Normal{Float64}(μ=1.028564691543579, σ=0.06796776553390338)\n Distributions.Normal{Float64}(μ=0.6847273111343384, σ=0.06322761308748256)\n Distributions.Normal{Float64}(μ=1.0285106897354126, σ=0.06796223500400367)\n Distributions.Normal{Float64}(μ=0.05158938467502594, σ=0.12225229033768022)\n Distributions.Normal{Float64}(μ=-0.06038850545883179, σ=0.055088019072815825)\n Distributions.Normal{Float64}(μ=0.8001070022583008, σ=0.06048200018805775)\n Distributions.Normal{Float64}(μ=0.5859723091125488, σ=0.0863628028315014)\n```\n:::\n:::\n\n\nthen we can plot the calibration plot of our neural model\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nCalibration_Plot(la,y_test,vec(predicted_distributions);n_bins = 20)\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_files/figure-commonmark/cell-10-output-1.svg){}\n:::\n:::\n\n\nand compute the sharpness of the predictive distribution\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nsharpness_regression(vec(predicted_distributions))\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.004975154105351387\n```\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files\\figure-commonmark"
    ],
    "filters": []
  }
}
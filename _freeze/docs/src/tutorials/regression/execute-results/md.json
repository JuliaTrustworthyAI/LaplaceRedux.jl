{
  "hash": "79cacba9fa0817151af46c5aa05860ad",
  "result": {
    "markdown": "---\ntitle: Data\n---\n\n\n```@meta\nCurrentModule = LaplaceRedux\n```\n\n\n\n\nWe first generate some synthetic data:\n\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 300       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](regression_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 115.75735421748533\nLog likelihood: -79.2200866848734\nLog det ratio: 41.447703478591464\nScatter: 31.626831586632402\nloss(exp.(logP₀), exp.(logσ)) = 117.86642880183206\nLog likelihood: -86.56623967031658\nLog det ratio: 47.038810369756135\nScatter: 15.561567893274834\nloss(exp.(logP₀), exp.(logσ)) = 99.47631633578109\nLog likelihood: -69.09330443512576\nLog det ratio: 50.58494846478561\nScatter: 10.18107533652504\nloss(exp.(logP₀), exp.(logσ)) = 98.39016022405285\nLog likelihood: -68.06679631670653\nLog det ratio: 52.35553006863444\nScatter: 8.291197746058197\nloss(exp.(logP₀), exp.(logσ)) = 98.32796211733199\nLog likelihood: -67.97958599228153\nLog det ratio: 52.957723488965314\nScatter: 7.739028761135608\nloss(exp.(logP₀), exp.(logσ)) = 97.4846502485071\nLog likelihood: -67.13783329716615\nLog det ratio: 52.93077798129934\nScatter: 7.7628559213825294\nloss(exp.(logP₀), exp.(logσ)) = 97.35181187147037\nLog likelihood: -67.01886503956803\nLog det ratio: 52.64896810867424\nScatter: 8.016925555130463\nloss(exp.(logP₀), exp.(logσ)) = 97.36438554054067\nLog likelihood: -67.04148717903837\nLog det ratio: 52.335624298852\nScatter: 8.310172424152613\nloss(exp.(logP₀), exp.(logσ)) = 97.349858431591\nLog likelihood: -67.03065735188642\nLog det ratio: 52.098394400933444\nScatter: 8.540007758475733\nloss(exp.(logP₀), exp.(logσ)) = 97.33749417115928\nLog likelihood: -67.01884461669445\nLog det ratio: 51.96485166001452\nScatter: 8.67244744891513\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-2.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files"
    ],
    "filters": []
  }
}
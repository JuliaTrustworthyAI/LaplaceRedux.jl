{
  "hash": "b564be7edfe0ecc539a995c2e4ff5c7c",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n## Libraries\n\nImport the libraries required to run this example\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:wong)\n```\n:::\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 3000       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\nand split them in a training set  and a test set\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# Shuffle the data\nRandom.seed!(1234)  # Set a seed for reproducibility\nshuffle_indices = shuffle(1:n)\n\n# Define split ratios\ntrain_ratio = 0.8\ntest_ratio = 0.2\n\n# Calculate split indices\ntrain_end = Int(floor(train_ratio * n))\n\n# Split the data\ntrain_indices = shuffle_indices[1:train_end]\ntest_indices = shuffle_indices[train_end+1:end]\n\n# Create the splits\nx_train, y_train = x[train_indices], y[train_indices]\nx_test, y_test = x[test_indices], y[test_indices]\n\n# Optional: Convert to desired format\nxs_train = [[x] for x in x_train]\nxs_test = [[x] for x in x_test]\nX_train = permutedims(x_train)\nX_test = permutedims(x_test)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\ntrain_data = zip(xs_train,y_train)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(train_data) = mean(map(d -> loss(d[1],d[2]), train_data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in train_data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(train_data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, train_data)\nplot(la, X_train, y_train; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X_train, y_train; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 702.3984988592939\nLog likelihood: -655.4226779297866\nLog det ratio: 63.636037890555485\nScatter: 30.315603968459115\nloss(exp.(logP₀), exp.(logσ)) = 762.3739203559888\nLog likelihood: -719.2893446531043\nLog det ratio: 70.83089414166076\nScatter: 15.338257264108307\nloss(exp.(logP₀), exp.(logσ)) = 628.966608466347\nLog likelihood: -586.2236101292576\nLog det ratio: 74.86014124914948\nScatter: 10.625855425029199\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nloss(exp.(logP₀), exp.(logσ)) = 618.1490551810743\nLog likelihood: -575.3262768621464\nLog det ratio: 76.32699108193992\nScatter: 9.318565555915965\nloss(exp.(logP₀), exp.(logσ)) = 618.4368182243323\nLog likelihood: -575.6185033852381\nLog det ratio: 76.27376400937706\nScatter: 9.362865668811326\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 611.9419028847238\nLog likelihood: -569.1706791963551\nLog det ratio: 75.59010798146141\nScatter: 9.952339395276015\nloss(exp.(logP₀), exp.(logσ)) = 610.7840744675561\nLog likelihood: -568.0409279734597\nLog det ratio: 74.8667416715335\nScatter: 10.619551316659306\nloss(exp.(logP₀), exp.(logσ)) = 610.9475830180504\nLog likelihood: -568.2098996989031\nLog det ratio: 74.39427933020599\nScatter: 11.081087308088774\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 610.8918867270929\nLog likelihood: -568.1535043639661\nLog det ratio: 74.21729022268943\nScatter: 11.259474503564268\nloss(exp.(logP₀), exp.(logσ)) = 610.8059882464455\nLog likelihood: -568.0677514294256\nLog det ratio: 74.23591352633076\nScatter: 11.24056010770908\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](regression_files/figure-commonmark/cell-8-output-5.svg){}\n:::\n:::\n\n\n## Calibration Plot\nOnce the prior precision has been optimized it is possible to evaluate the quality of the predictive distribution \nobtained through a calibration plot and a test dataset (y_test, X_test). \n\nFirst, we apply the trained network on the test dataset (y_test, X_test) and collect the neural network's predicted distributions \n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\npredicted_distributions= predict(la, X_test,ret_distr=true)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n600×1 Matrix{Distributions.Normal{Float64}}:\n Distributions.Normal{Float64}(μ=0.9150201678276062, σ=0.0712698621939572)\n Distributions.Normal{Float64}(μ=0.2141074538230896, σ=0.08509629880573434)\n Distributions.Normal{Float64}(μ=0.43884706497192383, σ=0.05711459300259106)\n Distributions.Normal{Float64}(μ=-0.696795642375946, σ=0.0528907494338802)\n Distributions.Normal{Float64}(μ=-0.4487711787223816, σ=0.05667919814727773)\n Distributions.Normal{Float64}(μ=1.0599703788757324, σ=0.07331830051512468)\n Distributions.Normal{Float64}(μ=-0.6711699366569519, σ=0.0531862091132709)\n Distributions.Normal{Float64}(μ=0.7063796520233154, σ=0.07448023253385215)\n Distributions.Normal{Float64}(μ=-0.5376694798469543, σ=0.05548268083704169)\n Distributions.Normal{Float64}(μ=-0.4553452730178833, σ=0.06913082294474085)\n Distributions.Normal{Float64}(μ=-0.7690489888191223, σ=0.0528685253654398)\n Distributions.Normal{Float64}(μ=-0.7591468691825867, σ=0.06655473146173639)\n Distributions.Normal{Float64}(μ=0.09281894564628601, σ=0.13290530919567922)\n ⋮\n Distributions.Normal{Float64}(μ=-0.8616088032722473, σ=0.05546205318284079)\n Distributions.Normal{Float64}(μ=-0.593717634677887, σ=0.07036250789010345)\n Distributions.Normal{Float64}(μ=-0.9107488989830017, σ=0.05602522103411315)\n Distributions.Normal{Float64}(μ=0.687362790107727, σ=0.07600257202140666)\n Distributions.Normal{Float64}(μ=0.15490028262138367, σ=0.05495145734017776)\n Distributions.Normal{Float64}(μ=-0.807168185710907, σ=0.05354646272047298)\n Distributions.Normal{Float64}(μ=-0.28883808851242065, σ=0.06961852085326199)\n Distributions.Normal{Float64}(μ=0.8665060997009277, σ=0.050668256164857886)\n Distributions.Normal{Float64}(μ=0.053950488567352295, σ=0.07557626514837819)\n Distributions.Normal{Float64}(μ=0.10951238870620728, σ=0.0750927300730865)\n Distributions.Normal{Float64}(μ=0.41646575927734375, σ=0.08247027059087846)\n Distributions.Normal{Float64}(μ=-0.30292409658432007, σ=0.05713547559568841)\n```\n:::\n:::\n\n\nthen we can plot the calibration plot of our neural model\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nCalibration_Plot(la,y_test,vec(predicted_distributions);n_bins = 20)\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_files/figure-commonmark/cell-10-output-1.svg){}\n:::\n:::\n\n\nand compute the sharpness of the predictive distribution\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nsharpness_regression(vec(predicted_distributions))\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.004653979376388681\n```\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files\\figure-commonmark"
    ],
    "filters": []
  }
}
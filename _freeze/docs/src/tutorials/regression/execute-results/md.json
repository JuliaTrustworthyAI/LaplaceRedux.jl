{
  "hash": "5a4b0f727b0fa06b8f883c934a123434",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = LaplaceRedux\n```\n## Libraries\n\nImport the libraries required to run this example\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:wong)\n```\n:::\n\n\n## Data\n\nWe first generate some synthetic data:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nn = 3000       # number of observations\nσtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=σtrue,seed=1234)\nxs = [[x] for x in x]\nX = permutedims(x)\n```\n:::\n\n\nand split them in a training set  and a test set\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# Shuffle the data\nRandom.seed!(1234)  # Set a seed for reproducibility\nshuffle_indices = shuffle(1:n)\n\n# Define split ratios\ntrain_ratio = 0.8\ntest_ratio = 0.2\n\n# Calculate split indices\ntrain_end = Int(floor(train_ratio * n))\n\n# Split the data\ntrain_indices = shuffle_indices[1:train_end]\ntest_indices = shuffle_indices[train_end+1:end]\n\n# Create the splits\nx_train, y_train = x[train_indices], y[train_indices]\nx_test, y_test = x[test_indices], y[test_indices]\n\n# Optional: Convert to desired format\nxs_train = [[x] for x in x_train]\nxs_test = [[x] for x in x_test]\nX_train = permutedims(x_train)\nX_test = permutedims(x_test)\n```\n:::\n\n\n## MLP\n\nWe set up a model and loss with weight regularization:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\ntrain_data = zip(xs_train,y_train)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)\n```\n:::\n\n\nWe train the model:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(train_data) = mean(map(d -> loss(d[1],d[2]), train_data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in train_data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(train_data)\n  end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nLaplace approximation can be implemented as follows:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nsubset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, train_data)\nplot(la, X_train, y_train; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](regression_files/figure-commonmark/cell-7-output-1.svg){}\n:::\n:::\n\n\nNext we optimize the prior precision $P_0$ and and observational noise $\\sigma$ using Empirical Bayes:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\noptimize_prior!(la; verbose=true)\nplot(la, X_train, y_train; zoom=-5, size=(400,400))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 668.3714946472106\nLog likelihood: -618.5175117610522\nLog det ratio: 68.76532606873238\nScatter: 30.942639703584522\nloss(exp.(logP₀), exp.(logσ)) = 719.2536119935747\nLog likelihood: -673.0996963447847\nLog det ratio: 76.53255037599948\nScatter: 15.775280921580569\nloss(exp.(logP₀), exp.(logσ)) = 574.605864472924\nLog likelihood: -528.694286608232\nLog det ratio: 80.73114330857285\nScatter: 11.092012420811196\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nloss(exp.(logP₀), exp.(logσ)) = 568.4433850825203\nLog likelihood: -522.4407550111031\nLog det ratio: 82.10089958560243\nScatter: 9.90436055723207\nloss(exp.(logP₀), exp.(logσ)) = 566.9485255672008\nLog likelihood: -520.9682443835385\nLog det ratio: 81.84516297272847\nScatter: 10.11539939459612\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 559.9852101992792\nLog likelihood: -514.0625630685765\nLog det ratio: 80.97813304453496\nScatter: 10.867161216870441\nloss(exp.(logP₀), exp.(logσ)) = 559.1404593114019\nLog likelihood: -513.2449017869876\nLog det ratio: 80.16026747795866\nScatter: 11.630847570869795\nloss(exp.(logP₀), exp.(logσ)) = 559.3201392562346\nLog likelihood: -513.4273312363501\nLog det ratio: 79.68892769076004\nScatter: 12.096688349008877\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nloss(exp.(logP₀), exp.(logσ)) = 559.2111983983311\nLog likelihood: -513.3174948065804\nLog det ratio: 79.56631681347287\nScatter: 12.2210903700287\nloss(exp.(logP₀), exp.(logσ)) = 559.1107459310829\nLog likelihood: -513.2176579845662\nLog det ratio: 79.63946732368183\nScatter: 12.146708569351494\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](regression_files/figure-commonmark/cell-8-output-5.svg){}\n:::\n:::\n\n\n## Calibration Plot\nOnce the prior precision has been optimized it is possible to evaluate the quality of the predictive distribution \nobtained through a calibration plot and a test dataset (y_test, X_test). \n\nFirst, we apply the trained network on the test dataset (y_test, X_test) and collect the neural network's predicted distributions \n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\npredicted_distributions= predict(la, X_test,ret_distr=true)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n600×1 Matrix{Distributions.Normal{Float64}}:\n Distributions.Normal{Float64}(μ=-0.1137533187866211, σ=0.07161056521032018)\n Distributions.Normal{Float64}(μ=0.7063850164413452, σ=0.050697938829269665)\n Distributions.Normal{Float64}(μ=-0.2211049497127533, σ=0.06876939416479119)\n Distributions.Normal{Float64}(μ=0.720299243927002, σ=0.08665125572287981)\n Distributions.Normal{Float64}(μ=-0.8338974714279175, σ=0.06464012115237727)\n Distributions.Normal{Float64}(μ=0.9910320043563843, σ=0.07452060172164382)\n Distributions.Normal{Float64}(μ=0.1507074236869812, σ=0.07316299850461126)\n Distributions.Normal{Float64}(μ=0.20875799655914307, σ=0.05507748397231652)\n Distributions.Normal{Float64}(μ=0.973572850227356, σ=0.07899004963915071)\n Distributions.Normal{Float64}(μ=0.9497100114822388, σ=0.07750126389821968)\n Distributions.Normal{Float64}(μ=0.22462180256843567, σ=0.07103664786246695)\n Distributions.Normal{Float64}(μ=-0.7654240131378174, σ=0.05501397704409917)\n Distributions.Normal{Float64}(μ=1.0029183626174927, σ=0.07619466916431794)\n ⋮\n Distributions.Normal{Float64}(μ=0.7475956678390503, σ=0.049875919157527815)\n Distributions.Normal{Float64}(μ=0.019430622458457947, σ=0.07445076746045155)\n Distributions.Normal{Float64}(μ=-0.9451781511306763, σ=0.05929712369810892)\n Distributions.Normal{Float64}(μ=-0.9813591241836548, σ=0.05844012710417755)\n Distributions.Normal{Float64}(μ=-0.6470385789871216, σ=0.055754609087554294)\n Distributions.Normal{Float64}(μ=-0.34288135170936584, σ=0.05533523375842789)\n Distributions.Normal{Float64}(μ=0.9912381172180176, σ=0.07872473667398772)\n Distributions.Normal{Float64}(μ=-0.824547290802002, σ=0.05499258101374759)\n Distributions.Normal{Float64}(μ=-0.3306621015071869, σ=0.06745251908756716)\n Distributions.Normal{Float64}(μ=0.3742436170578003, σ=0.10588913330223387)\n Distributions.Normal{Float64}(μ=0.0875578224658966, σ=0.07436153828228255)\n Distributions.Normal{Float64}(μ=-0.34871187806129456, σ=0.06742745343084512)\n```\n:::\n:::\n\n\nthen we can plot the calibration plot of our neural model\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nCalibration_Plot(la,y_test,vec(predicted_distributions);n_bins = 20)\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_files/figure-commonmark/cell-10-output-1.svg){}\n:::\n:::\n\n\nand compute the sharpness of the predictive distribution\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nsharpness_regression(vec(predicted_distributions))\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.005058067743863281\n```\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files\\figure-commonmark"
    ],
    "filters": []
  }
}
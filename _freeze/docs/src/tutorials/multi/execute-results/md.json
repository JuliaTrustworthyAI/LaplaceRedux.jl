{
  "hash": "06527b9b70d49522dd7732f90322db8b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Multi-class problem\n---\n\n\n\n## Libraries\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:lime)\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing LaplaceRedux.Data\nx, y = Data.toy_data_multi()\nX = hcat(x...)\ny_onehot = Flux.onehotbatch(y, unique(y))\ny_onehot = Flux.unstack(y_onehot',1)\n```\n:::\n\n\nsplit in training and test datasets\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# Shuffle the data\nn = length(y)\nindices = randperm(n)\n\n# Define the split ratio\nsplit_ratio = 0.8\nsplit_index = Int(floor(split_ratio * n))\n\n# Split the data into training and test sets\ntrain_indices = indices[1:split_index]\ntest_indices = indices[split_index+1:end]\n\nx_train = x[train_indices]\nx_test = x[test_indices]\ny_onehot_train = y_onehot[train_indices,:]\ny_onehot_test = y_onehot[test_indices,:]\n\ny_train = vec(y[train_indices,:])\ny_test = vec(y[test_indices,:])\n# bring into tabular format\nX_train = hcat(x_train...) \nX_test = hcat(x_test...) \n\ndata = zip(x_train,y_onehot_train)\n#data = zip(x,y_onehot)\n```\n:::\n\n\n## MLP\n\nWe set up a model\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nn_hidden = 3\nD = size(X,1)\nout_dim = length(unique(y))\nnn = Chain(\n    Dense(D, n_hidden, σ),\n    Dense(n_hidden, out_dim)\n)  \nloss(x, y) = Flux.Losses.logitcrossentropy(nn(x), y)\n```\n:::\n\n\ntraining:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n    for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        update!(opt, Flux.params(nn), gs)\n    end\n    if epoch % show_every == 0\n        println(\"Epoch \" * string(epoch))\n        @show avg_loss(data)\n    end\nend\n```\n:::\n\n\n## Laplace Approximation\n\nThe Laplace approximation can be implemented as follows:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nla = Laplace(nn; likelihood=:classification)\nfit!(la, data)\noptimize_prior!(la; verbose=true, n_steps=100)\n```\n:::\n\n\nwith either the probit approximation:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\n_labels = sort(unique(y))\nplt_list = []\nfor target in _labels\n    plt = plot(la, X_test, y_test; target=target, clim=(0,1))\n    push!(plt_list, plt)\nend\nplot(plt_list...)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](multi_files/figure-commonmark/cell-8-output-1.svg){}\n:::\n:::\n\n\n or the plugin approximation:\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\n_labels = sort(unique(y))\nplt_list = []\nfor target in _labels\n    plt = plot(la, X_test, y_test; target=target, clim=(0,1), link_approx=:plugin)\n    push!(plt_list, plt)\nend\nplot(plt_list...)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n![](multi_files/figure-commonmark/cell-9-output-1.svg){}\n:::\n:::\n\n\n## Calibration Plots\n\nIn the case of multiclass classification tasks, we cannot plot directly the calibration plots since they can only be used in the binary classification case. However, we can use them to plot the calibration of the predictions for 1 class against all the others. To do so, we first have to collect the predicted categorical distributions\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\npredicted_distributions= predict(la, X_test,ret_distr=true)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n1×20 Matrix{Distributions.Categorical{Float64, Vector{Float64}}}:\n Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(4), p=[0.677436, 0.0384923, 0.118736, 0.165336])  …  Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(4), p=[0.0261611, 0.688918, 0.199687, 0.085234])\n```\n:::\n:::\n\n\nthen we transform the categorical distributions into Bernoulli distributions by taking only the probability of the class of interest, for example the third one. \n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nusing Distributions\nbernoulli_distributions = [Bernoulli(p.p[3]) for p in vec(predicted_distributions)]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n20-element Vector{Bernoulli{Float64}}:\n Bernoulli{Float64}(p=0.11873558300173745)\n Bernoulli{Float64}(p=0.11498061136134086)\n Bernoulli{Float64}(p=0.6674405629824905)\n Bernoulli{Float64}(p=0.03286998149953749)\n Bernoulli{Float64}(p=0.6574893211057788)\n Bernoulli{Float64}(p=0.1133413386424115)\n Bernoulli{Float64}(p=0.11387611100108382)\n Bernoulli{Float64}(p=0.03241056796828973)\n Bernoulli{Float64}(p=0.6674299268262424)\n Bernoulli{Float64}(p=0.19985404101512141)\n Bernoulli{Float64}(p=0.03324098746202494)\n Bernoulli{Float64}(p=0.20007441459444497)\n Bernoulli{Float64}(p=0.19922259256089178)\n Bernoulli{Float64}(p=0.6697881738458813)\n Bernoulli{Float64}(p=0.03258399908041036)\n Bernoulli{Float64}(p=0.11381645240815817)\n Bernoulli{Float64}(p=0.03379412786302275)\n Bernoulli{Float64}(p=0.19994807070245754)\n Bernoulli{Float64}(p=0.11396233020534376)\n Bernoulli{Float64}(p=0.19968706593359128)\n```\n:::\n:::\n\n\nNow we can use the calibration Plot to see the level of calibration of the neural network\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nplt = Calibration_Plot(la,hcat(y_onehot_test...)[3,:],bernoulli_distributions;n_bins = 20);\n```\n\n::: {.cell-output .cell-output-display}\n![](multi_files/figure-commonmark/cell-12-output-1.svg){}\n:::\n:::\n\n\nThe plot is peaked around 0.7, and This may be due to various reasons.\n\nA possible reason is that class 3 is relatively easy for the model to identify from the other classes. The network is able to  correctly identify examples belonging to class 3, although he remains a bit underconfident in its predictions. \nAnother reason for the peak however may be the lack of cases where the predicted probability is lower (e.g., around 0.5), which could indicate that the network  has not encountered ambiguous or difficult-to-classify examples for class 3.  This once again might be because either class 3 has distinct features that the model can easily learn, leading to fewer uncertain predictions, or is a consequence of the limited dataset. \n\n We can measure how sharp is the neural network by computing the sharpness score\n\nsharpness_classification(hcat(y_onehot_test...)[3,:],bernoulli_distributions)\n\n```\n\n\nThe neural network seems to be able to correctly classify the majority of samples with  a relative high confidence, although not to the level of the binary case. This is most likely due to the greater difficulty in classifying 4 different classes when compared to having to classify only two classes.\n\n",
    "supporting": [
      "multi_files\\figure-commonmark"
    ],
    "filters": []
  }
}
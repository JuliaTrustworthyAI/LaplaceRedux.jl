```@meta
CurrentModule = LaplaceRedux
```

```{julia}
#| echo: false
# Import libraries.
using Flux, Plots, Random, Statistics, LaplaceRedux
theme(:lime)
include("dev/utils.jl") # some plotting functions
www_path = "docs/src/tutorials/www"
```

```{julia}
using LaplaceRedux.Data
fun(x) = sin(2*π*x)
x, y = Data.toy_data_regression(;fun=fun)
xs = [[x] for x in x]
X = permutedims(x)
scatter(x, y, label="ytrain")
xrange = 0:0.01:1
plot!(xrange, fun.(0:0.01:1), label="ytrue")
```

# MLP

```{julia}
data = zip(xs,y)
n_hidden = 50
D = size(X,1)
nn = Chain(
    Dense(D, n_hidden, relu),
    Dense(n_hidden, n_hidden, relu),
    Dense(n_hidden, 1)
)  
λ = 0.01
sqnorm(x) = sum(abs2, x)
weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))
loss(x, y) = Flux.Losses.mse(nn(x), y) + weight_regularization();
```

The model is trained for 200 epochs before the training loss stagnates.

```{julia}
using Flux.Optimise: update!, Adam
opt = Adam()
epochs = 100
avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
show_every = epochs/10

for epoch = 1:epochs
  for d in data
    gs = gradient(Flux.params(nn)) do
      l = loss(d...)
    end
    update!(opt, Flux.params(nn), gs)
  end
  if epoch % show_every == 0
    println("Epoch " * string(epoch))
    @show avg_loss(data)
  end
end
```

## Laplace Approximation

Laplace approximation can be implemented as follows:

```{julia}
la = Laplace(nn; likelihood=:regression, λ=λ, subset_of_weights=:last_layer)
fit!(la, data)
```

```{julia}
yhat = reduce(vcat, [predict(la, [x]) for x in xrange])
plot!(xrange, yhat, color=:orange, label="yhat")
```





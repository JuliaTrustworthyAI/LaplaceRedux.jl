{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```@meta\n",
        "CurrentModule = LaplaceRedux\n",
        "```\n"
      ],
      "id": "17f33854"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Pkg; Pkg.activate(\"docs\")\n",
        "# Import libraries\n",
        "using Flux, Plots, Random, Statistics, LaplaceRedux\n",
        "theme(:lime)"
      ],
      "id": "aff28d0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n",
        "\n",
        "We first generate some synthetic data:\n"
      ],
      "id": "5d1dbb65"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using LaplaceRedux.Data\n",
        "fun(x) = sin(2*π*x)\n",
        "n = 100     # number of observations\n",
        "σtrue = 0.3       # true observational noise\n",
        "x, y = Data.toy_data_regression(100;noise=σtrue,fun=fun)\n",
        "xs = [[x] for x in x]\n",
        "X = permutedims(x)"
      ],
      "id": "a5bf6d37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP\n",
        "\n",
        "We set up a model and loss with weight regularization:\n"
      ],
      "id": "53d448e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = zip(xs,y)\n",
        "n_hidden = 50\n",
        "D = size(X,1)\n",
        "nn = Chain(\n",
        "    Dense(D, n_hidden, tanh_fast),\n",
        "    Dense(n_hidden, n_hidden, tanh_fast),\n",
        "    Dense(n_hidden, 1)\n",
        ")  \n",
        "λ = 0.01\n",
        "sqnorm(x) = sum(abs2, x)\n",
        "weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\n",
        "loss(x, y) = Flux.Losses.mse(nn(x), y) + weight_regularization();"
      ],
      "id": "9fcaa5a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We train the model:\n"
      ],
      "id": "a6a1ad69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux.Optimise: update!, Adam\n",
        "opt = Adam()\n",
        "epochs = 100\n",
        "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
        "show_every = epochs/10\n",
        "\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "  if epoch % show_every == 0\n",
        "    println(\"Epoch \" * string(epoch))\n",
        "    @show avg_loss(data)\n",
        "  end\n",
        "end"
      ],
      "id": "b3fdb35e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Laplace Approximation\n",
        "\n",
        "Laplace approximation can be implemented as follows:\n"
      ],
      "id": "e26f832f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "la = Laplace(nn; likelihood=:regression, λ=λ, subset_of_weights=:last_layer, σ=σtrue)\n",
        "fit!(la, data)\n",
        "plot(la, X, y)"
      ],
      "id": "ee75ea3b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
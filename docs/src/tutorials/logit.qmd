---
format: 
  commonmark:
    variant: -raw_html
    wrap: none
    self-contained: true
crossref:
  fig-prefix: Figure
  tbl-prefix: Table
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
output: asis
execute: 
  echo: true
  eval: false
jupyter: julia-1.6
---

```@meta
CurrentModule = BayesLaplace
```

# Bayesian Logisitic Regression

```{julia}
#| echo: false
# Import libraries.
using Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace
theme(:lime)
using Logging
disable_logging(Logging.Info)
include("dev/utils.jl") # some plotting functions
www_path = "docs/src/tutorials/www"
```

We will use synthetic data with linearly separable samples:

```{julia}
# Number of points to generate.
xs, ys = BayesLaplace.Data.toy_data_linear(100)
X = hcat(xs...) # bring into tabular format
data = zip(xs,ys)
```

Logisitic regression with weight decay can be implemented in Flux.jl as a single dense (linear) layer with binary logit crossentropy loss:

```{julia}
nn = Chain(Dense(2,1))
λ = 0.5
sqnorm(x) = sum(abs2, x)
weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()
```

The code below simply trains the model. After about 50 training epochs training loss stagnates.

```{julia}
using Flux.Optimise: update!, ADAM
opt = ADAM()
epochs = 50
avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
show_every = epochs/10

for epoch = 1:epochs
  for d in data
    gs = gradient(params(nn)) do
      l = loss(d...)
    end
    update!(opt, params(nn), gs)
  end
  if epoch % show_every == 0
    println("Epoch " * string(epoch))
    @show avg_loss(data)
  end
end
```

## Laplace appoximation

Laplace approximation for the posterior predictive can be implemented as follows:

```{julia}
la = Laplace(nn, λ=λ, subset_of_weights=:last_layer)
fit!(la, data)
```

The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).

```{julia}
#| echo: false
p_plugin = plot_contour(X',ys,la;title="Plugin",type=:plugin)
p_laplace = plot_contour(X',ys,la;title="Laplace")
# Plot the posterior distribution with a contour plot.
plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))
savefig(plt, joinpath(www_path, "posterior_predictive.png"))
```

![](www/posterior_predictive.png)


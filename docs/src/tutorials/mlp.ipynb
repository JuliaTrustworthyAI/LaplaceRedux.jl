{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```@meta\n",
        "CurrentModule = LaplaceRedux\n",
        "```\n",
        "\n",
        "# Bayesian MLP\n"
      ],
      "id": "5e4f206d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Pkg; Pkg.activate(\"docs\")\n",
        "# Import libraries\n",
        "using Flux, Plots, Random, Statistics, LaplaceRedux\n",
        "theme(:lime)"
      ],
      "id": "0df62d80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we use a synthetic dataset containing samples that are not linearly separable:\n"
      ],
      "id": "127c4ef6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Number of points to generate.\n",
        "xs, ys = LaplaceRedux.Data.toy_data_non_linear(200)\n",
        "X = hcat(xs...) # bring into tabular format\n",
        "data = zip(xs,ys)"
      ],
      "id": "50762686",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the classification task we build a neural network with weight decay composed of a single hidden layer.\n"
      ],
      "id": "2a65a185"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_hidden = 32\n",
        "D = size(X,1)\n",
        "nn = Chain(\n",
        "    Dense(D, n_hidden, σ),\n",
        "    Dense(n_hidden, 1)\n",
        ")  \n",
        "λ = 0.01\n",
        "sqnorm(x) = sum(abs2, x)\n",
        "weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\n",
        "loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();"
      ],
      "id": "cddfc092",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model is trained for 200 epochs before the training loss stagnates.\n"
      ],
      "id": "07f73235"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux.Optimise: update!, Adam\n",
        "opt = Adam()\n",
        "epochs = 200\n",
        "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
        "show_every = epochs/10\n",
        "\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "  if epoch % show_every == 0\n",
        "    println(\"Epoch \" * string(epoch))\n",
        "    @show avg_loss(data)\n",
        "  end\n",
        "end"
      ],
      "id": "09ad2d79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Laplace Approximation\n",
        "\n",
        "Laplace approximation can be implemented as follows:\n"
      ],
      "id": "2621d64a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "la = Laplace(nn; likelihood=:classification, λ=λ, subset_of_weights=:last_layer)\n",
        "fit!(la, data)"
      ],
      "id": "9c16841a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).\n"
      ],
      "id": "cdcffe3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "\n",
        "# Plot the posterior distribution with a contour plot.\n",
        "zoom=0\n",
        "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1), zoom=zoom)\n",
        "p_laplace = plot(la, X, ys; title=\"Laplace\", clim=(0,1), zoom=zoom)\n",
        "plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))"
      ],
      "id": "c726c963",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zooming out we can note that the plugin estimator produces high-confidence estimates in regions scarce of any samples. The Laplace approximation is much more conservative about these regions.\n"
      ],
      "id": "9c8e7cba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "\n",
        "zoom=-50\n",
        "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1), zoom=zoom)\n",
        "p_laplace = plot(la, X, ys; title=\"Laplace\", clim=(0,1), zoom=zoom)\n",
        "plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))"
      ],
      "id": "b7954791",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
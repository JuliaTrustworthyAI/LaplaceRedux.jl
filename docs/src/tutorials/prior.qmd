```@meta
CurrentModule = LaplaceRedux
```

```{julia}
#| echo: false
using Pkg; Pkg.activate("docs")
# Import libraries
using Flux, Plots, Random, Statistics, LaplaceRedux
theme(:lime)
```

## A quick note on the prior 

Low prior uncertainty $\rightarrow$ posterior dominated by prior. High prior uncertainty $\rightarrow$ posterior approaches MLE.

```{julia}
# Number of points to generate:
xs, y = LaplaceRedux.Data.toy_data_non_linear(200)
X = hcat(xs...); # bring into tabular format
data = zip(xs,y)
```


```{julia}
#| echo: false
n_hidden = 32
D = size(X)[1]
Λ = [1e10, 0.01, 1e-10]
plts = []
nns = []
opt=Flux.Adam()
for λ ∈ Λ
    nn = Chain(
        Dense(randn(n_hidden,D)./10,zeros(n_hidden), σ),
        Dense(randn(1,n_hidden)./10,zeros(1))
    )  
    sqnorm(x) = sum(abs2, x)
    weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))
    loss(x, y; λ=λ) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization(λ)
    # train
    epochs = 200
    for epoch = 1:epochs
        for d in data
        gs = gradient(Flux.params(nn)) do
            l = loss(d...;λ=λ)
        end
        Flux.update!(opt, Flux.params(nn), gs)
        end
    end
    # laplace
    la = Laplace(nn; likelihood=:classification, λ=λ)
    fit!(la, data)  
    # plot 
    plt = plot(la, X, y;title="σ₀²=$(1/λ)")
    plts = vcat(plts..., plt)
    nns = vcat(nns..., nn)
end
plot(plts..., layout=(1,3), size=(1200,300))
```
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```@meta\n",
    "CurrentModule = BayesLaplace\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "using Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace\n",
    "theme(:lime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we use a synthetic dataset containing samples that are not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points to generate.\n",
    "xs, y = toy_data_non_linear(200)\n",
    "X = hcat(xs...); # bring into tabular format\n",
    "data = zip(xs,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification task we build a neural network with weight decay composed of a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 32\n",
    "D = size(X)[1]\n",
    "nn = Chain(\n",
    "    Dense(D, n_hidden, σ),\n",
    "    Dense(n_hidden, 1)\n",
    ")  \n",
    "λ = 0.01\n",
    "sqnorm(x) = sum(abs2, x)\n",
    "weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\n",
    "loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 200 epochs before the training loss stagnates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Saved animation to \n",
      "│   fn = /Users/FA31DU/Documents/code/BayesLaplace.jl/docs/src/www/nn_training_mlp.gif\n",
      "└ @ Plots /Users/FA31DU/.julia/packages/Plots/8ouqB/src/animation.jl:114\n"
     ]
    }
   ],
   "source": [
    "using Flux.Optimise: update!, ADAM\n",
    "opt = ADAM()\n",
    "epochs = 200\n",
    "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
    "\n",
    "using Plots\n",
    "anim = Animation()\n",
    "plt = plot(ylim=(0,avg_loss(data)), xlim=(0,epochs), legend=false, xlab=\"Epoch\")\n",
    "avg_l = []\n",
    "\n",
    "for epoch = 1:epochs\n",
    "  for d in data\n",
    "    gs = gradient(params(nn)) do\n",
    "      l = loss(d...)\n",
    "    end\n",
    "    update!(opt, params(nn), gs)\n",
    "  end\n",
    "  avg_l = vcat(avg_l,avg_loss(data))\n",
    "  plot!(plt, avg_l, color=1, title=\"Average (training) loss\")\n",
    "  frame(anim, plt)\n",
    "end\n",
    "\n",
    "gif(anim, \"www/nn_training_mlp.gif\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/nn_training_mlp.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace appoximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace approximation can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = laplace(nn, λ=λ, subset_of_weights=:last_layer)\n",
    "fit!(la, data);\n",
    "zoom=0\n",
    "p_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\n",
    "p_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior distribution with a contour plot.\n",
    "plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\n",
    "savefig(plt, \"www/posterior_predictive_mlp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/posterior_predictive_mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming out we can note that the plugin estimator produces high-confidence estimates in regions scarce of any samples. The Laplace approximation is much more conservative about these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom=-50\n",
    "p_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\n",
    "p_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);\n",
    "# Plot the posterior distribution with a contour plot.\n",
    "plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\n",
    "savefig(plt, \"www/posterior_predictive_mlp_zoomed.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/posterior_predictive_mlp_zoomed.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

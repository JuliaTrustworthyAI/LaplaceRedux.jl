{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Pkg; Pkg.activate(\"docs\")\n",
        "# Import libraries\n",
        "using Flux, Plots, Random, Statistics, LaplaceRedux\n",
        "theme(:lime)"
      ],
      "id": "9a11b0ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`LaplaceRedux.jl` is a library written in pure Julia that can be used for effortless Bayesian Deep Learning trough Laplace Approximation (LA). In the development of this package I have drawn inspiration from this Python [library](https://aleximmer.github.io/Laplace/index.html#setup) and its companion [paper](https://arxiv.org/abs/2106.14806) [@daxberger2021laplace].\n",
        "\n",
        "## üö© Installation\n",
        "\n",
        "The stable version of this package can be installed as follows:\n",
        "\n",
        "```{.julia}\n",
        "using Pkg\n",
        "Pkg.add(\"LaplaceRedux.jl\")\n",
        "```\n",
        "\n",
        "The development version can be installed like so:\n",
        "\n",
        "```{.julia}\n",
        "using Pkg\n",
        "Pkg.add(\"https://github.com/pat-alt/LaplaceRedux.jl\")\n",
        "```\n",
        "\n",
        "## üñ•Ô∏è Basic Usage\n",
        "\n",
        "`LaplaceRedux.jl` can be used for any neural network trained in [`Flux.jl`](https://fluxml.ai/Flux.jl/dev/). Below we show basic usage examples involving two simple models for a regression and a classification task, respectively.\n",
        "\n",
        "### Regression\n"
      ],
      "id": "3edae2c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "using LaplaceRedux\n",
        "using LaplaceRedux.Data: toy_data_regression\n",
        "using Flux.Optimise: update!, Adam\n",
        "\n",
        "# Data:\n",
        "n = 100           # number of observations\n",
        "œÉtrue = 0.3       # true observational noise\n",
        "x, y = toy_data_regression(100;noise=œÉtrue)\n",
        "xs = [[x] for x in x]\n",
        "X = permutedims(x)\n",
        "data = zip(xs,y)\n",
        "\n",
        "# Model:\n",
        "n_hidden = 50\n",
        "D = size(X,1)\n",
        "nn = Chain(\n",
        "    Dense(D, n_hidden, tanh),\n",
        "    Dense(n_hidden, 1)\n",
        ")  \n",
        "loss(x, y) = Flux.Losses.mse(nn(x), y)\n",
        "\n",
        "# Training:\n",
        "opt = Adam(1e-3)\n",
        "epochs = 1000\n",
        "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
        "show_every = epochs/10\n",
        "\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "  if epoch % show_every == 0\n",
        "    println(\"Epoch \" * string(epoch))\n",
        "    @show avg_loss(data)\n",
        "  end\n",
        "end"
      ],
      "id": "5e6198e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A complete worked example for a regression model can be found in the [docs](https://www.paltmeyer.com/LaplaceRedux.jl/dev/tutorials/regression/). Here we jump straight to Laplace Approximation and take the pre-trained model `nn` as given. Then LA can be implemented as follows, where we specify the model `likelihood`. The plot show the fitted values overlayed with a 95% confidence interval. As expected, predictive uncertainty quickly increases in areas that are not populated by any training data.\n"
      ],
      "id": "2a842149"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "\n",
        "la = Laplace(nn; likelihood=:regression)\n",
        "fit!(la, data)\n",
        "optimize_prior!(la)\n",
        "plot(la, X, y; zoom=-5, size=(400,400))"
      ],
      "id": "85d57c42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary Classification\n"
      ],
      "id": "d793f39c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "using LaplaceRedux.Data: toy_data_non_linear\n",
        "\n",
        "# Data:\n",
        "xs, ys = toy_data_non_linear(200)\n",
        "X = hcat(xs...)     # bring into tabular format\n",
        "data = zip(xs,ys)\n",
        "\n",
        "# Model:\n",
        "n_hidden = 32\n",
        "D = size(X,1)\n",
        "nn = Chain(\n",
        "    Dense(D, n_hidden, œÉ),\n",
        "    Dense(n_hidden, 1)\n",
        ")  \n",
        "loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) \n",
        "\n",
        "# Training:\n",
        "opt = Adam()\n",
        "epochs = 100\n",
        "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
        "show_every = epochs/10\n",
        "\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "  if epoch % show_every == 0\n",
        "    println(\"Epoch \" * string(epoch))\n",
        "    @show avg_loss(data)\n",
        "  end\n",
        "end"
      ],
      "id": "8dbd843b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again we jump straight to LA and refer to the [docs](https://www.paltmeyer.com/LaplaceRedux.jl/dev/tutorials/mlp/) for a complete worked example involving binary classification. In this case we need to specify `likelihood=:classification`. The plot below shows the resulting posterior predictive distributions as contours in the two-dimensional feature space: note how the **Plugin** Approximation on the left compares to the Laplace Approximation on the right.\n"
      ],
      "id": "87ab06e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "\n",
        "la = Laplace(nn; likelihood=:classification)\n",
        "fit!(la, data)\n",
        "la_untuned = deepcopy(la)   # saving for plotting\n",
        "optimize_prior!(la; verbose=true, n_steps=500)\n",
        "\n",
        "# Plot the posterior predictive distribution:\n",
        "zoom=0\n",
        "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\n",
        "p_untuned = plot(la_untuned, X, ys; title=\"LA - raw (Œª=$(unique(diag(la_untuned.P‚ÇÄ))[1]))\", clim=(0,1), zoom=zoom)\n",
        "p_laplace = plot(la, X, ys; title=\"LA - tuned (Œª=$(round(unique(diag(la.P‚ÇÄ))[1],digits=2)))\", clim=(0,1), zoom=zoom)\n",
        "plot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))"
      ],
      "id": "c9606033",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¢ JuliaCon 2022\n",
        "\n",
        "This project was presented at JuliaCon 2022 in July 2022. See [here](https://pretalx.com/juliacon-2022/talk/Z7MXFS/) for details.\n",
        "\n",
        "## üõ†Ô∏è Contribute\n",
        "\n",
        "Contributions are very much welcome! Please follow the [SciML ColPrac guide](https://github.com/SciML/ColPrac). You may want to start by having a look at any open issues. \n",
        "\n",
        "\n",
        "## ‚ùé Known Limitations\n",
        "\n",
        "This library currently offers native support only for models composed and trained in Flux. It also still lacks out-of-the-box support for hyperparameter tuning. \n",
        "\n",
        "## üéì References\n"
      ],
      "id": "ab206e45"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
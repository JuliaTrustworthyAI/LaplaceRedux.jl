```{julia}
#| echo: false
using Pkg; Pkg.activate("docs")
# Import libraries
using Flux, Plots, Random, Statistics, LaplaceRedux
theme(:lime)
```

`LaplaceRedux.jl` is a library written in pure Julia that can be used for effortless Bayesian Deep Learning trough Laplace Approximation (LA). In the development of this package I have drawn inspiration from this Python [library](https://aleximmer.github.io/Laplace/index.html#setup) and its companion [paper](https://arxiv.org/abs/2106.14806) [@daxberger2021laplace].

## üö© Installation

The stable version of this package can be installed as follows:

```{.julia}
using Pkg
Pkg.add("LaplaceRedux.jl")
```

The development version can be installed like so:

```{.julia}
using Pkg
Pkg.add("https://github.com/pat-alt/LaplaceRedux.jl")
```

## üñ•Ô∏è Basic Usage

`LaplaceRedux.jl` can be used for any neural network trained in [`Flux.jl`](https://fluxml.ai/Flux.jl/dev/). Below we show basic usage examples involving two simple models for a regression and a classification task, respectively.

### Regression

```{julia}
#| echo: false

using LaplaceRedux
using LaplaceRedux.Data: toy_data_regression
using Flux.Optimise: update!, Adam

# Data:
n = 100           # number of observations
œÉtrue = 0.3       # true observational noise
x, y = toy_data_regression(100;noise=œÉtrue)
xs = [[x] for x in x]
X = permutedims(x)
data = zip(xs,y)

# Model:
n_hidden = 50
D = size(X,1)
nn = Chain(
    Dense(D, n_hidden, tanh_fast),
    Dense(n_hidden, n_hidden, tanh_fast),
    Dense(n_hidden, 1)
)  
Œª = 0.01
sqnorm(x) = sum(abs2, x)
weight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))
loss(x, y) = Flux.Losses.mse(nn(x), y) + weight_regularization()

# Training:
opt = Adam()
epochs = 100
avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
show_every = epochs/10
for epoch = 1:epochs
  for d in data
    gs = gradient(Flux.params(nn)) do
      l = loss(d...)
    end
    update!(opt, Flux.params(nn), gs)
  end
  if epoch % show_every == 0
    println("Epoch " * string(epoch))
    @show avg_loss(data)
  end
end
```

A complete worked example for a regression model can be found in the[docs](https://www.paltmeyer.com/LaplaceRedux.jl/dev/tutorials/regression/). Here we jump straight to Laplace Approximation and take the pre-trained model `nn` as given. Then LA can be implemented as follows, where we specify the model `likelihood` and supply pre-determined values for the prior precision `Œª` and the observational noise `œÉ`. The plot show the fitted values overlayed with a 95% confidence interval. As expected, predictive uncertainty quickly increases in areas that are not populated by any training data.

```{julia}
#| output: true

la = Laplace(nn; likelihood=:regression, Œª=Œª, œÉ=œÉtrue)
fit!(la, data)
plot(la, X, y)
```

### Binary Classification

```{julia}
#| echo: false

using LaplaceRedux.Data: toy_data_non_linear

# Data:
xs, ys = toy_data_non_linear(200)
X = hcat(xs...)     # bring into tabular format
data = zip(xs,ys)

# Model:
n_hidden = 32
D = size(X,1)
nn = Chain(
    Dense(D, n_hidden, œÉ),
    Dense(n_hidden, 1)
)  
Œª = 0.01
sqnorm(x) = sum(abs2, x)
weight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()

# Training:
opt = Adam()
epochs = 200
avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
show_every = epochs/10

for epoch = 1:epochs
  for d in data
    gs = gradient(Flux.params(nn)) do
      l = loss(d...)
    end
    update!(opt, Flux.params(nn), gs)
  end
  if epoch % show_every == 0
    println("Epoch " * string(epoch))
    @show avg_loss(data)
  end
end
```

Once again we jump straight to LA and refer to the [docs](https://www.paltmeyer.com/LaplaceRedux.jl/dev/tutorials/mlp/) for a complete worked example involving binary classification. In this case we need to specify `likelihood=:classification`. The plot below shows the resulting posterior predictive distributions as contours in the two-dimensional feature space: note how the **Plugin** Approximation on the left compares to the Laplace Approximation on the right.

```{julia}
#| output: true

la = Laplace(nn; likelihood=:classification, Œª=Œª)
fit!(la, data)

# Plot the posterior predictive distribution:
p_plugin = plot(la, X, ys; title="Plugin", link_approx=:plugin, clim=(0,1))
p_laplace = plot(la, X, ys; title="Laplace", clim=(0,1))
plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))
```


## üì¢ JuliaCon 2022

This project was presented at JuliaCon 2022 in July 2022. See [here](https://pretalx.com/juliacon-2022/talk/Z7MXFS/) for details.

## üõ†Ô∏è Contribute

Contributions are very much welcome! Please follow the [SciML ColPrac guide](https://github.com/SciML/ColPrac). You may want to start by having a look at any open issues. 


## ‚ùé Known Limitations

This library currently offers native support only for models composed and trained in Flux. It also still lacks out-of-the-box support for hyperparameter tuning. 

## üéì References



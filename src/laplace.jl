using .Curvature
using Flux
using LinearAlgebra

mutable struct Laplace
    model::Flux.Chain
    loss::Function
    subset_of_weights::Symbol
    hessian_structure::Symbol
    curvature::Union{Curvature.CurvatureInterface,Nothing}
    H‚ÇÄ::Any
    H::Union{AbstractArray,Nothing}
    Œ£::Union{AbstractArray,Nothing}
    n_params::Union{Int,Nothing}
end

using Parameters

@with_kw struct LaplaceParams 
    loss_type::Symbol=:logitbinarycrossentropy
    subset_of_weights::Symbol=:all
    hessian_structure::Symbol=:full
    backend::Symbol=:EmpiricalFisher
    Œª::Real=1
    H‚ÇÄ::Union{Nothing, AbstractMatrix}=nothing
end

"""
    Laplace(model::Any; loss_type=:logitbinarycrossentropy, subset_of_weights=:last_layer, hessian_structure=:full,backend=:EmpiricalFisher,Œª=1)    

Wrapper function to prepare Laplace approximation.

# Examples

```julia-repl
using Flux, LaplaceRedux
nn = Chain(Dense(2,1))
la = Laplace(nn)
```

"""
function Laplace(model::Any;kwargs...) 

    # Load hyperparameters:
    args = LaplaceParams(;kwargs...)

    # Prior:
    if isnothing(args.H‚ÇÄ)
        H‚ÇÄ = UniformScaling(args.Œª)
    else
        H‚ÇÄ = args.H‚ÇÄ
    end

    # Model: 
    nn = model
    loss(x, y) = getfield(Flux.Losses,args.loss_type)(nn(x), y)

    # Instantiate:
    la = Laplace(model, loss, args.subset_of_weights, args.hessian_structure, nothing, H‚ÇÄ, nothing, nothing, nothing)
    params = get_params(la)
    la.curvature = getfield(Curvature,args.backend)(nn,la.loss,params) # instantiate chosen curvature interface
    la.n_params = length(reduce(vcat, [vec(Œ∏) for Œ∏ ‚àà params]))

    # Sanity:
    if isa(la.H‚ÇÄ, AbstractMatrix)
        @assert all(size(la.H‚ÇÄ) .== la.n_params) "Dimensions of prior Hessian $(size(la.H‚ÇÄ)) do not align with number of parameters ($(Fala.n_params))"
    end

    return la

end

"""
    get_params(la::Laplace) 

Retrieves the desired (sub)set of model parameters and stores them in a list.

# Examples

```julia-repl
using Flux, LaplaceRedux
nn = Chain(Dense(2,1))
la = Laplace(nn)
LaplaceRedux.get_params(la)
```

"""
function get_params(la::Laplace)
    nn = la.model
    params = Flux.params(nn)
    n_elements = length(params)
    if la.subset_of_weights == :all
        params = [Œ∏ for Œ∏ ‚àà params] # get all parameters and constants in logitbinarycrossentropy
    elseif la.subset_of_weights == :last_layer
        params = [params[n_elements-1],params[n_elements]] # only get last parameters and constants
    else
        @error "`subset_of_weights` of weights should be one of the following: `[:all, :last_layer]`"
    end 
    return params
end

"""
    hessian_approximation(la::Laplace, d)

Computes the local Hessian approximation at a single data `d`.
"""
function hessian_approximation(la::Laplace, d)
    H = getfield(Curvature, la.hessian_structure)(la.curvature,d)
    return H
end

"""
    fit!(la::Laplace,data)

Fits the Laplace approximation for a data set.

# Examples

```julia-repl
using Flux, LaplaceRedux
x, y = LaplaceRedux.Data.toy_data_linear()
data = zip(x,y)
nn = Chain(Dense(2,1))
la = Laplace(nn)
fit!(la, data)
```

"""
function fit!(la::Laplace,data)

    H = zeros(la.n_params,la.n_params)
    for d in data
        H += hessian_approximation(la, d)
    end
    la.H = H + la.H‚ÇÄ # posterior precision
    la.Œ£ = inv(la.H) # posterior covariance
    
end

"""
    glm_predictive_distribution(la::Laplace, X::AbstractArray)

Computes the linearized GLM predictive.
"""
function glm_predictive_distribution(la::Laplace, X::AbstractArray)
    ùêâ, yÃÇ = Curvature.jacobians(la.curvature,X)
    Œ£ = predictive_variance(la,ùêâ)
    Œ£ = reshape(Œ£, size(yÃÇ)...)
    return yÃÇ, Œ£
end

"""
    predictive_variance(la::Laplace,ùêâ)

Compute the linearized GLM predictive variance as `ùêâ‚ÇôŒ£ùêâ‚Çô'` where `ùêâ=‚àáf(x;Œ∏)|Œ∏ÃÇ` is the Jacobian evaluated at the MAP estimate and `Œ£ = H‚Åª¬π`.

"""
function predictive_variance(la::Laplace,ùêâ)
    N = size(ùêâ, 1)
    Œ£ = map(n -> ùêâ[n,:]' * la.Œ£ * ùêâ[n,:], 1:N)
    return Œ£
end

# Posterior predictions:
"""
    predict(la::Laplace, X::AbstractArray; link_approx=:probit)

Computes predictions from Bayesian neural network.

# Examples

```julia-repl
using Flux, LaplaceRedux
x, y = toy_data_linear()
data = zip(x,y)
nn = Chain(Dense(2,1))
la = Laplace(nn)
fit!(la, data)
predict(la, hcat(x...))
```

"""
function predict(la::Laplace, X::AbstractArray; link_approx=:probit)
    yÃÇ, Œ£ = glm_predictive_distribution(la, X)

    # Regression:
    if la.loss == Flux.Losses.mse
        return yÃÇ, Œ£
    end

    # Classification:
    if link_approx==:probit
        # Probit approximation
        Œ∫ = 1 ./ sqrt.(1 .+ œÄ/8 .* Œ£) 
        z = Œ∫ .* yÃÇ
        if size(z,1) == 1
            p = Flux.sigmoid(z)
        else
            p = Flux.softmax(z, dims=2)
        end
    end

    return p
end

"""
    plugin(la::Laplace, X::AbstractArray)

Computes the plugin estimate.
"""
function plugin(la::Laplace, X::AbstractArray)
    yÃÇ, Œ£ = glm_predictive_distribution(la, X)
    p = Flux.œÉ.(yÃÇ)
    return p
end

using Flux.Optimise: Adam
"""
    optimize_prior_precision(la::Laplace; n_steps=100, lr=1e-1, init_prior_prec=1.)
    
Optimize the prior precision post-hoc through empirical Bayes (marginal log-likelihood maximization).
"""
function optimize_prior_precision(la::Laplace; n_steps=100, lr=1e-1, init_prior_prec=1.)
    la.H‚ÇÄ = Diagonal(init_prior_prec)
    opt = Adam(lr)
    for i in 1:n_steps

    end
end

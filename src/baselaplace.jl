using LinearAlgebra

"Abstract base type of Laplace Approximation."
abstract type BaseLaplace end

"""
    outdim(la::BaseLaplace)

Helper function to determine the output dimension of a `Flux.Chain` with Laplace approximation.
"""
outdim(la::BaseLaplace) = la.n_out

"""
    get_params(la::BaseLaplace) 

Retrieves the desired (sub)set of model parameters and stores them in a list.

# Examples

```julia-repl
using Flux, LaplaceRedux
nn = Chain(Dense(2,1))
la = Laplace(nn)
LaplaceRedux.get_params(la)
```

"""
function get_params(la::BaseLaplace)
    nn = la.model
    params = Flux.params(nn)
    n_elements = length(params)
    if la.subset_of_weights == :all
        params = [Î¸ for Î¸ âˆˆ params]                         # get all parameters and constants in logitbinarycrossentropy
    elseif la.subset_of_weights == :last_layer
        params = [params[n_elements-1],params[n_elements]]  # only get last parameters and constants
    end
    return params
end

@doc raw"""
    posterior_precision(la::BaseLaplace)

Computes the posterior precision ``P`` for a fitted Laplace Approximation as follows,

``
P = \sum_{n=1}^N\nabla_{\theta}^2\log p(\mathcal{D}_n|\theta)|_{\theta}_{MAP} + \nabla_{\theta}^2 \log p(\theta)|_{\theta}_{MAP} 
``

where ``\sum_{n=1}^N\nabla_{\theta}^2\log p(\mathcal{D}_n|\theta)|_{\theta}_{MAP}=H`` and ``\nabla_{\theta}^2 \log p(\theta)|_{\theta}_{MAP}=P_0``.
"""
function posterior_precision(la::BaseLaplace, H=la.H, Pâ‚€=la.Pâ‚€)
    @assert !isnothing(H) "Hessian not available. Either no value supplied or Laplace Approximation has not yet been estimated."
    return H + Pâ‚€
end

@doc raw"""
    posterior_covariance(la::BaseLaplace, P=la.P)

Computes the posterior covariance as the inverse of the posterior precision: ``\Sigma=P^{-1}``.
"""
function posterior_covariance(la::BaseLaplace, P=posterior_precision(la))
    @assert !isnothing(P) "Posterior precision not available. Either no value supplied or Laplace Approximation has not yet been estimated."
    return inv(P)
end


"""
    log_likelihood(la::BaseLaplace)


"""
function log_likelihood(la::BaseLaplace)
    factor = - _H_factor(la)
    if la.likelihood == :regression
        c = la.n_data * la.n_out * log(la.Ïƒ * sqrt(2 * pi))
    else
        c = 0
    end
    return factor * la.loss - c
end

"""
    _H_factor(la::BaseLaplace)


"""
_H_factor(la::BaseLaplace) = 1 / (la.Ïƒ^2)

"""
    _init_H(la::BaseLaplace)


"""
_init_H(la::BaseLaplace) = zeros(la.n_params, la.n_params)

"""
    _weight_penalty(la::BaseLaplace)


"""
function _weight_penalty(la::BaseLaplace)
    Î¼ = la.Î¼    # MAP
    Î¼â‚€ = la.Î¼â‚€  # prior
    Î” = Î¼ .- Î¼â‚€
    return Î”'la.Pâ‚€*Î”
end

"""
    log_marginal_likelihood(la::BaseLaplace; Pâ‚€::Union{Nothing,UniformScaling}=nothing, Ïƒ::Union{Nothing, Real}=nothing)


"""
function log_marginal_likelihood(la::BaseLaplace; Pâ‚€::Union{Nothing,AbstractFloat,AbstractMatrix}=nothing, Ïƒ::Union{Nothing, Real}=nothing)

    # update prior precision:
    if !isnothing(Pâ‚€)
        la.Pâ‚€ = typeof(Pâ‚€) <: AbstractFloat ? UniformScaling(Pâ‚€)(la.n_params) : Pâ‚€
    end

    # update observation noise:
    if !isnothing(Ïƒ)
        @assert (la.likelihood==:regression || la.Ïƒ == Ïƒ) "Can only change observational noise Ïƒ for regression."
        la.Ïƒ = Ïƒ
    end 

    return log_likelihood(la) - 0.5 * (log_det_ratio(la) + _weight_penalty(la))
end

"""
    log_det_ratio(la::BaseLaplace)


"""
log_det_ratio(la::BaseLaplace) = log_det_posterior_precision(la) - log_det_prior_precision(la)

"""
    log_det_prior_precision(la::BaseLaplace)


"""
log_det_prior_precision(la::BaseLaplace) = sum(log.(diag(la.Pâ‚€)))

"""
    log_det_posterior_precision(la::BaseLaplace)


"""
log_det_posterior_precision(la::BaseLaplace) = logdet(posterior_precision(la))

"""
    hessian_approximation(la::BaseLaplace, d)

Computes the local Hessian approximation at a single data `d`.
"""
function hessian_approximation(la::BaseLaplace, d)
    loss, H = getfield(Curvature, la.hessian_structure)(la.curvature,d)
    return loss, H
end

"""
    fit!(la::BaseLaplace,data)

Fits the Laplace approximation for a data set.

# Examples

```julia-repl
using Flux, LaplaceRedux
x, y = LaplaceRedux.Data.toy_data_linear()
data = zip(x,y)
nn = Chain(Dense(2,1))
la = Laplace(nn)
fit!(la, data)
```

"""
function fit!(la::BaseLaplace, data; override::Bool=true)

    if override
        H = _init_H(la)
        loss = 0.0
        n_data = 0
    end

    # Training:
    for d in data
        loss_batch, H_batch = hessian_approximation(la, d)
        loss += loss_batch
        H += H_batch
        n_data += 1
    end

    # Store output:
    la.loss = loss                      # Loss
    la.H = H                            # Hessian
    la.P = posterior_precision(la)      # posterior precision
    la.Î£ = posterior_covariance(la)     # posterior covariance
    la.n_data = n_data                  # number of observations
    
end

"""
    glm_predictive_distribution(la::BaseLaplace, X::AbstractArray)

Computes the linearized GLM predictive.
"""
function glm_predictive_distribution(la::BaseLaplace, X::AbstractArray)
    ð‰, fÎ¼ = Curvature.jacobians(la.curvature,X)
    fvar = functional_variance(la,ð‰)
    fvar = reshape(fvar, size(fÎ¼)...)
    return fÎ¼, fvar
end

# Posterior predictions:
"""
    predict(la::BaseLaplace, X::AbstractArray; link_approx=:probit)

Computes predictions from Bayesian neural network.

# Examples

```julia-repl
using Flux, LaplaceRedux
x, y = toy_data_linear()
data = zip(x,y)
nn = Chain(Dense(2,1))
la = Laplace(nn)
fit!(la, data)
predict(la, hcat(x...))
```

"""
function predict(la::BaseLaplace, X::AbstractArray; link_approx=:probit)
    fÎ¼, fvar = glm_predictive_distribution(la, X)

    # Regression:
    if la.likelihood == :regression
        return fÎ¼, fvar
    end

    # Classification:
    if la.likelihood == :classification
        
        # Probit approximation
        if link_approx==:probit
            Îº = 1 ./ sqrt.(1 .+ Ï€/8 .* fvar) 
            z = Îº .* fÎ¼
        end

        if link_approx==:plugin
            z = fÎ¼
        end

        # Sigmoid/Softmax
        if outdim(la) == 1
            p = Flux.sigmoid(z)
        else
            p = Flux.softmax(z, dims=1)
        end

        return p
    end
end

"""
    (la::BaseLaplace)(X::AbstractArray; kwrgs...)

Calling a model with Laplace Approximation on an array of inputs is equivalent to explicitly calling the `predict` function.
"""
function (la::BaseLaplace)(X::AbstractArray; kwrgs...)
    return predict(la, X; kwrgs...)
end

"""
    optimize_prior!(
        la::BaseLaplace; 
        n_steps::Int=100, lr::Real=1e-1,
        Î»init::Union{Nothing,Real}=nothing,
        Ïƒinit::Union{Nothing,Real}=nothing
    )
    
Optimize the prior precision post-hoc through Empirical Bayes (marginal log-likelihood maximization).
"""
function optimize_prior!(
    la::BaseLaplace; 
    n_steps::Int=100, lr::Real=1e-1,
    Î»init::Union{Nothing,Real}=nothing,
    Ïƒinit::Union{Nothing,Real}=nothing,
    verbose::Bool=false,
    tune_Ïƒ::Bool=la.likelihood==:regression
)

    # Setup:
    logPâ‚€ = isnothing(Î»init) ? log.(unique(diag(la.Pâ‚€))) : log.([Î»init])   # prior precision (scalar)
    logÏƒ = isnothing(Ïƒinit) ? log.([la.Ïƒ]) : log.([Ïƒinit])                 # noise (scalar)
    opt = Adam(lr)
    show_every = round(n_steps/10)
    i = 0
    if tune_Ïƒ
        @assert la.likelihood == :regression "Observational noise Ïƒ tuning only applicable to regression."
        ps = Flux.params(logPâ‚€,logÏƒ)
    else
        if la.likelihood == :regression
            @warn "You have specified not to tune observational noise Ïƒ, even though this is a regression model. Are you sure you do not want to tune Ïƒ?"
        end
        ps = Flux.params(logPâ‚€)
    end
    loss(Pâ‚€,Ïƒ) = - log_marginal_likelihood(la; Pâ‚€=Pâ‚€[1], Ïƒ=Ïƒ[1])

    # Optimization:
    while i < n_steps
        gs = gradient(ps) do 
            loss(exp.(logPâ‚€), exp.(logÏƒ))
        end
        update!(opt, ps, gs)
        i += 1
        if verbose
            if i % show_every == 0
                @info "Iteration $(i): Pâ‚€=$(exp(logPâ‚€[1])), Ïƒ=$(exp(logÏƒ[1]))"
                @show loss(exp.(logPâ‚€), exp.(logÏƒ))
                println("Log likelihood: $(log_likelihood(la))")
                println("Log det ratio: $(log_det_ratio(la))")
                println("Scatter: $(_weight_penalty(la))")
            end
        end
    end

end



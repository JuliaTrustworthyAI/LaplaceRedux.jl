# Stochastic Gradient Descent:
function sgd(X,y,âˆ‡,w_0,H_0,Ï_0=1.0,T=10000,Îµ=0.001)
    # Initialization:
    N = length(y)
    w_t = w_0 # initial parameters (prior mode)
    w_hat = 1/T * w_t # iterate averaging
    Ï = Ï_0 # initial step size
    t = 0 # iteration count
    while t<T
        n_t = rand(1:N) # sample minibatch (single sample)
        Ï = Ï * exp(-Îµ*t) # exponential decay 
        w_t = w_t - Ï .* âˆ‡(w_t,w_0,X[n_t,:]',y[n_t],H_0) # update mode
        w_hat += 1/T * w_t # iterate averaging
        t += 1 # update count
    end
    return w_hat
end

# Newton's Method
function arminjo(ğ“, g_t, Î¸_t, d_t, args, Ï, c=1e-4)
    ğ“(Î¸_t .+ Ï .* d_t, args...) <= ğ“(Î¸_t, args...) .+ c .* Ï .* d_t'g_t
end

function newton(ğ“, Î¸, âˆ‡ğ“, âˆ‡âˆ‡ğ“, args; max_iter=100, Ï„=1e-5)
    # Intialize:
    converged = false # termination state
    t = 1 # iteration count
    Î¸_t = Î¸ # initial parameters
    # Descent:
    while !converged && t<max_iter 
        global g_t = âˆ‡ğ“(Î¸_t, args...) # gradient
        global H_t = âˆ‡âˆ‡ğ“(Î¸_t, args...) # hessian
        converged = all(abs.(g_t) .< Ï„) && isposdef(H_t) # check first-order condition
        # If not converged, descend:
        if !converged
            d_t = -inv(H_t)*g_t # descent direction
            # Line search:
            Ï_t = 1.0 # initialize at 1.0
            count = 1
            while !arminjo(ğ“, g_t, Î¸_t, d_t, args, Ï_t) 
                Ï_t /= 2
            end
            Î¸_t = Î¸_t .+ Ï_t .* d_t # update parameters
        end
        t += 1
    end
    # Output:
    return Î¸_t, H_t 
end
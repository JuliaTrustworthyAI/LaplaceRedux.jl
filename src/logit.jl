# Packages
using LinearAlgebra
include("optim.jl")

âˆ‘(vector)=sum(vector)

# Sigmoid function:
function ğ›”(a)
    trunc = 8.0 # truncation to avoid numerical over/underflow
    a = clamp.(a,-trunc,trunc)
    p = exp.(a)
    p = p ./ (1 .+ p)
    return p
end

# Negative log likelihood
function ğ“(w,w_0,H_0,X,y)
    N = length(y)
    D = size(X)[2]
    Î¼ = ğ›”(X*w)
    Î”w = w-w_0
    return - âˆ‘( y[n] * log(Î¼[n]) + (1-y[n]) * log(1-Î¼[n]) for n=1:N) + 1/2 * Î”w'H_0*Î”w
end

# Gradient:
function âˆ‡ğ“(w,w_0,H_0,X,y)
    N = length(y)
    Î¼ = ğ›”(X*w)
    Î”w = w-w_0
    g = âˆ‘((Î¼[n]-y[n]) * X[n,:] for n=1:N)
    return g + H_0*Î”w
end

# Hessian:
function âˆ‡âˆ‡ğ“(w,w_0,H_0,X,y)
    N = length(y)
    Î¼ = ğ›”(X*w)
    H = âˆ‘(Î¼[n] * (1-Î¼[n]) * X[n,:] * X[n,:]' for n=1:N)
    return H + H_0
end

# Main function:
struct BayesLogreg
    Î¼::Vector{Float64}
    Î£::Matrix{Float64}
end
function bayes_logreg(X,y;w_0=nothing,H_0=nothing,ğ“=ğ“,âˆ‡ğ“=âˆ‡ğ“,âˆ‡âˆ‡ğ“=âˆ‡âˆ‡ğ“,constant=true,Î»=0.005,optim_options...)
    # Setup:
    if constant
        if !all(X[:,1] .== 1)
            X = hcat(ones(size(X)[1]), X)
        else
        end
    end
    N, D = size(X)
    if isnothing(w_0)
        w_0 = zeros(D)
    end
    if isnothing(H_0)
        H_0 = UniformScaling(Î»)
    end
    
    # Model:
    w_map, H_map = newton(ğ“, w_0, âˆ‡ğ“, âˆ‡âˆ‡ğ“, (w_0=w_0, H_0=H_0, X=X, y=y), optim_options...) # fit the model (find mode of posterior distribution)
    Î£_map = inv(H_map) # inverse Hessian at the mode
    Î£_map = Symmetric(Î£_map) # to ensure matrix is Hermitian (i.e. avoid rounding issues)
    
    # Output:
    mod = BayesLogreg(w_map, Î£_map)
    return mod
end

#  ------------ Outer constructor methods: ------------
# Accessing fields:
Î¼(mod::BayesLogreg) = mod.Î¼
Î£(mod::BayesLogreg) = mod.Î£
# Coefficients:
coef(mod::BayesLogreg) = mod.Î¼ 

# Sampling from posterior distribution:
using Distributions
sample_posterior(mod::BayesLogreg, n) = rand(MvNormal(mod.Î¼, mod.Î£),n)
# Posterior predictions:
function predict(mod::BayesLogreg, X)
    Î¼ = mod.Î¼ # MAP mean vector
    Î£ = mod.Î£ # MAP covariance matrix
    if !isa(X, Matrix)
        X = reshape(X, 1, length(X))
    end
    # Inner product:
    z = X*Î¼
    # Probit approximation
    v = [X[n,:]'Î£*X[n,:] for n=1:size(X)[1]]
    Îº = 1 ./ sqrt.(1 .+ Ï€/8 .* v) 
    z = Îº .* z
    # Truncation to avoid numerical over/underflow:
    trunc = 8.0 
    z = clamp.(z,-trunc,trunc)
    p = exp.(z)
    p = p ./ (1 .+ p)
    return p
end

using AlgorithmicRecourse

"""
    retrain(ğ‘´::AlgorithmicRecourse.Models.BayesianLogisticModel, data; n_epochs=10) 

Retrains a fitted deep ensemble for (new) data.
"""
function retrain(ğ‘´::AlgorithmicRecourse.Models.LogisticModel, data; n_epochs=10, Ï„=1.0) 
    X = Flux.stack(map(d -> d[1], data),1)
    y = Flux.stack(map(d -> d[2], data),1)
    model = bayes_logreg(X, y)
    ğ‘´ = AlgorithmicRecourse.Models.LogisticModel(reshape(model.Î¼[2:end],1,length(model.Î¼)-1), [model.Î¼[1]]);
    return ğ‘´
end

"""
    retrain(ğ‘´::AlgorithmicRecourse.Models.BayesianLogisticModel, data; n_epochs=10) 

Retrains a fitted deep ensemble for (new) data.
"""
function retrain(ğ‘´::AlgorithmicRecourse.Models.BayesianLogisticModel, data; n_epochs=10, Ï„=1.0) 
    X = Flux.stack(map(d -> d[1], data),1)
    y = Flux.stack(map(d -> d[2], data),1)
    model = bayes_logreg(X, y)
    ğ‘´ = AlgorithmicRecourse.Models.BayesianLogisticModel(reshape(model.Î¼,1,length(model.Î¼)), model.Î£);
    return ğ‘´
end
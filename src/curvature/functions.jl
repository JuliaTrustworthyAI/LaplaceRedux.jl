using Flux
using ..LaplaceRedux: get_loss_fun, outdim
using LinearAlgebra
using Zygote
using Tullio

"Basetype for any curvature interface."
abstract type CurvatureInterface end

"""
    jacobians(curvature::CurvatureInterface, X::AbstractArray)

Computes the Jacobian `âˆ‡f(x;Î¸)` where `f: â„á´° â†¦ â„á´·`.
The Jacobian function can be used to compute the Jacobian of any function that supports automatic differentiation. 
Here, the nn function is wrapped in an anonymous function using the () -> syntax, which allows it to be differentiated using automatic differentiation.
"""

function jacobians(curvature::CurvatureInterface, X::AbstractArray)
    nn = curvature.model
    # Output:
    yÌ‚ = nn(X)
    # Jacobian:
    # Differentiate f with regards to the model parameters
    ğ‰ = jacobian(() -> nn(X), Flux.params(nn))
    # Concatenate Jacobians for the selected parameters, to produce a matrix (K, P), where P is the total number of parameter scalars.                      
    ğ‰ = reduce(hcat, [ğ‰[Î¸] for Î¸ âˆˆ curvature.params])
    return ğ‰, yÌ‚
end
end

"""
    gradients(curvature::CurvatureInterface, X::AbstractArray, y::Number)

Compute the gradients with respect to the loss function: `âˆ‡â„“(f(x;Î¸),y)` where `f: â„á´° â†¦ â„á´·`.
"""
function gradients(
    curvature::CurvatureInterface, X::AbstractArray, y::Union{Number,AbstractArray}
)::Zygote.Grads
    model = curvature.model
    ğ  = gradient(() -> curvature.loss_fun(X, y), Flux.params(model))           # compute the gradients of the loss function with respect to the model parameters
    return ğ 
end

"Constructor for Generalized Gauss Newton."
struct GGN <: CurvatureInterface
    model::Any
    likelihood::Symbol
    loss_fun::Function
    params::AbstractArray
    factor::Union{Nothing,Real}
end

function GGN(model::Any, likelihood::Symbol, params::AbstractArray)
    # Define loss function:
    loss_fun = get_loss_fun(likelihood, model)
    factor = likelihood == :regression ? 0.5 : 1.0

    return GGN(model, likelihood, loss_fun, params, factor)
end

"""
    full(curvature::GGN, d::Union{Tuple,NamedTuple})

Compute the full GGN.
"""
function full(curvature::GGN, d::Tuple)
    x, y = d

    loss = curvature.factor * curvature.loss_fun(x, y)

    ğ‰, fÎ¼ = jacobians(curvature, x)

    if curvature.likelihood == :regression
        H = ğ‰' * ğ‰
    else
        p = outdim(curvature.model) > 1 ? softmax(fÎ¼) : sigmoid(fÎ¼)
        H_lik = diagm(p) - p * p'
        H = ğ‰' * H_lik * ğ‰
    end

    return loss, H
end

    end

    return loss, H
end

"Constructor for Empirical Fisher."
struct EmpiricalFisher <: CurvatureInterface
    model::Any
    likelihood::Symbol
    loss_fun::Function
    params::AbstractArray
    factor::Union{Nothing,Real}
end

function EmpiricalFisher(model::Any, likelihood::Symbol, params::AbstractArray)

    # Define loss function:
    loss_fun = get_loss_fun(likelihood, model)
    factor = likelihood == :regression ? 0.5 : 1.0

    return EmpiricalFisher(model, likelihood, loss_fun, params, factor)
end

"""
    full(curvature::EmpiricalFisher, d::Union{Tuple,NamedTuple})

Compute the full empirical Fisher.
"""
function full(curvature::EmpiricalFisher, d::Tuple; batched::Bool=false)
    if batched
        full_batched(curvature, d)
    else
        full_unbatched(curvature, d)
    end
end

function full_unbatched(curvature::EmpiricalFisher, d::Tuple)
    x, y = d

    loss = curvature.factor * curvature.loss_fun(x, y)
    ğ  = gradients(curvature, x, y)
    # Concatenate the selected gradients into a vector, column-wise
    ğ  = reduce(vcat, [vec(ğ [Î¸]) for Î¸ in curvature.params])

    # Empirical Fisher:
    # - the product of the gradient vector with itself transposed
    H = ğ  * ğ '

    return loss, H
end

function full_batched(curvature::EmpiricalFisher, d::Tuple)
    x, y = d

    loss = curvature.factor * curvature.loss_fun(x, y)
    grads::Zygote.Grads = jacobian(
        () -> curvature.loss_fun(x, y; agg=identity), Flux.params(curvature.model)
    )
    ğ  = transpose(reduce(hcat, [grads[Î¸] for Î¸ in curvature.params]))

    # Empirical Fisher:
    # H = ğ  * ğ '
    @tullio H[i, j] := ğ [i, b] * ğ [j, b]

    return loss, H
end

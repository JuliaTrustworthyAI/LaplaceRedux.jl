{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Full, Last Layer and Subnetwork functionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\marka\\Documents\\VSCode\\Julia\\LaplaceRedux.jl`"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package Zygote not found in current path.\n- Run `import Pkg; Pkg.add(\"Zygote\")` to install the Zygote package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Zygote not found in current path.\n",
      "- Run `import Pkg; Pkg.add(\"Zygote\")` to install the Zygote package.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ .\\loading.jl:1163 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ .\\lock.jl:223 [inlined]\n",
      "  [3] require(into::Module, mod::Symbol)\n",
      "    @ Base .\\loading.jl:1144\n",
      "  [4] eval\n",
      "    @ .\\boot.jl:368 [inlined]\n",
      "  [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:1428\n",
      "  [6] #invokelatest#2\n",
      "    @ .\\essentials.jl:729 [inlined]\n",
      "  [7] invokelatest\n",
      "    @ .\\essentials.jl:726 [inlined]\n",
      "  [8] (::VSCodeServer.var\"#198#199\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\marka\\.vscode-insiders\\extensions\\julialang.language-julia-1.47.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:19\n",
      "  [9] withpath(f::VSCodeServer.var\"#198#199\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\marka\\.vscode-insiders\\extensions\\julialang.language-julia-1.47.2\\scripts\\packages\\VSCodeServer\\src\\repl.jl:249\n",
      " [10] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\marka\\.vscode-insiders\\extensions\\julialang.language-julia-1.47.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      " [11] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\marka\\.vscode-insiders\\extensions\\julialang.language-julia-1.47.2\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      " [12] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\marka\\.vscode-insiders\\extensions\\julialang.language-julia-1.47.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:139\n",
      " [13] top-level scope\n",
      "    @ c:\\Users\\marka\\.vscode-insiders\\extensions\\julialang.language-julia-1.47.2\\scripts\\notebook\\notebook.jl:32"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: gradient\n",
    "using LaplaceRedux\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using Statistics\n",
    "using Zygote\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip([[3.3094690003501204, 1.105015603790398], [4.785206519692352, 2.302865692853331], [4.5978141179509695, 1.6641979932455095], [1.2039502150385224, 2.4143042079779162], [3.9535143318937274, 3.2749292394892926], [4.888228161913915, 4.798296383810684], [3.731562431596806, 4.8429509015736185], [0.7728393706989826, 2.5543798302040344], [4.734407081878071, 1.6562880772227127], [4.5508107698894085, 3.007820551587571]  …  [-4.001911977451673, 1.4413371112018547], [-1.401450781550467, 2.993436092989832], [-4.562711925033253, 2.145276692360886], [-0.662226465500714, 2.5663224474138024], [-1.1473346035389773, 2.490975960799372], [-3.9262243689688914, 4.612646203151002], [-3.2833710659982778, 4.90971289047463], [-3.141620549880873, 2.352464396233791], [-4.6898831470302955, 1.0777769882777402], [-2.5031164212937167, 1.389913242038404]], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs, ys = LaplaceRedux.Data.toy_data_non_linear(200)\n",
    "X = hcat(xs...) # bring into tabular format\n",
    "data = zip(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params(["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.20560294 -0.69944406; 0.40646386 -0.33234173; -0.41245437 0.3885508; -0.29035503 0.23656149; -0.30834043 -0.1253471; 0.19812028 -0.14759278; -0.6857486 -0.08098635; -0.6883295 0.56313396; -0.4344271 -0.18774693; -0.4647752 -0.50013113], "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.16986336 -0.23157422 -0.67837626 0.43372405 0.0011231505 0.26485175 -0.39528698 -0.025584798 0.49851653 0.39710125], Float32[0.0]])"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_hidden = 10\n",
    "D = size(X,1)\n",
    "nn = Chain(\n",
    "    Dense(D, n_hidden, σ),\n",
    "    Dense(n_hidden, 1)\n",
    ")\n",
    "println(D)\n",
    "print(Flux.params(nn))\n",
    "loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(2 => 10, σ)\n",
      "│   summary(x) = 2-element Vector{Float64}\n",
      "└ @ Flux C:\\Users\\marka\\.julia\\packages\\Flux\\FWgS0\\src\\layers\\stateless.jl:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_loss(data) = 0.6622597560286522"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20\n",
      "avg_loss(data) = 0.5813140459358692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30\n",
      "avg_loss(data) = 0.46266817778348923\n",
      "Epoch 40\n",
      "avg_loss(data) = 0.3480928037315607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "avg_loss(data) = 0.2602370107546449"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60\n",
      "avg_loss(data) = 0.1983439323492348\n",
      "Epoch 70\n",
      "avg_loss(data) = 0.15517515078186989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80\n",
      "avg_loss(data) = 0.12445215816609562\n",
      "Epoch 90\n",
      "avg_loss(data) = 0.10208293938543647"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100\n",
      "avg_loss(data) = 0.0855055350321345\n"
     ]
    }
   ],
   "source": [
    "using Flux.Optimise: update!, Adam\n",
    "opt = Adam(1e-3)\n",
    "epochs = 100\n",
    "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
    "show_every = epochs/10\n",
    "\n",
    "for epoch = 1:epochs\n",
    "  for d in data\n",
    "    gs = gradient(Flux.params(nn)) do\n",
    "      l = loss(d...)\n",
    "    end\n",
    "    update!(opt, Flux.params(nn), gs)\n",
    "  end\n",
    "  if epoch % show_every == 0\n",
    "    println(\"Epoch \" * string(epoch))\n",
    "    @show avg_loss(data)\n",
    "  end\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LA for Full Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(nn; likelihood=:classification, subset_of_weights=:all)\n",
    "fit!(la, data)\n",
    "\n",
    "la_untuned = deepcopy(la)   # saving for plotting\n",
    "optimize_prior!(la; verbose=true, n_steps=500)\n",
    "\n",
    "zoom=0\n",
    "println(\"...\")\n",
    "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\n",
    "p_untuned = plot(la_untuned, X, ys; title=\"LA - raw (λ=$(unique(diag(la_untuned.P₀))[1]))\", clim=(0,1), zoom=zoom)\n",
    "p_laplace = plot(la, X, ys; title=\"LA - tuned (λ=$(round(unique(diag(la.P₀))[1],digits=2)))\", clim=(0,1), zoom=zoom)\n",
    "plot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LA for Last Layer of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(nn; likelihood=:classification, subset_of_weights=:last_layer)\n",
    "fit!(la, data)\n",
    "\n",
    "la_untuned = deepcopy(la)   # saving for plotting\n",
    "optimize_prior!(la; verbose=true, n_steps=500)\n",
    "\n",
    "zoom=0\n",
    "println(\"...\")\n",
    "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\n",
    "p_untuned = plot(la_untuned, X, ys; title=\"LA - raw (λ=$(unique(diag(la_untuned.P₀))[1]))\", clim=(0,1), zoom=zoom)\n",
    "p_laplace = plot(la, X, ys; title=\"LA - tuned (λ=$(round(unique(diag(la.P₀))[1],digits=2)))\", clim=(0,1), zoom=zoom)\n",
    "plot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LA for Subset of Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot for tuned Laplace, by selecting just 4 of the 41 weights in the neural network provided in this notebook, we can see that there is not much information from the gradients and the jacobian that Laplace Approximation can use in order to adjust its predictive uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(nn; likelihood=:classification, subset_of_weights=:subnetwork, subnetwork_indices=[[3, 1, 1], [2, 8], [2, 9], [2, 10]])\n",
    "fit!(la, data)\n",
    "\n",
    "la_untuned = deepcopy(la)   # saving for plotting\n",
    "optimize_prior!(la; verbose=true, n_steps=500)\n",
    "\n",
    "zoom=0\n",
    "println(\"...\")\n",
    "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\n",
    "p_untuned = plot(la_untuned, X, ys; title=\"LA - raw (λ=$(unique(diag(la_untuned.P₀))[1]))\", clim=(0,1), zoom=zoom)\n",
    "p_laplace = plot(la, X, ys; title=\"LA - tuned (λ=$(round(unique(diag(la.P₀))[1],digits=2)))\", clim=(0,1), zoom=zoom)\n",
    "plot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When providing all 41 weights, the uncertainties are much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(nn; likelihood=:classification, subset_of_weights=:subnetwork, subnetwork_indices=[[1, 2, 1], [1, 1, 1], [1, 1, 2], [1, 2, 2], [1, 3, 1], [1, 3, 2], [1, 4, 1], [1, 4, 2], [1, 5, 1], [1, 5, 2], [1, 6, 1], [1, 6, 2], [1, 7, 1], [1, 7, 2], [1, 8, 1], [1, 8, 2], [1, 9, 1], [1, 9, 2], [1, 10, 1], [1, 10, 2], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [3, 1, 1], [3, 1, 2], [3, 1, 3], [3, 1, 4], [3, 1, 5], [3, 1, 6], [3, 1, 7], [3, 1, 8], [3, 1, 9], [3, 1, 10], [4, 1]])\n",
    "fit!(la, data)\n",
    "\n",
    "la_untuned = deepcopy(la)   # saving for plotting\n",
    "optimize_prior!(la; verbose=true, n_steps=500)\n",
    "\n",
    "zoom=0\n",
    "println(\"...\")\n",
    "p_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\n",
    "p_untuned = plot(la_untuned, X, ys; title=\"LA - raw (λ=$(unique(diag(la_untuned.P₀))[1]))\", clim=(0,1), zoom=zoom)\n",
    "p_laplace = plot(la, X, ys; title=\"LA - tuned (λ=$(round(unique(diag(la.P₀))[1],digits=2)))\", clim=(0,1), zoom=zoom)\n",
    "plot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `C:\\Users\\Pasqu\\Documents\\julia_projects\\LaplaceRedux.jl\\docs`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `C:\\Users\\Pasqu\\Documents\\julia_projects\\LaplaceRedux.jl\\docs\\Project.toml`\n",
      "  \u001b[90m[324d7699] \u001b[39mCategoricalArrays v0.10.8\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.6.1\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.111\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[e30172f5] \u001b[39mDocumenter v1.6.0\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[587475ba] \u001b[39mFlux v0.14.19\n",
      "  \u001b[90m[c52c1a26] \u001b[39mLaplaceRedux v1.1.1 `C:\\Users\\Pasqu\\Documents\\julia_projects\\LaplaceRedux.jl`\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[094fc8d1] \u001b[39mMLJFlux v0.5.1\n",
      "  \u001b[90m[e80e1ace] \u001b[39mMLJModelInterface v1.11.0\n",
      "  \u001b[90m[3bd65402] \u001b[39mOptimisers v0.3.3\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.8\n",
      "  \u001b[90m[ce6b1742] \u001b[39mRDatasets v0.7.7\n",
      "  \u001b[90m[860ef19b] \u001b[39mStableRNGs v1.0.2\n",
      "  \u001b[90m[2913bbd2] \u001b[39mStatsBase v0.34.3\n",
      "  \u001b[90m[bd369af6] \u001b[39mTables v1.12.0\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[bd7198b4] \u001b[39mTaijaPlotting v1.2.0\n",
      "  \u001b[90m[592b5752] \u001b[39mTrapz v2.0.3\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[e88e6eb3] \u001b[39mZygote v0.6.70\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[02a925ec] \u001b[39mcuDNN v1.3.2\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.10.0\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available. Those with \u001b[32m⌃\u001b[39m may be upgradable, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using MLJBase.predict in module LaplaceRedux conflicts with an existing identifier.\n",
      "WARNING: using MLJBase.fit! in module LaplaceRedux conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "\n",
    "Pkg.activate(\"C:/Users/Pasqu/Documents/julia_projects/LaplaceRedux.jl/docs\")\n",
    "Pkg.status()\n",
    "using LaplaceRedux\n",
    "using MLJBase\n",
    "using Random\n",
    "using DataFrames\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tables.MatrixTable{Matrix{Float64}} with 100 rows, 2 columns, and schema:\n",
       " :x1  Float64\n",
       " :x2  Float64, [-0.15053240354230857, -0.16143107735113107, -0.28104782384528254, 0.8905842690519058, -0.2716955057136559, 0.9606721208381163, 0.14403243794060133, 0.13743002853667605, 0.820641892942472, 0.2270783932443115  …  0.4639933046961763, 0.30449384622096687, 0.2744588755171263, -31.785240173822324, 2.58951832655098, 0.10969223924903307, 0.1600255529817666, 0.5913011997917647, -0.39253898214541977, -0.7247243478011852])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = make_regression(100, 2; noise=0.5, sparse=0.2, outliers=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "using Tables\n",
    "# Get the number of rows\n",
    "num_rows = Tables.rows(X) |> length\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = Tables.columnnames(X) |> length\n",
    "\n",
    "# Display the dimensions\n",
    "println(\"Number of rows: \", num_rows)\n",
    "println(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(2 => 10, relu),                 \u001b[90m# 30 parameters\u001b[39m\n",
       "  Dense(10 => 10, relu),                \u001b[90m# 110 parameters\u001b[39m\n",
       "  Dense(10 => 1),                       \u001b[90m# 11 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m151 parameters, 988 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux\n",
    "# Define the model\n",
    "flux_model = Chain(\n",
    "    Dense(2, 10, relu),\n",
    "    Dense(10, 10, relu),\n",
    "    Dense(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaplaceRegressor(\n",
       "  model = Chain(Dense(2 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), \n",
       "  flux_loss = Flux.Losses.mse, \n",
       "  optimiser = Adam(0.001, (0.9, 0.999), 1.0e-8), \n",
       "  epochs = 1000, \n",
       "  batch_size = 32, \n",
       "  subset_of_weights = :all, \n",
       "  subnetwork_indices = nothing, \n",
       "  hessian_structure = :full, \n",
       "  backend = :GGN, \n",
       "  σ = 1.0, \n",
       "  μ₀ = 0.0, \n",
       "  P₀ = nothing, \n",
       "  fit_prior_nsteps = 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create an instance of LaplaceRegressor with the Flux model\n",
    "laplace_regressor = LaplaceRegressor(\n",
    "    model = flux_model,\n",
    "    subset_of_weights = :all,\n",
    "    subnetwork_indices = nothing,\n",
    "    hessian_structure = :full,\n",
    "    backend = :GGN,\n",
    "    σ = 1.0,\n",
    "    μ₀ = 0.0,\n",
    "    P₀ = nothing,\n",
    "    fit_prior_nsteps = 100\n",
    ")\n",
    "\n",
    "# Display the LaplaceRegressor object\n",
    "laplace_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tables.MatrixTable{Matrix{Float64}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "typeof(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>17×5 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">x1</th><th style = \"text-align: left;\">x2</th><th style = \"text-align: left;\">x3</th><th style = \"text-align: left;\">x4</th><th style = \"text-align: left;\">y</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"CategoricalArrays.CategoricalValue{Int64, UInt32}\" style = \"text-align: left;\">Cat…</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">5.89419</td><td style = \"text-align: right;\">12.9979</td><td style = \"text-align: right;\">6.18512</td><td style = \"text-align: right;\">8.99286</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">-8.14834</td><td style = \"text-align: right;\">6.23246</td><td style = \"text-align: right;\">-1.68497</td><td style = \"text-align: right;\">8.96905</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">-4.88229</td><td style = \"text-align: right;\">5.35276</td><td style = \"text-align: right;\">-0.45876</td><td style = \"text-align: right;\">8.20756</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">4.02585</td><td style = \"text-align: right;\">6.94769</td><td style = \"text-align: right;\">13.4032</td><td style = \"text-align: right;\">-0.0419223</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">-5.53635</td><td style = \"text-align: right;\">6.55656</td><td style = \"text-align: right;\">-1.67063</td><td style = \"text-align: right;\">8.77041</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">-6.61858</td><td style = \"text-align: right;\">4.65032</td><td style = \"text-align: right;\">-1.15198</td><td style = \"text-align: right;\">8.34897</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">10.2344</td><td style = \"text-align: right;\">11.4278</td><td style = \"text-align: right;\">13.0544</td><td style = \"text-align: right;\">8.53025</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">-6.05052</td><td style = \"text-align: right;\">8.12027</td><td style = \"text-align: right;\">-3.68708</td><td style = \"text-align: right;\">8.78732</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">-5.06769</td><td style = \"text-align: right;\">4.8631</td><td style = \"text-align: right;\">-3.58346</td><td style = \"text-align: right;\">8.41371</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">10.8373</td><td style = \"text-align: right;\">6.32472</td><td style = \"text-align: right;\">9.79163</td><td style = \"text-align: right;\">6.65962</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">-6.63226</td><td style = \"text-align: right;\">5.45149</td><td style = \"text-align: right;\">-0.38861</td><td style = \"text-align: right;\">9.0007</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">1.62812</td><td style = \"text-align: right;\">4.61073</td><td style = \"text-align: right;\">11.6602</td><td style = \"text-align: right;\">11.7241</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">-6.48679</td><td style = \"text-align: right;\">6.68166</td><td style = \"text-align: right;\">-3.32569</td><td style = \"text-align: right;\">9.19618</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">14</td><td style = \"text-align: right;\">7.95596</td><td style = \"text-align: right;\">2.23928</td><td style = \"text-align: right;\">12.6897</td><td style = \"text-align: right;\">1.77857</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">15</td><td style = \"text-align: right;\">-6.36466</td><td style = \"text-align: right;\">5.82985</td><td style = \"text-align: right;\">-0.702502</td><td style = \"text-align: right;\">8.44976</td><td style = \"text-align: left;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">16</td><td style = \"text-align: right;\">8.03294</td><td style = \"text-align: right;\">3.85901</td><td style = \"text-align: right;\">5.50741</td><td style = \"text-align: right;\">2.2014</td><td style = \"text-align: left;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">17</td><td style = \"text-align: right;\">-7.45067</td><td style = \"text-align: right;\">7.01011</td><td style = \"text-align: right;\">-1.96187</td><td style = \"text-align: right;\">7.84336</td><td style = \"text-align: left;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& x1 & x2 & x3 & x4 & y\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.89419 & 12.9979 & 6.18512 & 8.99286 & 2 \\\\\n",
       "\t2 & -8.14834 & 6.23246 & -1.68497 & 8.96905 & 1 \\\\\n",
       "\t3 & -4.88229 & 5.35276 & -0.45876 & 8.20756 & 1 \\\\\n",
       "\t4 & 4.02585 & 6.94769 & 13.4032 & -0.0419223 & 2 \\\\\n",
       "\t5 & -5.53635 & 6.55656 & -1.67063 & 8.77041 & 1 \\\\\n",
       "\t6 & -6.61858 & 4.65032 & -1.15198 & 8.34897 & 1 \\\\\n",
       "\t7 & 10.2344 & 11.4278 & 13.0544 & 8.53025 & 2 \\\\\n",
       "\t8 & -6.05052 & 8.12027 & -3.68708 & 8.78732 & 1 \\\\\n",
       "\t9 & -5.06769 & 4.8631 & -3.58346 & 8.41371 & 1 \\\\\n",
       "\t10 & 10.8373 & 6.32472 & 9.79163 & 6.65962 & 2 \\\\\n",
       "\t11 & -6.63226 & 5.45149 & -0.38861 & 9.0007 & 1 \\\\\n",
       "\t12 & 1.62812 & 4.61073 & 11.6602 & 11.7241 & 2 \\\\\n",
       "\t13 & -6.48679 & 6.68166 & -3.32569 & 9.19618 & 1 \\\\\n",
       "\t14 & 7.95596 & 2.23928 & 12.6897 & 1.77857 & 2 \\\\\n",
       "\t15 & -6.36466 & 5.82985 & -0.702502 & 8.44976 & 1 \\\\\n",
       "\t16 & 8.03294 & 3.85901 & 5.50741 & 2.2014 & 2 \\\\\n",
       "\t17 & -7.45067 & 7.01011 & -1.96187 & 7.84336 & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m17×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x1       \u001b[0m\u001b[1m x2       \u001b[0m\u001b[1m x3        \u001b[0m\u001b[1m x4         \u001b[0m\u001b[1m y    \u001b[0m\n",
       "     │\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat… \u001b[0m\n",
       "─────┼─────────────────────────────────────────────────\n",
       "   1 │  5.89419  12.9979    6.18512    8.99286    2\n",
       "   2 │ -8.14834   6.23246  -1.68497    8.96905    1\n",
       "   3 │ -4.88229   5.35276  -0.45876    8.20756    1\n",
       "   4 │  4.02585   6.94769  13.4032    -0.0419223  2\n",
       "   5 │ -5.53635   6.55656  -1.67063    8.77041    1\n",
       "   6 │ -6.61858   4.65032  -1.15198    8.34897    1\n",
       "   7 │ 10.2344   11.4278   13.0544     8.53025    2\n",
       "   8 │ -6.05052   8.12027  -3.68708    8.78732    1\n",
       "   9 │ -5.06769   4.8631   -3.58346    8.41371    1\n",
       "  10 │ 10.8373    6.32472   9.79163    6.65962    2\n",
       "  11 │ -6.63226   5.45149  -0.38861    9.0007     1\n",
       "  12 │  1.62812   4.61073  11.6602    11.7241     2\n",
       "  13 │ -6.48679   6.68166  -3.32569    9.19618    1\n",
       "  14 │  7.95596   2.23928  12.6897     1.77857    2\n",
       "  15 │ -6.36466   5.82985  -0.702502   8.44976    1\n",
       "  16 │  8.03294   3.85901   5.50741    2.2014     2\n",
       "  17 │ -7.45067   7.01011  -1.96187    7.84336    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLJ, DataFrames\n",
    "X, y = make_blobs(100, 4; centers=2, cluster_std=[1.0, 3.0 ])\n",
    "dfBlobs = DataFrame(X)\n",
    "dfBlobs.y = y\n",
    "first(dfBlobs, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4 => 10, relu),                 \u001b[90m# 50 parameters\u001b[39m\n",
       "  Dense(10 => 10, relu),                \u001b[90m# 110 parameters\u001b[39m\n",
       "  Dense(10 => 2),                       \u001b[90m# 22 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m182 parameters, 1.086 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux\n",
    "# Define the model\n",
    "flux_model = Chain(\n",
    "    Dense(4, 10, relu),\n",
    "    Dense(10, 10, relu),\n",
    "    Dense(10, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaplaceClassifier(\n",
       "  model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 2)), \n",
       "  flux_loss = Flux.Losses.logitcrossentropy, \n",
       "  optimiser = Adam(0.001, (0.9, 0.999), 1.0e-8), \n",
       "  epochs = 1000, \n",
       "  batch_size = 32, \n",
       "  subset_of_weights = :all, \n",
       "  subnetwork_indices = nothing, \n",
       "  hessian_structure = :full, \n",
       "  backend = :GGN, \n",
       "  σ = 1.0, \n",
       "  μ₀ = 0.0, \n",
       "  P₀ = nothing, \n",
       "  fit_prior_nsteps = 100, \n",
       "  link_approx = :probit)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of LaplaceRegressor with the Flux model\n",
    "laplace_classifier = LaplaceClassifier(\n",
    "    model = flux_model,\n",
    "    subset_of_weights = :all,\n",
    "    subnetwork_indices = nothing,\n",
    "    hessian_structure = :full,\n",
    "    backend = :GGN,\n",
    "    σ = 1.0,\n",
    "    μ₀ = 0.0,\n",
    "    P₀ = nothing,\n",
    "    fit_prior_nsteps = 100\n",
    ")\n",
    "\n",
    "# Display the LaplaceRegressor object\n",
    "laplace_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5.894190948229072 -8.148338397036424 … 4.6901216310216896 8.754850004556681; 12.99791729188348 6.232458473633034 … 10.904369405648966 12.187158715203248; 6.185122057812649 -1.6849673572899138 … 10.09295616393206 15.616135680604215; 8.992861668498659 8.969052840985539 … 8.673387526088336 3.444822170655238], (Bool[1 0 … 1 1; 0 1 … 0 0], CategoricalArrays.CategoricalValue{Int64, UInt32} 2))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_reformat,y_test_reformat= MLJBase.reformat(laplace_classifier,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×100 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " 1  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1  ⋅  …  1  1  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  1\n",
       " ⋅  1  1  ⋅  1  1  ⋅  1  1  ⋅  1  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  1  1  1  1  1  ⋅  ⋅"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_reformat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 view(OneHotMatrix(::Vector{UInt32}), :, [2, 3, 4]) with eltype Bool:\n",
       " 0  0  1\n",
       " 1  1  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(y_test_reformat[1],:, [2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 view(::Matrix{Float64}, :, [2, 3, 4]) with eltype Float64:\n",
       " -8.14834  -4.88229   4.02585\n",
       "  6.23246   5.35276   6.94769\n",
       " -1.68497  -0.45876  13.4032\n",
       "  8.96905   8.20756  -0.0419223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(X_test_reformat, :, [2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((sepal_length = [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9  …  6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9], sepal_width = [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1  …  3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0], petal_length = [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5  …  5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1], petal_width = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1  …  2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2.0, 2.3, 1.8]), CategoricalArrays.CategoricalValue{String, UInt32}[\"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"  …  \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"virginica\", \"virginica\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLJ\n",
    "#DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n",
    "\n",
    "using Flux\n",
    "# Define the model\n",
    "flux_model = Chain(\n",
    "    Dense(4, 10, relu),\n",
    "    Dense(10, 10, relu),\n",
    "    Dense(10, 3)\n",
    ")\n",
    "\n",
    "model = LaplaceClassifier(model=flux_model)\n",
    "\n",
    "X, y = @load_iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{String}:\n",
       " \"setosa\"\n",
       " \"versicolor\"\n",
       " \"virginica\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "levels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(LaplaceClassifier(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 3)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:499\n",
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(4 => 10, relu)\n",
      "│   summary(x) = 4×32 Matrix{Float64}\n",
      "└ @ Flux C:\\Users\\Pasqu\\.julia\\packages\\Flux\\HBF2N\\src\\layers\\stateless.jl:60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss: 4.042439937591553 \n",
      "Epoch 200: Loss: 3.533539354801178 \n",
      "Epoch 300: Loss: 3.2006053030490875 \n",
      "Epoch 400: Loss: 2.9201304614543915 \n",
      "Epoch 500: Loss: 2.647124856710434 \n",
      "Epoch 600: Loss: 2.3577273190021515 \n",
      "Epoch 700: Loss: 2.0284294933080673 \n",
      "Epoch 800: Loss: 1.6497879326343536 \n",
      "Epoch 900: Loss: 1.3182911276817322 \n",
      "Epoch 1000: Loss: 1.077766239643097 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LaplaceClassifier(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 3)), …)\n",
       "  args: \n",
       "    1:\tSource @378 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @187 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mach = machine(model, X, y) |> MLJBase.fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.9488185211171959\n",
       " 0.7685919895442062\n",
       " 0.9454937120287679"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xnew = (sepal_length = [6.4, 7.2, 7.4],\n",
    "        sepal_width = [2.8, 3.0, 2.8],\n",
    "        petal_length = [5.6, 5.8, 6.1],\n",
    "        petal_width = [2.1, 1.6, 1.9],)\n",
    "yhat = MLJBase.predict(mach, Xnew) # probabilistic predictions\n",
    "predict_mode(mach, Xnew)   # point predictions\n",
    "pdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV(\n",
       "  nfolds = 3, \n",
       "  shuffle = false, \n",
       "  rng = TaskLocalRNG())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "cv = CV(nfolds=3)\n",
    "CV(\n",
    "  nfolds = 3,\n",
    "  shuffle = false,\n",
    "  rng = Random.default_rng())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(μ = Float32[0.36796558, -0.590969, -0.0005966219, -0.1918786, -0.21046183, 0.33172563, -0.6454487, -0.43057427, 0.09267368, -0.31252357  …  0.31747338, -0.42893136, -0.036560666, 0.12848923, -0.022290554, -0.17562295, 0.10335993, 2.0584264, -0.27860576, -0.9749556],\n",
       " H = [19221.80854034424 0.0 … 175.01133728027344 416.5687526464462; 0.0 0.0 … 0.0 0.0; … ; 175.01133728027344 0.0 … 150.0 0.0; 416.5687526464462 0.0 … 0.0 150.0],\n",
       " P = [19222.80854034424 0.0 … 175.01133728027344 416.5687526464462; 0.0 1.0 … 0.0 0.0; … ; 175.01133728027344 0.0 … 151.0 0.0; 416.5687526464462 0.0 … 0.0 151.0],\n",
       " Σ = [0.4942412474220449 0.0 … 0.00012692197672594825 -0.00029493662610333626; 0.0 1.0 … -0.0 -0.0; … ; 0.00012692197672495198 0.0 … 0.019500175331854216 2.549733600297664e-5; -0.000294936626108007 0.0 … 2.5497336002974752e-5 0.019426448013349775],\n",
       " n_data = 160,\n",
       " n_params = 193,\n",
       " n_out = 3,\n",
       " loss = 76.21374633908272,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLJBase.fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100: Loss: 0.9005963057279587 \n",
      "Epoch 1200: Loss: 0.7674828618764877 \n",
      "Epoch 1300: Loss: 0.6668129041790962 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating machine(LaplaceClassifier(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 3)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1400: Loss: 0.5900264084339142 \n",
      "Epoch 1500: Loss: 0.5311783701181412 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LaplaceClassifier(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 3)), …)\n",
       "  args: \n",
       "    1:\tSource @378 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @187 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.epochs= 1500\n",
    "uff=MLJBase.fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " updating only the laplace optimization part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating machine(LaplaceClassifier(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 3)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LaplaceClassifier(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 3)), …)\n",
       "  args: \n",
       "    1:\tSource @378 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @187 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit_prior_nsteps = 200\n",
    "uff=MLJBase.fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss: 123.99619626934783 \n",
      "Epoch 200: Loss: 110.70134709925446 \n",
      "Epoch 300: Loss: 101.54836914035226 \n",
      "Epoch 400: Loss: 93.59849316803738 \n",
      "Epoch 500: Loss: 85.18775669356168 \n",
      "Epoch 600: Loss: 75.76182416873147 \n",
      "Epoch 700: Loss: 67.35201676247976 \n",
      "Epoch 800: Loss: 61.22682561405186 \n",
      "Epoch 900: Loss: 56.388587609986764 \n",
      "Epoch 1000: Loss: 51.8525101259686 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " -2.0295  -1.03661  1.29323"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLJ\n",
    "#LaplaceRegressor = @load LaplaceRegressor pkg=LaplaceRedux\n",
    "flux_model = Chain(\n",
    "    Dense(4, 10, relu),\n",
    "    Dense(10, 10, relu),\n",
    "    Dense(10, 1)\n",
    ")\n",
    "model = LaplaceRegressor(model=flux_model)\n",
    "\n",
    "X, y = make_regression(100, 4; noise=0.5, sparse=0.2, outliers=0.1)\n",
    "mach = machine(model, X, y) \n",
    "uff=MLJBase.fit!(mach)\n",
    "Xnew, _ = make_regression(3, 4; rng=123)\n",
    "yhat = MLJBase.predict(mach, Xnew) # probabilistic predictions\n",
    "MLJBase.predict_mode(mach, Xnew)   # point predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(μ = Float32[1.0187618, 1.1801981, 1.5047036, 0.09397529, -1.0688609, -0.59974676, 1.2977643, -1.8645017, -0.6493797, 0.8598072  …  -3.1050463, -2.267896, 1.018275, -2.0316908, 0.72548866, -2.7309833, -2.1938443, 0.8200029, 1.2517836, 0.3313799],\n",
       " H = [8124.873962402344 1238.1352081298828 … 1980.7237396240234 324.216007232666; 1238.1352081298828 496.2319145202637 … 451.8493309020996 62.39444828033447; … ; 1980.7237396240234 451.8493309020996 … 1277.6469421386719 212.83043670654297; 324.216007232666 62.39444828033447 … 212.83043670654297 100.0],\n",
       " P = [8125.873962402344 1238.1352081298828 … 1980.7237396240234 324.216007232666; 1238.1352081298828 497.2319145202637 … 451.8493309020996 62.39444828033447; … ; 1980.7237396240234 451.8493309020996 … 1278.6469421386719 212.83043670654297; 324.216007232666 62.39444828033447 … 212.83043670654297 101.0],\n",
       " Σ = [0.08818398027482557 0.01068994964485469 … -0.028911073548363073 0.0029380249323247053; 0.01068994964484666 0.3740392430965252 … -0.03321343036791264 0.03808189340124685; … ; -0.02891107354841918 -0.03321343036788035 … 0.37993018751981145 -0.007007167070441715; 0.002938024932320219 0.03808189340124439 … -0.007007167070367749 0.8779610764649773],\n",
       " n_data = 128,\n",
       " n_params = 171,\n",
       " n_out = 1,\n",
       " loss = 25.89132467732224,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLJBase.fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Vector{Any}:\n",
       " 132.4757012476366\n",
       " 132.34841820836348\n",
       " 132.2594675536756\n",
       " 132.1764167781873\n",
       " 132.09720400291872\n",
       " 132.02269947440837\n",
       " 131.95014241005182\n",
       " 131.87929837827625\n",
       " 131.8104252672236\n",
       " 131.74319014461662\n",
       "   ⋮\n",
       "  52.175198365270234\n",
       "  52.148401306572445\n",
       "  52.10149557862212\n",
       "  52.032551867920525\n",
       "  51.99690913470439\n",
       "  51.994054497037524\n",
       "  51.97321907106968\n",
       "  51.932579915942064\n",
       "  51.8525101259686"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a= MLJBase.training_losses(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating machine(LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100: Loss: 47.37399118448011 \n",
      "Epoch 1200: Loss: 42.67007994813126 \n",
      "Epoch 1300: Loss: 38.057605481177234 \n",
      "Epoch 1400: Loss: 33.846777755430416 \n",
      "Epoch 1500: Loss: 29.514834265471144 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …)\n",
       "  args: \n",
       "    1:\tSource @447 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @645 ⏎ AbstractVector{Continuous}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.epochs= 1500\n",
    "uff=MLJBase.fit!(mach)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500-element Vector{Any}:\n",
       " 132.4757012476366\n",
       " 132.34841820836348\n",
       " 132.2594675536756\n",
       " 132.1764167781873\n",
       " 132.09720400291872\n",
       " 132.02269947440837\n",
       " 131.95014241005182\n",
       " 131.87929837827625\n",
       " 131.8104252672236\n",
       " 131.74319014461662\n",
       "   ⋮\n",
       "  29.979098347946184\n",
       "  29.896050775635096\n",
       "  29.83609799111873\n",
       "  29.763058304997937\n",
       "  29.69908817371154\n",
       "  29.646864259231556\n",
       "  29.623977677653713\n",
       "  29.549562760589527\n",
       "  29.514834265471144"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLJBase.training_losses(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of epochs inserted is lower than the number of epochs already been trained. No update is necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating machine(LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …)\n",
       "  args: \n",
       "    1:\tSource @447 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @645 ⏎ AbstractVector{Continuous}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.epochs= 1200\n",
    "uff=MLJBase.fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " updating only the laplace optimization part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating machine(LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …), …).\n",
      "└ @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LaplaceRegressor(model = Chain(Dense(4 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1)), …)\n",
       "  args: \n",
       "    1:\tSource @447 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @645 ⏎ AbstractVector{Continuous}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit_prior_nsteps = 200\n",
    "uff=MLJBase.fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJBase.selectrows(model, I, Xmatrix, y) = (view(Xmatrix, :, I), view(y[1], I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLJBase.evaluate(model, X, y, resampling=cv, measure=l2, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 4×100 Matrix{Float64} at index [[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], 1:100]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 4×100 Matrix{Float64} at index [[35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], 1:100]\n",
      "\n",
      "Stacktrace:\n",
      "  [1] throw_boundserror(A::Matrix{Float64}, I::Tuple{Vector{Int64}, Base.Slice{Base.OneTo{Int64}}})\n",
      "    @ Base .\\abstractarray.jl:737\n",
      "  [2] checkbounds\n",
      "    @ .\\abstractarray.jl:702 [inlined]\n",
      "  [3] _getindex\n",
      "    @ .\\multidimensional.jl:888 [inlined]\n",
      "  [4] getindex\n",
      "    @ .\\abstractarray.jl:1291 [inlined]\n",
      "  [5] _selectrows(::MLJModelInterface.FullInterface, ::Val{:other}, X::Matrix{Float64}, r::Vector{Int64})\n",
      "    @ MLJModelInterface C:\\Users\\Pasqu\\.julia\\packages\\MLJModelInterface\\y9x5A\\src\\data_utils.jl:350\n",
      "  [6] selectrows\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJModelInterface\\y9x5A\\src\\data_utils.jl:340 [inlined]\n",
      "  [7] selectrows\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJModelInterface\\y9x5A\\src\\data_utils.jl:336 [inlined]\n",
      "  [8] #29\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJModelInterface\\y9x5A\\src\\model_api.jl:88 [inlined]\n",
      "  [9] map\n",
      "    @ .\\tuple.jl:292 [inlined]\n",
      " [10] selectrows(::LaplaceRegressor, ::Vector{Int64}, ::Matrix{Float64}, ::Tuple{Matrix{Float64}, Nothing})\n",
      "    @ MLJModelInterface C:\\Users\\Pasqu\\.julia\\packages\\MLJModelInterface\\y9x5A\\src\\model_api.jl:88\n",
      " [11] fit_only!(mach::Machine{LaplaceRegressor, LaplaceRegressor, true}; rows::Vector{Int64}, verbosity::Int64, force::Bool, composite::Nothing)\n",
      "    @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:676\n",
      " [12] fit_only!\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:617 [inlined]\n",
      " [13] #fit!#63\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:789 [inlined]\n",
      " [14] fit!\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\machines.jl:786 [inlined]\n",
      " [15] fit_and_extract_on_fold\n",
      "    @ C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\resampling.jl:1463 [inlined]\n",
      " [16] (::MLJBase.var\"#277#278\"{MLJBase.var\"#fit_and_extract_on_fold#304\"{Vector{Tuple{Vector{Int64}, UnitRange{Int64}}}, Nothing, Nothing, Int64, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, Vector{typeof(predict_mean)}, Bool, Bool, Vector{Float64}}, Machine{LaplaceRegressor, LaplaceRegressor, true}, Int64})(k::Int64)\n",
      "    @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\resampling.jl:1289\n",
      " [17] _mapreduce(f::MLJBase.var\"#277#278\"{MLJBase.var\"#fit_and_extract_on_fold#304\"{Vector{Tuple{Vector{Int64}, UnitRange{Int64}}}, Nothing, Nothing, Int64, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, Vector{typeof(predict_mean)}, Bool, Bool, Vector{Float64}}, Machine{LaplaceRegressor, LaplaceRegressor, true}, Int64}, op::typeof(vcat), ::IndexLinear, A::UnitRange{Int64})\n",
      "    @ Base .\\reduce.jl:440\n",
      " [18] _mapreduce_dim\n",
      "    @ .\\reducedim.jl:365 [inlined]\n",
      " [19] mapreduce\n",
      "    @ .\\reducedim.jl:357 [inlined]\n",
      " [20] _evaluate!(func::MLJBase.var\"#fit_and_extract_on_fold#304\"{Vector{Tuple{Vector{Int64}, UnitRange{Int64}}}, Nothing, Nothing, Int64, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, Vector{typeof(predict_mean)}, Bool, Bool, Vector{Float64}}, mach::Machine{LaplaceRegressor, LaplaceRegressor, true}, ::CPU1{Nothing}, nfolds::Int64, verbosity::Int64)\n",
      "    @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\resampling.jl:1288\n",
      " [21] evaluate!(mach::Machine{LaplaceRegressor, LaplaceRegressor, true}, resampling::Vector{Tuple{Vector{Int64}, UnitRange{Int64}}}, weights::Nothing, class_weights::Nothing, rows::Nothing, verbosity::Int64, repeats::Int64, measures::Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, operations::Vector{typeof(predict_mean)}, acceleration::CPU1{Nothing}, force::Bool, per_observation_flag::Bool, logger::Nothing, user_resampling::CV, compact::Bool)\n",
      "    @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\resampling.jl:1510\n",
      " [22] evaluate!(::Machine{LaplaceRegressor, LaplaceRegressor, true}, ::CV, ::Nothing, ::Nothing, ::Nothing, ::Int64, ::Int64, ::Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, ::Vector{typeof(predict_mean)}, ::CPU1{Nothing}, ::Bool, ::Bool, ::Nothing, ::CV, ::Bool)\n",
      "    @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\resampling.jl:1603\n",
      " [23] evaluate!(mach::Machine{LaplaceRegressor, LaplaceRegressor, true}; resampling::CV, measures::Nothing, measure::StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}, weights::Nothing, class_weights::Nothing, operations::Nothing, operation::Nothing, acceleration::CPU1{Nothing}, rows::Nothing, repeats::Int64, force::Bool, check_measure::Bool, per_observation::Bool, verbosity::Int64, logger::Nothing, compact::Bool)\n",
      "    @ MLJBase C:\\Users\\Pasqu\\.julia\\packages\\MLJBase\\7nGJF\\src\\resampling.jl:1232\n",
      " [24] top-level scope\n",
      "    @ c:\\Users\\Pasqu\\Documents\\julia_projects\\LaplaceRedux.jl\\dev\\notebooks\\mlj-interfacing\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X54sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "evaluate!(mach, resampling=cv, measure=l2, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flux_model = Chain(\n",
    "    #Dense(4, 10, relu),\n",
    "    #Dense(10, 10, relu),\n",
    "    #Dense(10, 1))\n",
    "\n",
    "\n",
    "#nested_flux_model = Chain(\n",
    "    #Chain(Dense(10, 5, relu), Dense(5, 5, relu)),\n",
    "    #Chain(Dense(5, 3, relu), Dense(3, 2)))\n",
    "#model = LaplaceRegressor()\n",
    "#model.model= nested_flux_model\n",
    "\n",
    "#copy_model= deepcopy(model)\n",
    "\n",
    "#copy_model.epochs= 2000\n",
    "#copy_model.optimiser = Descent()\n",
    "#MLJBase.is_same_except(model , copy_model,:epochs )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding a neural network from a JSON file to Python, then encoding the hessian from laplace.py to a CSV (multi-class classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Read the dataset\n",
    "- Step 2: Initialize model\n",
    "- Step 3: Read the weights and biases from the JSON file, then load them into the previously initialized model\n",
    "- Step 4: Generate the hessians and write them to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from laplace import Laplace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(43)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data from csv\n",
    "\n",
    "# Load data from CSV file using pandas\n",
    "df = pd.read_csv('data_multi.csv')\n",
    "\n",
    "# Split the dataframe into x and y tensors\n",
    "x = torch.from_numpy(df[['x1', 'x2']].to_numpy()).to(torch.float32)\n",
    "y = torch.from_numpy(df['y'].to_numpy(dtype=int))\n",
    "\n",
    "X = x.T\n",
    "\n",
    "y_unique = torch.unique(y)\n",
    "y_indices = y - 1\n",
    "y_indices = y_indices.long()\n",
    "y_train = nn.functional.one_hot(y_indices, num_classes=len(y_unique)).float()\n",
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "\n",
    "data = list(zip(x, y_train))\n",
    "n_hidden = 3\n",
    "D = X.shape[0]  # == 2\n",
    "out_dim = y_train.shape[1]  # == 4\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, n_hidden),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(n_hidden, out_dim)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Read the weights and biases from the JSON file, then load them into the previously initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': [2.2643619, 1.4519827, -0.82282156],\n",
       "  'weight': [[4.5985866, -0.33530173, -0.3286117],\n",
       "   [0.020469662, 1.8550323, -1.9706149]]},\n",
       " {'bias': [-2.3268585, 0.75011677, -2.5603814, 0.76309216],\n",
       "  'weight': [[6.1197248, -8.305814, 6.425951, -8.373602],\n",
       "   [-0.5828485, -6.455521, -9.15029, 3.3358324],\n",
       "   [-9.106588, 3.1897228, -0.39506513, -6.322449]]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('nn_multi.json') as fin:\n",
    "    nn_json_str = fin.read()\n",
    "    nn_json = json.loads(nn_json_str)\n",
    "\n",
    "nn_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model.state_dict()) == 2 * len(nn_json)\n",
    "iter_states = iter(model.state_dict())\n",
    "\n",
    "# for layer in model.state_dict():\n",
    "#     print(layer)\n",
    "for layer_read in nn_json:\n",
    "    state_w = next(iter_states)\n",
    "    state_b = next(iter_states)\n",
    "    tensor_w = torch.tensor(layer_read['weight']).T\n",
    "    tensor_b = torch.tensor(layer_read['bias']).T\n",
    "    model.state_dict()[state_w].data.copy_(tensor_w)\n",
    "    model.state_dict()[state_b].data.copy_(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5986,  0.0205],\n",
       "        [-0.3353,  1.8550],\n",
       "        [-0.3286, -1.9706]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2644,  1.4520, -0.8228])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.1197, -0.5828, -9.1066],\n",
       "        [-8.3058, -6.4555,  3.1897],\n",
       "        [ 6.4260, -9.1503, -0.3951],\n",
       "        [-8.3736,  3.3358, -6.3224]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.3269,  0.7501, -2.5604,  0.7631])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for val in model.state_dict().values():\n",
    "    display(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.argmax(torch.softmax(model.forward(x), dim=1), dim=1) + 1\n",
    "\n",
    "y_hat == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3.1009, -13.7871,  -5.0352,  -4.4544],\n",
      "        [  3.1989, -13.9695,  -5.2335,  -4.3036],\n",
      "        [  3.2100, -14.0110,  -5.2845,  -4.2748],\n",
      "        [  2.9709, -13.5555,  -4.7860,  -4.6491],\n",
      "        [  3.2099, -14.0098,  -5.2828,  -4.2755],\n",
      "        [  3.2044, -13.9704,  -5.2317,  -4.3004],\n",
      "        [  3.2034, -13.9785,  -5.2434,  -4.2963],\n",
      "        [  3.2090, -13.8829,  -5.1094,  -4.3472],\n",
      "        [  3.2101, -14.0105,  -5.2837,  -4.2751],\n",
      "        [  3.2091, -14.0099,  -5.2853,  -4.2737],\n",
      "        [  3.1903, -13.8480,  -5.0714,  -4.3762],\n",
      "        [  3.1848, -13.8804,  -5.1186,  -4.3607],\n",
      "        [  3.2097, -14.0104,  -5.2839,  -4.2751],\n",
      "        [  3.2099, -14.0109,  -5.2847,  -4.2746],\n",
      "        [  3.2099, -14.0104,  -5.2837,  -4.2752],\n",
      "        [  3.2100, -14.0108,  -5.2842,  -4.2749],\n",
      "        [  3.2058, -14.0051,  -5.2866,  -4.2714],\n",
      "        [  3.2106, -13.9959,  -5.2634,  -4.2830],\n",
      "        [  3.2269, -13.1274,  -4.0649,  -4.7627],\n",
      "        [  3.2079, -14.0082,  -5.2861,  -4.2725],\n",
      "        [  3.1916, -13.7749,  -4.9706,  -4.4166],\n",
      "        [  3.2101, -14.0050,  -5.2761,  -4.2781],\n",
      "        [  3.2100, -14.0098,  -5.2828,  -4.2755],\n",
      "        [  3.2100, -14.0105,  -5.2838,  -4.2751],\n",
      "        [  3.2130, -13.7429,  -4.9156,  -4.4239],\n",
      "        [  3.2094, -14.0023,  -5.2728,  -4.2800],\n",
      "        [  3.2099, -14.0098,  -5.2829,  -4.2755],\n",
      "        [  3.2082, -14.0083,  -5.2848,  -4.2738],\n",
      "        [  3.2100, -14.0106,  -5.2838,  -4.2750],\n",
      "        [  3.2088, -14.0051,  -5.2770,  -4.2787],\n",
      "        [  3.2051, -14.0019,  -5.2767,  -4.2800],\n",
      "        [  3.2159, -13.6980,  -4.8525,  -4.4477],\n",
      "        [  3.2104, -13.9975,  -5.2658,  -4.2822],\n",
      "        [  3.1714, -13.7201,  -4.9060,  -4.4574],\n",
      "        [  3.1222, -13.7191,  -4.9305,  -4.4824],\n",
      "        [  3.1352, -13.8114,  -5.0501,  -4.4241],\n",
      "        [  3.2068, -13.9843,  -5.2495,  -4.2914],\n",
      "        [  3.1960, -13.9888,  -5.2673,  -4.2876],\n",
      "        [  3.2099, -14.0101,  -5.2833,  -4.2754],\n",
      "        [  3.2063, -13.9387,  -5.1873,  -4.3173],\n",
      "        [  3.2086, -13.6149,  -4.7424,  -4.4980],\n",
      "        [  3.2092, -14.0093,  -5.2830,  -4.2756],\n",
      "        [  3.2100, -14.0107,  -5.2839,  -4.2750],\n",
      "        [  3.2089, -14.0080,  -5.2810,  -4.2769],\n",
      "        [  3.2088, -14.0091,  -5.2845,  -4.2743],\n",
      "        [  3.2110, -13.9879,  -5.2522,  -4.2873],\n",
      "        [  3.2096, -14.0069,  -5.2790,  -4.2773],\n",
      "        [  3.2030, -13.9811,  -5.2471,  -4.2951],\n",
      "        [  3.2086, -13.9581,  -5.2127,  -4.3052],\n",
      "        [  3.1117, -13.6491,  -4.8400,  -4.5270],\n",
      "        [-11.4339,   3.9320,  -2.9664,  -5.5552],\n",
      "        [-11.4518,   3.6968,  -3.2980,  -5.4320],\n",
      "        [-11.4397,   3.8277,  -3.1123,  -5.4995],\n",
      "        [-11.4707,   3.4623,  -3.6293,  -5.3096],\n",
      "        [-11.4662,   3.5531,  -3.5024,  -5.3584],\n",
      "        [-11.4239,   3.4700,  -3.5930,  -5.2920],\n",
      "        [-11.4399,   3.8620,  -3.0654,  -5.5189],\n",
      "        [-11.4335,   3.9394,  -2.9560,  -5.5591],\n",
      "        [-11.4452,   3.7993,  -3.1541,  -5.4863],\n",
      "        [-11.4455,   3.7906,  -3.1663,  -5.4815],\n",
      "        [-11.4336,   3.9382,  -2.9578,  -5.5585],\n",
      "        [-11.4335,   3.9394,  -2.9561,  -5.5591],\n",
      "        [-11.4342,   3.9316,  -2.9671,  -5.5551],\n",
      "        [-11.4335,   3.9394,  -2.9560,  -5.5592],\n",
      "        [-11.4387,   3.8766,  -3.0448,  -5.5265],\n",
      "        [-11.4333,   3.9391,  -2.9560,  -5.5593],\n",
      "        [-11.4334,   3.9305,  -2.9677,  -5.5547],\n",
      "        [-11.4353,   3.9155,  -2.9898,  -5.5466],\n",
      "        [-11.4899,   3.2568,  -3.9209,  -5.2038],\n",
      "        [-11.4466,   3.7184,  -3.2657,  -5.4416],\n",
      "        [-11.4339,   3.9340,  -2.9637,  -5.5563],\n",
      "        [-11.4411,   3.5923,  -3.4356,  -5.3680],\n",
      "        [-11.4335,   3.9393,  -2.9562,  -5.5591],\n",
      "        [-11.4474,   3.6776,  -3.3221,  -5.4190],\n",
      "        [-11.4344,   3.9013,  -3.0087,  -5.5383],\n",
      "        [-11.4338,   3.9346,  -2.9628,  -5.5566],\n",
      "        [-11.4353,   3.9174,  -2.9871,  -5.5477],\n",
      "        [-11.5245,   2.6686,  -4.7449,  -4.8907],\n",
      "        [-11.4335,   3.9392,  -2.9563,  -5.5590],\n",
      "        [-11.4348,   3.9167,  -2.9879,  -5.5470],\n",
      "        [-11.4386,   3.8419,  -3.0923,  -5.5069],\n",
      "        [-11.4333,   3.9234,  -2.9770,  -5.5509],\n",
      "        [-11.4338,   3.9353,  -2.9618,  -5.5570],\n",
      "        [-11.4380,   3.8801,  -3.0397,  -5.5281],\n",
      "        [-11.4865,   3.2650,  -3.9078,  -5.2067],\n",
      "        [-11.4366,   3.8869,  -3.0296,  -5.5312],\n",
      "        [-11.4337,   3.9365,  -2.9602,  -5.5576],\n",
      "        [-11.4550,   3.5828,  -3.4559,  -5.3696],\n",
      "        [-11.4361,   3.8717,  -3.0502,  -5.5224],\n",
      "        [-11.4335,   3.9394,  -2.9561,  -5.5591],\n",
      "        [-11.4475,   3.7258,  -3.2560,  -5.4461],\n",
      "        [-11.4356,   3.8836,  -3.0336,  -5.5289],\n",
      "        [-11.4335,   3.9393,  -2.9561,  -5.5591],\n",
      "        [-11.4336,   3.9374,  -2.9589,  -5.5581],\n",
      "        [-11.4345,   3.9277,  -2.9727,  -5.5530],\n",
      "        [-11.4344,   3.9271,  -2.9734,  -5.5527],\n",
      "        [-11.4338,   3.9354,  -2.9617,  -5.5570],\n",
      "        [-11.4339,   3.9260,  -2.9745,  -5.5519],\n",
      "        [-11.4370,   3.8960,  -3.0174,  -5.5365],\n",
      "        [-11.4334,   3.9393,  -2.9561,  -5.5591],\n",
      "        [ -5.2256,  -4.4800,   3.3535, -13.8174],\n",
      "        [ -5.3121,  -4.3668,   3.4703, -13.9317],\n",
      "        [ -5.3136,  -4.3661,   3.4704, -13.9328],\n",
      "        [ -5.3122,  -4.3674,   3.4692, -13.9313],\n",
      "        [ -5.3118,  -4.3683,   3.4676, -13.9299],\n",
      "        [ -5.3132,  -4.3664,   3.4702, -13.9324],\n",
      "        [ -5.1428,  -4.4556,   3.4372, -13.7976],\n",
      "        [ -5.3098,  -4.3689,   3.4685, -13.9294],\n",
      "        [ -5.3131,  -4.3664,   3.4703, -13.9324],\n",
      "        [ -5.2433,  -4.3955,   3.4669, -13.8813],\n",
      "        [ -5.3117,  -4.3669,   3.4702, -13.9314],\n",
      "        [ -5.2136,  -4.4987,   3.3319, -13.7985],\n",
      "        [ -5.3134,  -4.3662,   3.4704, -13.9327],\n",
      "        [ -5.2995,  -4.3720,   3.4697, -13.9225],\n",
      "        [ -5.1580,  -4.5139,   3.3490, -13.7720],\n",
      "        [ -5.2866,  -4.3784,   3.4677, -13.9125],\n",
      "        [ -5.3052,  -4.3771,   3.4567, -13.9191],\n",
      "        [ -5.3118,  -4.3673,   3.4697, -13.9312],\n",
      "        [ -5.3049,  -4.3704,   3.4691, -13.9261],\n",
      "        [ -5.3133,  -4.3664,   3.4698, -13.9322],\n",
      "        [ -5.2028,  -4.4519,   3.4108, -13.8295],\n",
      "        [ -5.2162,  -4.4783,   3.3651, -13.8187],\n",
      "        [ -5.3056,  -4.3694,   3.4701, -13.9270],\n",
      "        [ -5.3068,  -4.3712,   3.4670, -13.9266],\n",
      "        [ -5.3125,  -4.3670,   3.4697, -13.9317],\n",
      "        [ -5.3036,  -4.3704,   3.4698, -13.9254],\n",
      "        [ -5.3086,  -4.3690,   3.4690, -13.9287],\n",
      "        [ -4.6732,  -4.6473,   3.4202, -13.4563],\n",
      "        [ -5.3093,  -4.3684,   3.4694, -13.9294],\n",
      "        [ -5.3136,  -4.3660,   3.4705, -13.9329],\n",
      "        [ -5.3133,  -4.3663,   3.4702, -13.9325],\n",
      "        [ -5.2989,  -4.3785,   3.4611, -13.9185],\n",
      "        [ -5.3136,  -4.3661,   3.4705, -13.9328],\n",
      "        [ -5.1429,  -4.4896,   3.3905, -13.7785],\n",
      "        [ -5.3047,  -4.3707,   3.4688, -13.9258],\n",
      "        [ -5.3154,  -4.3637,   3.4686, -13.9305],\n",
      "        [ -5.2009,  -4.4454,   3.4207, -13.8322],\n",
      "        [ -5.1522,  -4.4396,   3.4541, -13.8112],\n",
      "        [ -5.3134,  -4.3662,   3.4704, -13.9327],\n",
      "        [ -5.3136,  -4.3661,   3.4704, -13.9328],\n",
      "        [ -5.3086,  -4.3685,   3.4697, -13.9290],\n",
      "        [ -5.0142,  -4.5454,   3.3814, -13.6831],\n",
      "        [ -5.3082,  -4.3696,   3.4685, -13.9282],\n",
      "        [ -5.3136,  -4.3660,   3.4705, -13.9328],\n",
      "        [ -5.3128,  -4.3668,   3.4698, -13.9320],\n",
      "        [ -5.3106,  -4.3697,   3.4661, -13.9283],\n",
      "        [ -5.3141,  -4.3655,   3.4699, -13.9323],\n",
      "        [ -5.3134,  -4.3663,   3.4700, -13.9324],\n",
      "        [ -5.2888,  -4.3846,   3.4581, -13.9101],\n",
      "        [ -5.0722,  -4.4647,   3.4617, -13.7573],\n",
      "        [ -3.0275,  -5.6629, -11.7140,   4.0164],\n",
      "        [ -3.6971,  -5.3691, -11.6616,   3.5179],\n",
      "        [ -3.2066,  -5.5934, -11.7126,   3.8883],\n",
      "        [ -2.9234,  -5.7003, -11.7108,   4.0893],\n",
      "        [ -3.2394,  -5.5848, -11.7179,   3.8672],\n",
      "        [ -2.9366,  -5.6954, -11.7110,   4.0799],\n",
      "        [ -3.0150,  -5.6665, -11.7124,   4.0247],\n",
      "        [ -2.9104,  -5.7051, -11.7106,   4.0984],\n",
      "        [ -3.1726,  -5.6112, -11.7192,   3.9152],\n",
      "        [ -2.9135,  -5.7039, -11.7106,   4.0962],\n",
      "        [ -5.1064,  -4.9231, -11.7883,   2.5666],\n",
      "        [ -2.9271,  -5.6991, -11.7112,   4.0867],\n",
      "        [ -2.9100,  -5.7054, -11.7105,   4.0985],\n",
      "        [ -2.9169,  -5.7025, -11.7105,   4.0937],\n",
      "        [ -3.0204,  -5.6640, -11.7118,   4.0206],\n",
      "        [ -3.0250,  -5.6617, -11.7112,   4.0170],\n",
      "        [ -3.0406,  -5.6517, -11.7055,   4.0036],\n",
      "        [ -2.9187,  -5.7020, -11.7107,   4.0925],\n",
      "        [ -3.6607,  -5.4027, -11.6889,   3.5553],\n",
      "        [ -2.9963,  -5.6744, -11.7135,   4.0384],\n",
      "        [ -2.9097,  -5.7057, -11.7104,   4.0984],\n",
      "        [ -2.9286,  -5.6974, -11.7094,   4.0849],\n",
      "        [ -2.9103,  -5.7052, -11.7107,   4.0985],\n",
      "        [ -3.4016,  -5.5027, -11.6903,   3.7403],\n",
      "        [ -2.9151,  -5.7035, -11.7109,   4.0952],\n",
      "        [ -2.9116,  -5.7047, -11.7107,   4.0976],\n",
      "        [ -3.1092,  -5.6308, -11.7129,   3.9578],\n",
      "        [ -2.9165,  -5.7029, -11.7108,   4.0941],\n",
      "        [ -2.9376,  -5.6941, -11.7098,   4.0787],\n",
      "        [ -3.1314,  -5.6256, -11.7174,   3.9438],\n",
      "        [ -3.7609,  -5.3978, -11.7347,   3.5027],\n",
      "        [ -3.0343,  -5.6598, -11.7134,   4.0113],\n",
      "        [ -3.0798,  -5.6442, -11.7159,   3.9799],\n",
      "        [ -2.9204,  -5.7011, -11.7104,   4.0912],\n",
      "        [ -2.9302,  -5.6978, -11.7110,   4.0845],\n",
      "        [ -3.0158,  -5.6665, -11.7129,   4.0243],\n",
      "        [ -2.9102,  -5.7052, -11.7106,   4.0985],\n",
      "        [ -3.2187,  -5.5936, -11.7192,   3.8824],\n",
      "        [ -2.9164,  -5.7028, -11.7107,   4.0942],\n",
      "        [ -2.9117,  -5.7046, -11.7105,   4.0974],\n",
      "        [ -2.9109,  -5.7050, -11.7107,   4.0981],\n",
      "        [ -3.9188,  -5.3308, -11.7254,   3.3864],\n",
      "        [ -2.9413,  -5.6937, -11.7111,   4.0766],\n",
      "        [ -3.0779,  -5.6445, -11.7152,   3.9811],\n",
      "        [ -3.3308,  -5.5502, -11.7184,   3.8022],\n",
      "        [ -2.9110,  -5.7050, -11.7104,   4.0977],\n",
      "        [ -2.9278,  -5.6976, -11.7088,   4.0848],\n",
      "        [ -2.9117,  -5.7046, -11.7105,   4.0973],\n",
      "        [ -4.2396,  -5.2310, -11.7566,   3.1708],\n",
      "        [ -3.5322,  -5.4717, -11.7162,   3.6580]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate the hessians and write them to CSV files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In this case of regression, it is no longer necessary to convert the format of the hessian to match the one from PyTorch. This is because both the input and output layers have size == 1. Therefore all weights and biases are vectors, and for a vector x: vec(x) == vec(transpose(x))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='all',\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_all_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='last_layer',\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_ll_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='subnetwork',\n",
    "             subnetwork_indices = torch.LongTensor([0, 5, 6, 8, 9, 20, 21, 24]),\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_subnet_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace.curvature import AsdlEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='all',\n",
    "             hessian_structure='full', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_all_full_empfisher.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='last_layer',\n",
    "             hessian_structure='full', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_ll_full_empfisher.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='subnetwork',\n",
    "             subnetwork_indices = torch.LongTensor([0, 5, 6, 8, 9, 20, 21, 24]),\n",
    "             hessian_structure='full', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_subnet_full_empfisher.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unavailable through Backpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlaplace\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcurvature\u001b[39;00m \u001b[39mimport\u001b[39;00m BackPackEF\n\u001b[0;32m      2\u001b[0m la \u001b[39m=\u001b[39m Laplace(model, \u001b[39m'\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m              subset_of_weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m              hessian_structure\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkron\u001b[39m\u001b[39m'\u001b[39m, backend\u001b[39m=\u001b[39mBackPackEF)\n\u001b[1;32m----> 6\u001b[0m la\u001b[39m.\u001b[39;49mfit(DataLoader(TensorDataset(x, y_train), batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m hessian \u001b[39m=\u001b[39m la\u001b[39m.\u001b[39mH\n\u001b[0;32m      9\u001b[0m torch\u001b[39m.\u001b[39mset_printoptions(precision\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, sci_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\adeli\\OneDrive\\Desktop\\facultate\\2nd year\\Q4 - Software Project\\LaplaceRedux.jl\\myenv\\lib\\site-packages\\laplace\\baselaplace.py:797\u001b[0m, in \u001b[0;36mKronLaplace.fit\u001b[1;34m(self, train_loader, override)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[39m# discount previous Kronecker factors to sum up properly together with new ones\u001b[39;00m\n\u001b[0;32m    795\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH_facs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rescale_factors(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH_facs, n_data_old \u001b[39m/\u001b[39m (n_data_old \u001b[39m+\u001b[39m n_data_new))\n\u001b[1;32m--> 797\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(train_loader, override\u001b[39m=\u001b[39;49moverride)\n\u001b[0;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH_facs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH_facs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH\n",
      "File \u001b[1;32mc:\\Users\\adeli\\OneDrive\\Desktop\\facultate\\2nd year\\Q4 - Software Project\\LaplaceRedux.jl\\myenv\\lib\\site-packages\\laplace\\baselaplace.py:377\u001b[0m, in \u001b[0;36mParametricLaplace.fit\u001b[1;34m(self, train_loader, override)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    376\u001b[0m X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device), y\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[1;32m--> 377\u001b[0m loss_batch, H_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_curv_closure(X, y, N)\n\u001b[0;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_batch\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m H_batch\n",
      "File \u001b[1;32mc:\\Users\\adeli\\OneDrive\\Desktop\\facultate\\2nd year\\Q4 - Software Project\\LaplaceRedux.jl\\myenv\\lib\\site-packages\\laplace\\baselaplace.py:777\u001b[0m, in \u001b[0;36mKronLaplace._curv_closure\u001b[1;34m(self, X, y, N)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_curv_closure\u001b[39m(\u001b[39mself\u001b[39m, X, y, N):\n\u001b[1;32m--> 777\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49mkron(X, y, N\u001b[39m=\u001b[39;49mN)\n",
      "File \u001b[1;32mc:\\Users\\adeli\\OneDrive\\Desktop\\facultate\\2nd year\\Q4 - Software Project\\LaplaceRedux.jl\\myenv\\lib\\site-packages\\laplace\\curvature\\backpack.py:156\u001b[0m, in \u001b[0;36mBackPackEF.kron\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkron\u001b[39m(\u001b[39mself\u001b[39m, X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnavailable through Backpack.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Unavailable through Backpack."
     ]
    }
   ],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='all',\n",
    "             hessian_structure='kron', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_all_kron_empfisher.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='last_layer',\n",
    "             hessian_structure='kron', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_ll_kron_empfisher.csv', array, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

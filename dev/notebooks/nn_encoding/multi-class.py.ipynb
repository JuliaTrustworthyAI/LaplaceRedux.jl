{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding a neural network from a JSON file to Python, then encoding the hessian from laplace.py to a JSON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from laplace import Laplace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(43)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data from csv\n",
    "\n",
    "# Load data from CSV file using pandas\n",
    "df = pd.read_csv('data_multi.csv')\n",
    "\n",
    "# Split the dataframe into x and y tensors\n",
    "x = torch.from_numpy(df[['x1', 'x2']].to_numpy()).to(torch.float32)\n",
    "y = torch.from_numpy(df['y'].to_numpy(dtype=int))\n",
    "\n",
    "X = x.T\n",
    "\n",
    "y_unique = torch.unique(y)\n",
    "y_indices = y - 1\n",
    "y_indices = y_indices.long()\n",
    "y_train = nn.functional.one_hot(y_indices, num_classes=len(y_unique)).float()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "\n",
    "data = list(zip(x, y_train))\n",
    "n_hidden = 3\n",
    "D = X.shape[0]  # == 2\n",
    "out_dim = y_train.shape[1]  # == 4\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, n_hidden),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(n_hidden, out_dim)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': [-2.0739872, 1.6585876, -0.7064312],\n",
       "  'weight': [[-4.0660324, 0.29628262, 0.3625705],\n",
       "   [0.03851904, 1.4500277, -2.256552]]},\n",
       " {'bias': [1.3787575, -2.5149424, 0.47881675, -2.0565474],\n",
       "  'weight': [[-8.137701, 6.033078, -9.181937, 5.5667276],\n",
       "   [2.4892216, -8.89387, -5.512849, -0.49369964],\n",
       "   [-7.280996, -0.25884467, 4.017399, -8.365101]]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('nn_multi.json') as fin:\n",
    "    nn_json_str = fin.read()\n",
    "    nn_json = json.loads(nn_json_str)\n",
    "\n",
    "nn_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adeli\\AppData\\Local\\Temp\\ipykernel_4916\\3361615766.py:10: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  tensor_b = torch.tensor(layer_read['bias']).T\n"
     ]
    }
   ],
   "source": [
    "assert len(model.state_dict()) == 2 * len(nn_json)\n",
    "iter_states = iter(model.state_dict())\n",
    "\n",
    "# for layer in model.state_dict():\n",
    "#     print(layer)\n",
    "for layer_read in nn_json:\n",
    "    state_w = next(iter_states)\n",
    "    state_b = next(iter_states)\n",
    "    tensor_w = torch.tensor(layer_read['weight']).T\n",
    "    tensor_b = torch.tensor(layer_read['bias']).T\n",
    "    model.state_dict()[state_w].data.copy_(tensor_w)\n",
    "    model.state_dict()[state_b].data.copy_(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0660,  0.0385],\n",
       "        [ 0.2963,  1.4500],\n",
       "        [ 0.3626, -2.2566]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.0740,  1.6586, -0.7064])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-8.1377,  2.4892, -7.2810],\n",
       "        [ 6.0331, -8.8939, -0.2588],\n",
       "        [-9.1819, -5.5128,  4.0174],\n",
       "        [ 5.5667, -0.4937, -8.3651]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3788, -2.5149,  0.4788, -2.0565])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for val in model.state_dict().values():\n",
    "    display(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.argmax(torch.softmax(model.forward(x), dim=1), dim=1) + 1\n",
    "\n",
    "y_hat == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3.8487, -11.4021,  -5.0200,  -2.5696],\n",
      "        [  3.8476, -11.4050,  -5.0207,  -2.5719],\n",
      "        [  3.8174, -11.3840,  -4.9939,  -2.5984],\n",
      "        [  3.8586, -11.3980,  -5.0351,  -2.5483],\n",
      "        [  3.5138, -11.3866,  -4.8226,  -2.9442],\n",
      "        [  3.5241, -11.2808,  -4.7814,  -2.8916],\n",
      "        [  3.8641, -11.4066,  -5.0308,  -2.5538],\n",
      "        [  3.8285, -11.3935,  -5.0046,  -2.5893],\n",
      "        [  3.8600, -11.4020,  -5.0417,  -2.5452],\n",
      "        [  2.6337, -11.3735,  -4.3165,  -3.9387],\n",
      "        [  3.8672, -11.4083,  -5.0334,  -2.5509],\n",
      "        [  3.8631, -11.4032,  -5.0290,  -2.5535],\n",
      "        [  3.8552, -11.3950,  -5.0225,  -2.5580],\n",
      "        [  3.8673, -11.4084,  -5.0334,  -2.5509],\n",
      "        [  3.7627, -11.3534,  -4.9498,  -2.6483],\n",
      "        [  3.8674, -11.4081,  -5.0333,  -2.5507],\n",
      "        [  3.8672, -11.4084,  -5.0334,  -2.5510],\n",
      "        [  2.9519, -11.2762,  -4.4525,  -3.5411],\n",
      "        [  3.8628, -11.4058,  -5.0297,  -2.5550],\n",
      "        [  3.8302, -11.3968,  -5.0070,  -2.5887],\n",
      "        [  3.6849, -11.3811,  -4.9173,  -2.7478],\n",
      "        [  3.8467, -11.4042,  -5.0198,  -2.5727],\n",
      "        [  3.8603, -11.4045,  -5.0277,  -2.5573],\n",
      "        [  3.8358, -11.4031,  -5.0131,  -2.5847],\n",
      "        [  3.5724, -11.3868,  -4.8559,  -2.8777],\n",
      "        [  3.8677, -11.4083,  -5.0337,  -2.5503],\n",
      "        [  3.8677, -11.4084,  -5.0337,  -2.5504],\n",
      "        [  3.8279, -11.3729,  -5.0016,  -2.5772],\n",
      "        [  3.8256, -11.3986,  -5.0053,  -2.5945],\n",
      "        [  3.8678, -11.4087,  -5.0338,  -2.5504],\n",
      "        [  3.4062, -11.2754,  -4.7108,  -3.0244],\n",
      "        [  3.8669, -11.4075,  -5.0328,  -2.5510],\n",
      "        [  3.8679, -11.4086,  -5.0339,  -2.5503],\n",
      "        [  3.5029, -11.3479,  -4.7986,  -2.9421],\n",
      "        [  3.8678, -11.4085,  -5.0338,  -2.5504],\n",
      "        [  3.1832, -11.3916,  -4.6370,  -3.3215],\n",
      "        [  3.8672, -11.4082,  -5.0333,  -2.5509],\n",
      "        [  3.8441, -11.3830,  -5.0318,  -2.5501],\n",
      "        [  3.8670, -11.4078,  -5.0347,  -2.5497],\n",
      "        [  3.8677, -11.4083,  -5.0337,  -2.5503],\n",
      "        [  3.8596, -11.4062,  -5.0281,  -2.5588],\n",
      "        [  3.8677, -11.4084,  -5.0340,  -2.5502],\n",
      "        [  3.1963, -11.3221,  -4.6124,  -3.2806],\n",
      "        [  3.6926, -11.3550,  -4.9097,  -2.7293],\n",
      "        [  3.8442, -11.4032,  -5.0180,  -2.5752],\n",
      "        [  3.8676, -11.4085,  -5.0337,  -2.5506],\n",
      "        [  3.8644, -11.4073,  -5.0313,  -2.5538],\n",
      "        [  3.7578, -11.3307,  -4.9482,  -2.6365],\n",
      "        [  3.8339, -11.4012,  -5.0112,  -2.5861],\n",
      "        [  3.8678, -11.4086,  -5.0338,  -2.5503],\n",
      "        [-13.9781,   3.1993,  -4.7485,  -4.8071],\n",
      "        [-14.0279,   3.2255,  -4.7070,  -4.8547],\n",
      "        [-14.0389,   3.2574,  -4.6872,  -4.8545],\n",
      "        [-13.9843,   3.1979,  -4.7456,  -4.8147],\n",
      "        [-14.0397,   3.2585,  -4.6862,  -4.8550],\n",
      "        [-14.0395,   3.2581,  -4.6862,  -4.8551],\n",
      "        [-13.7130,   3.1389,  -4.9269,  -4.5286],\n",
      "        [-14.0385,   3.2549,  -4.6886,  -4.8549],\n",
      "        [-14.0178,   3.2264,  -4.7134,  -4.8421],\n",
      "        [-13.9564,   3.2110,  -4.7554,  -4.7781],\n",
      "        [-14.0392,   3.2574,  -4.6870,  -4.8548],\n",
      "        [-14.0290,   3.2287,  -4.7041,  -4.8554],\n",
      "        [-14.0390,   3.2576,  -4.6870,  -4.8545],\n",
      "        [-14.0215,   3.2099,  -4.7183,  -4.8529],\n",
      "        [-13.8214,   3.0304,  -4.9153,  -4.6922],\n",
      "        [-13.7623,   2.8148,  -5.0479,  -4.7059],\n",
      "        [-14.0141,   3.2097,  -4.7232,  -4.8441],\n",
      "        [-14.0367,   3.2499,  -4.6909,  -4.8554],\n",
      "        [-14.0131,   3.2421,  -4.7089,  -4.8309],\n",
      "        [-14.0081,   3.2229,  -4.7206,  -4.8323],\n",
      "        [-13.9752,   3.2043,  -4.7478,  -4.8020],\n",
      "        [-14.0001,   3.2362,  -4.7190,  -4.8184],\n",
      "        [-14.0354,   3.2472,  -4.6939,  -4.8543],\n",
      "        [-14.0085,   3.1775,  -4.7398,  -4.8508],\n",
      "        [-14.0309,   3.2512,  -4.6946,  -4.8476],\n",
      "        [-14.0394,   3.2576,  -4.6868,  -4.8549],\n",
      "        [-13.9477,   3.2023,  -4.7644,  -4.7714],\n",
      "        [-14.0333,   3.2454,  -4.6959,  -4.8526],\n",
      "        [-13.9879,   3.1822,  -4.7508,  -4.8246],\n",
      "        [-14.0339,   3.2503,  -4.6933,  -4.8515],\n",
      "        [-14.0365,   3.2501,  -4.6919,  -4.8544],\n",
      "        [-14.0174,   3.2232,  -4.7152,  -4.8428],\n",
      "        [-13.9146,   3.0497,  -4.8535,  -4.7909],\n",
      "        [-14.0312,   3.2370,  -4.6964,  -4.8567],\n",
      "        [-14.0395,   3.2581,  -4.6865,  -4.8548],\n",
      "        [-13.9351,   3.1137,  -4.8124,  -4.7902],\n",
      "        [-14.0398,   3.2588,  -4.6861,  -4.8549],\n",
      "        [-14.0398,   3.2587,  -4.6861,  -4.8549],\n",
      "        [-14.0397,   3.2586,  -4.6862,  -4.8549],\n",
      "        [-14.0101,   3.2308,  -4.7158,  -4.8316],\n",
      "        [-14.0062,   3.1767,  -4.7425,  -4.8478],\n",
      "        [-13.9193,   3.1438,  -4.8075,  -4.7610],\n",
      "        [-14.0320,   3.2421,  -4.6981,  -4.8524],\n",
      "        [-14.0341,   3.2426,  -4.6965,  -4.8547],\n",
      "        [-14.0123,   3.1856,  -4.7327,  -4.8530],\n",
      "        [-14.0395,   3.2584,  -4.6864,  -4.8547],\n",
      "        [-13.9092,   3.1020,  -4.8325,  -4.7652],\n",
      "        [-14.0393,   3.2572,  -4.6871,  -4.8549],\n",
      "        [-14.0340,   3.2424,  -4.6967,  -4.8546],\n",
      "        [-14.0171,   3.2212,  -4.7163,  -4.8432],\n",
      "        [ -5.7614,  -3.2677,   4.1877, -10.4455],\n",
      "        [ -5.8995,  -2.7835,   4.4902, -10.4222],\n",
      "        [ -5.6867,  -3.5374,   4.0219, -10.4619],\n",
      "        [ -5.3802,  -4.6244,   3.3468, -10.5197],\n",
      "        [ -5.6974,  -3.5018,   4.0444, -10.4608],\n",
      "        [ -5.9004,  -2.7807,   4.4917, -10.4220],\n",
      "        [ -5.8839,  -2.8392,   4.4556, -10.4252],\n",
      "        [ -5.6628,  -3.6098,   3.9746, -10.4616],\n",
      "        [ -5.8878,  -2.8254,   4.4642, -10.4245],\n",
      "        [ -5.8982,  -2.7885,   4.4870, -10.4224],\n",
      "        [ -5.7378,  -3.3528,   4.1359, -10.4510],\n",
      "        [ -5.6705,  -3.5939,   3.9866, -10.4647],\n",
      "        [ -5.8978,  -2.7897,   4.4864, -10.4225],\n",
      "        [ -5.6788,  -3.5655,   4.0044, -10.4635],\n",
      "        [ -5.6346,  -3.7250,   3.9058, -10.4729],\n",
      "        [ -5.8977,  -2.7900,   4.4862, -10.4225],\n",
      "        [ -5.9000,  -2.7817,   4.4913, -10.4221],\n",
      "        [ -5.8982,  -2.7881,   4.4873, -10.4224],\n",
      "        [ -5.8957,  -2.7973,   4.4816, -10.4229],\n",
      "        [ -5.8914,  -2.8125,   4.4722, -10.4238],\n",
      "        [ -5.8691,  -2.8923,   4.4228, -10.4282],\n",
      "        [ -5.8919,  -2.8109,   4.4730, -10.4236],\n",
      "        [ -5.8911,  -2.8136,   4.4715, -10.4238],\n",
      "        [ -5.7303,  -3.3831,   4.1177, -10.4538],\n",
      "        [ -5.8998,  -2.7826,   4.4908, -10.4221],\n",
      "        [ -5.8843,  -2.8377,   4.4566, -10.4252],\n",
      "        [ -5.6507,  -3.6681,   3.9412, -10.4698],\n",
      "        [ -5.8845,  -2.8373,   4.4566, -10.4250],\n",
      "        [ -5.6905,  -3.5254,   4.0295, -10.4617],\n",
      "        [ -5.6187,  -3.7657,   3.8779, -10.4699],\n",
      "        [ -5.7184,  -3.4274,   4.0905, -10.4569],\n",
      "        [ -5.8711,  -2.8847,   4.4274, -10.4277],\n",
      "        [ -5.8237,  -3.0513,   4.3233, -10.4358],\n",
      "        [ -5.8922,  -2.8141,   4.4685, -10.4227],\n",
      "        [ -4.9818,  -6.0316,   2.4719, -10.5926],\n",
      "        [ -5.8117,  -3.0953,   4.2966, -10.4388],\n",
      "        [ -5.8913,  -2.8137,   4.4709, -10.4236],\n",
      "        [ -5.9010,  -2.7800,   4.4914, -10.4216],\n",
      "        [ -5.1447,  -5.4349,   2.8395, -10.5549],\n",
      "        [ -5.8992,  -2.7847,   4.4895, -10.4223],\n",
      "        [ -5.8736,  -2.8761,   4.4328, -10.4273],\n",
      "        [ -5.9111,  -2.7791,   4.4722, -10.4135],\n",
      "        [ -5.8979,  -2.7891,   4.4867, -10.4225],\n",
      "        [ -5.9002,  -2.7809,   4.4918, -10.4220],\n",
      "        [ -5.8989,  -2.7859,   4.4885, -10.4223],\n",
      "        [ -5.8884,  -2.8442,   4.4401, -10.4203],\n",
      "        [ -5.7732,  -3.2305,   4.2125, -10.4456],\n",
      "        [ -5.8957,  -2.7971,   4.4818, -10.4229],\n",
      "        [ -5.2503,  -5.0839,   3.0612, -10.5438],\n",
      "        [ -5.8887,  -2.8220,   4.4663, -10.4243],\n",
      "        [ -4.2953,  -5.2914, -14.1625,   3.0189],\n",
      "        [ -4.2786,  -5.3450, -14.1967,   3.0179],\n",
      "        [ -4.4139,  -4.8829, -13.9069,   3.0367],\n",
      "        [ -4.3820,  -5.0205, -13.9885,   3.0215],\n",
      "        [ -4.2820,  -5.3330, -14.1893,   3.0185],\n",
      "        [ -4.3321,  -5.1773, -14.0891,   3.0197],\n",
      "        [ -4.3559,  -5.0735, -14.0278,   3.0315],\n",
      "        [ -4.3011,  -5.2646, -14.1469,   3.0224],\n",
      "        [ -4.2758,  -5.3539, -14.2012,   3.0168],\n",
      "        [ -4.4623,  -4.7112, -13.8003,   3.0459],\n",
      "        [ -4.2707,  -5.3724, -14.2139,   3.0167],\n",
      "        [ -4.3358,  -5.1465, -14.0728,   3.0271],\n",
      "        [ -4.5194,  -4.7217, -13.7724,   2.9769],\n",
      "        [ -4.2734,  -5.3626, -14.2078,   3.0172],\n",
      "        [ -4.2716,  -5.3689, -14.2118,   3.0169],\n",
      "        [ -4.2694,  -5.3724, -14.2114,   3.0156],\n",
      "        [ -4.2802,  -5.3385, -14.1929,   3.0185],\n",
      "        [ -4.2781,  -5.3464, -14.1977,   3.0179],\n",
      "        [ -4.3023,  -5.2602, -14.1442,   3.0227],\n",
      "        [ -4.6326,  -4.2034, -13.4695,   3.0421],\n",
      "        [ -4.3292,  -5.1853, -14.0944,   3.0200],\n",
      "        [ -4.2734,  -5.3628, -14.2079,   3.0171],\n",
      "        [ -4.2777,  -5.3473, -14.1983,   3.0180],\n",
      "        [ -4.2762,  -5.3533, -14.2019,   3.0174],\n",
      "        [ -4.2706,  -5.3726, -14.2140,   3.0166],\n",
      "        [ -4.2708,  -5.3719, -14.2136,   3.0167],\n",
      "        [ -4.2912,  -5.2998, -14.1688,   3.0205],\n",
      "        [ -4.2944,  -5.2884, -14.1617,   3.0211],\n",
      "        [ -4.2755,  -5.3556, -14.2032,   3.0172],\n",
      "        [ -4.2765,  -5.3495, -14.1969,   3.0161],\n",
      "        [ -4.2707,  -5.3720, -14.2136,   3.0166],\n",
      "        [ -4.2708,  -5.3718, -14.2134,   3.0166],\n",
      "        [ -4.3958,  -5.0264, -13.9827,   3.0032],\n",
      "        [ -4.2721,  -5.3671, -14.2106,   3.0169],\n",
      "        [ -4.5053,  -4.6149, -13.7315,   3.0331],\n",
      "        [ -4.3555,  -5.1258, -14.0515,   3.0120],\n",
      "        [ -4.2744,  -5.3595, -14.2058,   3.0173],\n",
      "        [ -4.2754,  -5.3553, -14.2025,   3.0169],\n",
      "        [ -4.2713,  -5.3702, -14.2126,   3.0168],\n",
      "        [ -4.6174,  -4.4705, -13.6012,   2.9596],\n",
      "        [ -4.2703,  -5.3726, -14.2134,   3.0164],\n",
      "        [ -4.2946,  -5.2877, -14.1612,   3.0211],\n",
      "        [ -4.3803,  -5.0053, -13.9824,   3.0292],\n",
      "        [ -4.2779,  -5.3471, -14.1981,   3.0179],\n",
      "        [ -4.3262,  -5.1993, -14.1024,   3.0181],\n",
      "        [ -4.2751,  -5.3571, -14.2043,   3.0174],\n",
      "        [ -4.3206,  -5.1970, -14.1047,   3.0255],\n",
      "        [ -4.5692,  -4.3309, -13.5644,   3.0665],\n",
      "        [ -4.2743,  -5.3598, -14.2059,   3.0172],\n",
      "        [ -4.3037,  -5.2555, -14.1412,   3.0228]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7072e-06,  1.6723e-06, -5.5242e-06,  7.7286e-07, -1.4560e-06,\n",
      "         -1.5128e-06,  2.0981e-06, -1.4059e-06, -6.9988e-07,  1.8376e-05,\n",
      "         -1.1840e-04, -5.1769e-07, -1.4401e-05, -3.7332e-08,  2.3887e-05,\n",
      "          1.3789e-05, -1.1470e-06, -2.4407e-05, -1.7764e-05,  1.1959e-04,\n",
      "          1.0379e-06, -1.1858e-04,  2.3653e-05, -2.5354e-05,  1.2028e-04],\n",
      "        [ 1.6723e-06,  3.1724e-05,  7.7286e-07, -1.9813e-05, -1.5128e-06,\n",
      "         -2.1345e-06,  9.2902e-07,  7.5741e-07, -8.6206e-07, -2.4699e-05,\n",
      "         -3.5507e-04, -1.3379e-06, -2.2362e-05, -5.9598e-07, -1.9375e-04,\n",
      "          2.2075e-05, -1.2950e-06,  1.9378e-04,  2.4987e-05,  3.5696e-04,\n",
      "          1.3093e-06, -3.5640e-04, -1.9343e-04,  1.9157e-04,  3.5826e-04],\n",
      "        [-5.5242e-06,  7.7286e-07,  1.7039e-01, -7.7296e-02, -8.0129e-03,\n",
      "         -1.3081e-03, -1.4059e-06,  8.7116e-03, -3.2322e-04, -8.0921e-03,\n",
      "         -1.2578e-03,  1.2096e-02,  2.3431e-02,  1.5130e-02, -4.9810e-03,\n",
      "         -1.4786e-03, -3.1149e-03, -3.0810e-03, -1.3860e-02, -1.0757e-02,\n",
      "         -4.0350e-03,  6.7926e-03,  1.3002e-02, -4.1461e-03, -1.5649e-02],\n",
      "        [ 7.7286e-07, -1.9813e-05, -7.7297e-02,  4.6875e-02, -1.3081e-03,\n",
      "         -1.3804e-03,  7.5741e-07, -1.0896e-02, -1.8234e-04,  3.2819e-03,\n",
      "          2.7399e-03, -7.7703e-03, -2.0045e-03, -4.5101e-03,  1.4517e-02,\n",
      "         -1.1419e-03,  3.4831e-04, -3.7355e-03, -1.3559e-04,  1.4220e-03,\n",
      "         -3.0107e-03, -2.9652e-03,  8.4533e-03, -4.2236e-03, -1.2642e-03],\n",
      "        [-1.4560e-06, -1.5128e-06, -8.0129e-03, -1.3081e-03,  7.9871e-02,\n",
      "          2.3114e-02, -6.9988e-07, -3.2322e-04,  1.7160e-02, -1.0273e-04,\n",
      "         -1.0349e-02, -1.4928e-03, -2.3776e-03, -6.5711e-04, -1.7019e-03,\n",
      "         -1.4341e-03,  1.4620e-02,  5.1975e-04,  3.9145e-03, -3.6136e-03,\n",
      "          2.6749e-03, -1.0520e-02, -2.4166e-03,  1.3449e-02, -5.1284e-04],\n",
      "        [-1.5128e-06, -2.1345e-06, -1.3081e-03, -1.3804e-03,  2.3114e-02,\n",
      "          7.3031e-03, -8.6206e-07, -1.8234e-04,  4.9274e-03,  4.0579e-05,\n",
      "         -3.0018e-03, -3.3712e-04, -7.5596e-04,  2.0967e-04, -9.2437e-04,\n",
      "         -8.8400e-04,  4.8530e-03, -4.5413e-04,  1.5994e-03, -2.0609e-03,\n",
      "          1.7156e-03, -2.9914e-03, -7.0414e-04,  3.9486e-03, -2.5308e-04],\n",
      "        [ 2.0981e-06,  9.2902e-07, -1.4059e-06,  7.5741e-07, -6.9988e-07,\n",
      "         -8.6206e-07,  1.8681e-06, -1.8065e-06, -5.3689e-07, -6.7402e-06,\n",
      "         -1.1130e-04, -3.3743e-07,  5.2027e-06,  3.1479e-07,  3.7264e-05,\n",
      "         -4.9858e-06, -9.4446e-07, -3.7253e-05,  6.5232e-06,  1.1193e-04,\n",
      "          3.2592e-07, -1.1158e-04,  3.7372e-05, -3.7988e-05,  1.1219e-04],\n",
      "        [-1.4059e-06,  7.5741e-07,  8.7116e-03, -1.0896e-02, -3.2322e-04,\n",
      "         -1.8234e-04, -1.8065e-06,  7.9378e-03, -4.9955e-04,  1.6661e-03,\n",
      "          3.3326e-03,  2.8827e-03, -4.9095e-03, -3.7195e-03, -4.2928e-03,\n",
      "          3.4743e-04, -7.4757e-04,  5.0008e-04,  2.8960e-03,  1.1344e-03,\n",
      "          9.0991e-04,  5.5055e-03, -7.9187e-03,  1.6499e-04,  2.2481e-03],\n",
      "        [-6.9988e-07, -8.6206e-07, -3.2322e-04, -1.8234e-04,  1.7160e-02,\n",
      "          4.9274e-03, -5.3689e-07, -4.9955e-04,  4.4832e-03,  2.6775e-05,\n",
      "         -2.5271e-03, -3.5622e-04,  5.0975e-04,  1.6688e-04,  3.1463e-04,\n",
      "          2.8419e-04,  3.7932e-03,  7.5193e-04, -8.2072e-04, -1.4330e-03,\n",
      "         -7.1035e-04, -2.5683e-03,  4.9733e-04,  4.1403e-03, -2.0694e-03],\n",
      "        [ 1.8376e-05, -2.4699e-05, -8.0921e-03,  3.2819e-03, -1.0273e-04,\n",
      "          4.0579e-05, -6.7402e-06,  1.6661e-03,  2.6775e-05,  3.2269e-02,\n",
      "          3.1680e-02,  2.6836e-05, -1.0642e-05, -8.7002e-06, -1.7052e-06,\n",
      "         -2.4913e-09, -6.8576e-07, -1.0270e-07, -3.2258e-02, -3.1671e-02,\n",
      "         -2.5028e-05,  3.2278e-02, -1.0643e-05, -7.8641e-07, -3.2266e-02],\n",
      "        [-1.1840e-04, -3.5507e-04, -1.2578e-03,  2.7399e-03, -1.0349e-02,\n",
      "         -3.0018e-03, -1.1130e-04,  3.3326e-03, -2.5271e-03,  3.1680e-02,\n",
      "          1.2082e-01,  2.1298e-03, -8.7002e-06, -2.2446e-05, -5.7758e-07,\n",
      "         -6.8576e-07, -9.4759e-03, -8.6729e-04, -3.1671e-02, -1.1133e-01,\n",
      "         -1.2619e-03,  1.2200e-01, -2.2856e-05, -9.8733e-03, -1.1210e-01],\n",
      "        [-5.1769e-07, -1.3379e-06,  1.2096e-02, -7.7703e-03, -1.4928e-03,\n",
      "         -3.3712e-04, -3.3743e-07,  2.8827e-03, -3.5622e-04,  2.6836e-05,\n",
      "          2.1298e-03,  3.3384e-03, -1.7052e-06, -5.7758e-07, -3.1977e-06,\n",
      "         -1.0270e-07, -8.6729e-04, -3.2301e-03, -2.5028e-05, -1.2619e-03,\n",
      "         -1.0504e-04,  4.8490e-03, -3.5921e-06, -3.5695e-03, -1.2759e-03],\n",
      "        [-1.4401e-05, -2.2362e-05,  2.3431e-02, -2.0045e-03, -2.3776e-03,\n",
      "         -7.5596e-04,  5.2027e-06, -4.9095e-03,  5.0975e-04, -1.0642e-05,\n",
      "         -8.7002e-06, -1.7052e-06,  4.8373e-02,  1.4034e-02,  3.3967e-02,\n",
      "         -1.7763e-02, -1.0494e-04, -1.7698e-02, -3.0600e-02, -1.3920e-02,\n",
      "         -1.6268e-02, -1.0643e-05,  4.8376e-02, -1.7765e-02, -3.0601e-02],\n",
      "        [-3.7332e-08, -5.9598e-07,  1.5130e-02, -4.5101e-03, -6.5711e-04,\n",
      "          2.0967e-04,  3.1479e-07, -3.7195e-03,  1.6688e-04, -8.7002e-06,\n",
      "         -2.2446e-05, -5.7758e-07,  1.4034e-02,  1.3540e-02,  1.2642e-03,\n",
      "         -1.0494e-04, -1.4028e-04, -1.1309e-03, -1.3920e-02, -1.3378e-02,\n",
      "         -1.3269e-04, -2.2856e-05,  1.5076e-02, -1.1326e-03, -1.3920e-02],\n",
      "        [ 2.3887e-05, -1.9375e-04, -4.9810e-03,  1.4517e-02, -1.7019e-03,\n",
      "         -9.2437e-04,  3.7264e-05, -4.2928e-03,  3.1463e-04, -1.7052e-06,\n",
      "         -5.7758e-07, -3.1977e-06,  3.3967e-02,  1.2642e-03,  6.4103e-02,\n",
      "         -1.7698e-02, -1.1309e-03, -4.7930e-02, -1.6268e-02, -1.3269e-04,\n",
      "         -1.6171e-02, -3.5921e-06,  6.4271e-02, -4.7999e-02, -1.6268e-02],\n",
      "        [ 1.3789e-05,  2.2075e-05, -1.4786e-03, -1.1419e-03, -1.4341e-03,\n",
      "         -8.8400e-04, -4.9858e-06,  3.4743e-04,  2.8419e-04, -2.4913e-09,\n",
      "         -6.8576e-07, -1.0270e-07, -1.7763e-02, -1.0494e-04, -1.7698e-02,\n",
      "          1.7770e-02,  1.0750e-04,  1.7704e-02, -7.6851e-06, -1.8666e-06,\n",
      "         -5.7849e-06, -7.8641e-07, -1.7765e-02,  1.7774e-02, -7.6875e-06],\n",
      "        [-1.1470e-06, -1.2950e-06, -3.1149e-03,  3.4831e-04,  1.4620e-02,\n",
      "          4.8530e-03, -9.4446e-07, -7.4757e-04,  3.7932e-03, -6.8576e-07,\n",
      "         -9.4759e-03, -8.6729e-04, -1.0494e-04, -1.4028e-04, -1.1309e-03,\n",
      "          1.0750e-04,  9.6333e-03,  2.0013e-03, -1.8666e-06, -1.7209e-05,\n",
      "         -3.0197e-06, -9.8733e-03, -1.1326e-03,  1.1025e-02, -1.9187e-05],\n",
      "        [-2.4407e-05,  1.9378e-04, -3.0810e-03, -3.7355e-03,  5.1975e-04,\n",
      "         -4.5413e-04, -3.7253e-05,  5.0008e-04,  7.5193e-04, -1.0270e-07,\n",
      "         -8.6729e-04, -3.2301e-03, -1.7698e-02, -1.1309e-03, -4.7930e-02,\n",
      "          1.7704e-02,  2.0013e-03,  5.1190e-02, -5.7849e-06, -3.0197e-06,\n",
      "         -2.9679e-05, -3.5695e-03, -4.7999e-02,  5.1599e-02, -3.0220e-05],\n",
      "        [-1.7764e-05,  2.4987e-05, -1.3860e-02, -1.3559e-04,  3.9145e-03,\n",
      "          1.5994e-03,  6.5232e-06,  2.8960e-03, -8.2072e-04, -3.2258e-02,\n",
      "         -3.1671e-02, -2.5028e-05, -3.0600e-02, -1.3920e-02, -1.6268e-02,\n",
      "         -7.6851e-06, -1.8666e-06, -5.7849e-06,  6.2866e-02,  4.5593e-02,\n",
      "          1.6299e-02, -3.2266e-02, -3.0601e-02, -7.6875e-06,  6.2874e-02],\n",
      "        [ 1.1959e-04,  3.5696e-04, -1.0757e-02,  1.4220e-03, -3.6136e-03,\n",
      "         -2.0609e-03,  1.1193e-04,  1.1344e-03, -1.4330e-03, -3.1671e-02,\n",
      "         -1.1133e-01, -1.2619e-03, -1.3920e-02, -1.3378e-02, -1.3269e-04,\n",
      "         -1.8666e-06, -1.7209e-05, -3.0197e-06,  4.5593e-02,  1.2472e-01,\n",
      "          1.3976e-03, -1.1210e-01, -1.3920e-02, -1.9187e-05,  1.2604e-01],\n",
      "        [ 1.0379e-06,  1.3093e-06, -4.0350e-03, -3.0107e-03,  2.6749e-03,\n",
      "          1.7156e-03,  3.2592e-07,  9.0991e-04, -7.1035e-04, -2.5028e-05,\n",
      "         -1.2619e-03, -1.0504e-04, -1.6268e-02, -1.3269e-04, -1.6171e-02,\n",
      "         -5.7849e-06, -3.0197e-06, -2.9679e-05,  1.6299e-02,  1.3976e-03,\n",
      "          1.6305e-02, -1.2759e-03, -1.6268e-02, -3.0220e-05,  1.7575e-02],\n",
      "        [-1.1858e-04, -3.5640e-04,  6.7926e-03, -2.9652e-03, -1.0520e-02,\n",
      "         -2.9914e-03, -1.1158e-04,  5.5055e-03, -2.5683e-03,  3.2278e-02,\n",
      "          1.2200e-01,  4.8490e-03, -1.0643e-05, -2.2856e-05, -3.5921e-06,\n",
      "         -7.8641e-07, -9.8733e-03, -3.5695e-03, -3.2266e-02, -1.1210e-01,\n",
      "         -1.2759e-03,  1.2557e-01, -2.6185e-05, -1.2619e-02, -1.1292e-01],\n",
      "        [ 2.3653e-05, -1.9343e-04,  1.3002e-02,  8.4533e-03, -2.4166e-03,\n",
      "         -7.0414e-04,  3.7372e-05, -7.9187e-03,  4.9733e-04, -1.0643e-05,\n",
      "         -2.2856e-05, -3.5921e-06,  4.8376e-02,  1.5076e-02,  6.4271e-02,\n",
      "         -1.7765e-02, -1.1326e-03, -4.7999e-02, -3.0601e-02, -1.3920e-02,\n",
      "         -1.6268e-02, -2.6185e-05,  7.8697e-02, -4.8070e-02, -3.0601e-02],\n",
      "        [-2.5354e-05,  1.9157e-04, -4.1461e-03, -4.2236e-03,  1.3449e-02,\n",
      "          3.9486e-03, -3.7988e-05,  1.6499e-04,  4.1403e-03, -7.8641e-07,\n",
      "         -9.8733e-03, -3.5695e-03, -1.7765e-02, -1.1326e-03, -4.7999e-02,\n",
      "          1.7774e-02,  1.1025e-02,  5.1599e-02, -7.6875e-06, -1.9187e-05,\n",
      "         -3.0220e-05, -1.2619e-02, -4.8070e-02,  6.0737e-02, -4.6539e-05],\n",
      "        [ 1.2028e-04,  3.5826e-04, -1.5649e-02, -1.2642e-03, -5.1284e-04,\n",
      "         -2.5308e-04,  1.1219e-04,  2.2481e-03, -2.0694e-03, -3.2266e-02,\n",
      "         -1.1210e-01, -1.2759e-03, -3.0601e-02, -1.3920e-02, -1.6268e-02,\n",
      "         -7.6875e-06, -1.9187e-05, -3.0220e-05,  6.2874e-02,  1.2604e-01,\n",
      "          1.7575e-02, -1.1292e-01, -3.0601e-02, -4.6539e-05,  1.4357e-01]])\n"
     ]
    }
   ],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='all',\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "print(hessian)\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_all_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='last_layer',\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_ll_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='subnetwork',\n",
    "             subnetwork_indices = torch.LongTensor([1, 3, 5, 6, 7, 9]),\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_subnet_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace.curvature import AsdlEF\n",
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='all',\n",
    "             hessian_structure='full', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_all_full_empfisher.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace.curvature import AsdlEF\n",
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='last_layer',\n",
    "             hessian_structure='full', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_ll_full_empfisher.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace.curvature import AsdlEF\n",
    "la = Laplace(model, 'classification',\n",
    "             subset_of_weights='subnetwork',\n",
    "             subnetwork_indices = torch.LongTensor([1, 3, 5, 6, 7, 9]),\n",
    "             hessian_structure='full', backend=AsdlEF)\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(x, y_train), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_multi_subnet_full_empfisher.csv', array, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

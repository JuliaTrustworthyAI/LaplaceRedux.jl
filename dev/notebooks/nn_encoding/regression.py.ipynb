{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding a neural network from a JSON file to Python, then encoding the hessian from laplace.py to a CSV (regression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Read the dataset\n",
    "- Step 2: Initialize model\n",
    "- Step 3: Read the weights and biases from the JSON file, then load them into the previously initialized model\n",
    "- Step 4: Generate the hessians and write them to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cecfc6fb70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from laplace import Laplace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv\n",
    "\n",
    "# Load data from CSV file using pandas\n",
    "df = pd.read_csv('data_regression.csv')\n",
    "\n",
    "# Split the dataframe into x and y tensors\n",
    "data = torch.from_numpy(df[['xs', 'y']].to_numpy()).to(torch.float32)\n",
    "x = torch.from_numpy(df['xs'].to_numpy()).to(torch.float32)\n",
    "y = torch.from_numpy(df['y'].to_numpy()).to(torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "\n",
    "n_hidden = 10\n",
    "D = 1\n",
    "out_dim = 1 \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, n_hidden),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_hidden, out_dim)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Read the weights and biases from the JSON file, then load them into the previously initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': [-0.42261407,\n",
       "   -0.21227753,\n",
       "   -0.0096151605,\n",
       "   -0.7529069,\n",
       "   1.3016218,\n",
       "   3.116596,\n",
       "   -2.1422381,\n",
       "   -1.2806743,\n",
       "   0.78418183,\n",
       "   -1.3360132],\n",
       "  'weight': [[-0.55855876,\n",
       "    0.2368329,\n",
       "    0.7279194,\n",
       "    1.1712134,\n",
       "    -0.2849682,\n",
       "    -0.82284534,\n",
       "    0.76305974,\n",
       "    0.28283465,\n",
       "    -0.24539436,\n",
       "    0.6470228]]},\n",
       " {'bias': [0.13576016],\n",
       "  'weight': [[-0.27233723],\n",
       "   [0.7866149],\n",
       "   [0.26231468],\n",
       "   [0.16347119],\n",
       "   [-1.6750962],\n",
       "   [1.7097319],\n",
       "   [-1.3647312],\n",
       "   [1.6296383],\n",
       "   [-1.1626163],\n",
       "   [-0.75578994]]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('nn_regression.json') as fin:\n",
    "    nn_json_str = fin.read()\n",
    "    nn_json = json.loads(nn_json_str)\n",
    "\n",
    "nn_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model.state_dict()) == 2 * len(nn_json)\n",
    "iter_states = iter(model.state_dict())\n",
    "\n",
    "# for layer in model.state_dict():\n",
    "#     print(layer)\n",
    "for layer_read in nn_json:\n",
    "    state_w = next(iter_states)\n",
    "    state_b = next(iter_states)\n",
    "    tensor_w = torch.tensor(layer_read['weight']).T\n",
    "    tensor_b = torch.tensor(layer_read['bias']).T\n",
    "    model.state_dict()[state_w].data.copy_(tensor_w)\n",
    "    model.state_dict()[state_b].data.copy_(tensor_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.585588e-01],\n",
       "        [2.368329e-01],\n",
       "        [7.279194e-01],\n",
       "        [1.171213e+00],\n",
       "        [-2.849682e-01],\n",
       "        [-8.228453e-01],\n",
       "        [7.630597e-01],\n",
       "        [2.828346e-01],\n",
       "        [-2.453944e-01],\n",
       "        [6.470228e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-4.226141e-01, -2.122775e-01, -9.615161e-03, -7.529069e-01, 1.301622e+00, 3.116596e+00,\n",
       "        -2.142238e+00, -1.280674e+00, 7.841818e-01, -1.336013e+00])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.723372e-01, 7.866149e-01, 2.623147e-01, 1.634712e-01, -1.675096e+00, 1.709732e+00,\n",
       "         -1.364731e+00, 1.629638e+00, -1.162616e+00, -7.557899e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.357602e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for val in model.state_dict().values():\n",
    "    display(val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate the hessians and write them to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the unsqueeze() method with the argument 1 to add an extra dimension\n",
    "# this extra dimension converts each value into an array of its own\n",
    "X = x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adeli\\OneDrive\\Desktop\\facultate\\2nd year\\Q4 - Software Project\\LaplaceRedux.jl\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "la = Laplace(model, 'regression',\n",
    "             subset_of_weights='all',\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(X, y), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_regression_all_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'regression',\n",
    "             subset_of_weights='last_layer',\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(X, y), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_regression_ll_full_ggn.csv', array, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(model, 'regression',\n",
    "             subset_of_weights='subnetwork',\n",
    "             subnetwork_indices = torch.LongTensor([1, 3, 5, 6, 7, 9]),\n",
    "             hessian_structure='full')\n",
    "\n",
    "la.fit(DataLoader(TensorDataset(X, y), batch_size=1))\n",
    "\n",
    "hessian = la.H\n",
    "array = hessian.numpy()\n",
    "np.savetxt('hessian_regression_subnet_full_ggn.csv', array, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

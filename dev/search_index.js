var documenterSearchIndex = {"docs":
[{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/mlp/#Bayesian-MLP","page":"MLP Binary Classifier","title":"Bayesian MLP","text":"","category":"section"},{"location":"tutorials/mlp/#Libraries","page":"MLP Binary Classifier","title":"Libraries","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"using Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux, LinearAlgebra\ntheme(:lime)","category":"page"},{"location":"tutorials/mlp/#Data","page":"MLP Binary Classifier","title":"Data","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"This time we use a synthetic dataset containing samples that are not linearly separable:","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"# Number of points to generate.\nxs, ys = LaplaceRedux.Data.toy_data_non_linear(200)\nX = hcat(xs...) # bring into tabular format\ndata = zip(xs,ys)","category":"page"},{"location":"tutorials/mlp/#Model","page":"MLP Binary Classifier","title":"Model","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"For the classification task we build a neural network with weight decay composed of a single hidden layer.","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"n_hidden = 10\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, Ïƒ),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) ","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"The model is trained until training loss stagnates.","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"using Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/mlp/#Laplace-Approximation","page":"MLP Binary Classifier","title":"Laplace Approximation","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"Laplace approximation can be implemented as follows:","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"la = Laplace(nn; likelihood=:classification, subset_of_weights=:all)\nfit!(la, data)\nla_untuned = deepcopy(la)   # saving for plotting\noptimize_prior!(la; verbose=true, n_steps=500)","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"# Plot the posterior distribution with a contour plot.\nzoom=0\np_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\np_untuned = plot(la_untuned, X, ys; title=\"LA - raw (Î»=$(unique(diag(la_untuned.prior.Pâ‚€))[1]))\", clim=(0,1), zoom=zoom)\np_laplace = plot(la, X, ys; title=\"LA - tuned (Î»=$(round(unique(diag(la.prior.Pâ‚€))[1],digits=2)))\", clim=(0,1), zoom=zoom)\nplot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"(Image: )","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"Zooming out we can note that the plugin estimator produces high-confidence estimates in regions scarce of any samples. The Laplace approximation is much more conservative about these regions.","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"zoom=-50\np_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\np_untuned = plot(la_untuned, X, ys; title=\"LA - raw (Î»=$(unique(diag(la_untuned.prior.Pâ‚€))[1]))\", clim=(0,1), zoom=zoom)\np_laplace = plot(la, X, ys; title=\"LA - tuned (Î»=$(round(unique(diag(la.prior.Pâ‚€))[1],digits=2)))\", clim=(0,1), zoom=zoom)\nplot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"(Image: )","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/logit/#Bayesian-Logistic-Regression","page":"Logistic Regression","title":"Bayesian Logistic Regression","text":"","category":"section"},{"location":"tutorials/logit/#Libraries","page":"Logistic Regression","title":"Libraries","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"using Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux, LinearAlgebra\ntheme(:lime)","category":"page"},{"location":"tutorials/logit/#Data","page":"Logistic Regression","title":"Data","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"We will use synthetic data with linearly separable samples:","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"# Number of points to generate.\nxs, ys = LaplaceRedux.Data.toy_data_linear(100)\nX = hcat(xs...) # bring into tabular format\ndata = zip(xs,ys)","category":"page"},{"location":"tutorials/logit/#Model","page":"Logistic Regression","title":"Model","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"Logistic regression with weight decay can be implemented in Flux.jl as a single dense (linear) layer with binary logit crossentropy loss:","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"nn = Chain(Dense(2,1))\nÎ» = 0.5\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Î»=Î») = 1/2 * Î»^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"The code below simply trains the model. After about 50 training epochs training loss stagnates.","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 50\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/logit/#Laplace-approximation","page":"Logistic Regression","title":"Laplace approximation","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"Laplace approximation for the posterior predictive can be implemented as follows:","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"la = Laplace(nn; likelihood=:classification, Î»=Î», subset_of_weights=:last_layer)\nfit!(la, data)\nla_untuned = deepcopy(la)   # saving for plotting\noptimize_prior!(la; verbose=true, n_steps=500)","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"zoom = 0\np_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\np_untuned = plot(la_untuned, X, ys; title=\"LA - raw (Î»=$(unique(diag(la_untuned.prior.Pâ‚€))[1]))\", clim=(0,1), zoom=zoom)\np_laplace = plot(la, X, ys; title=\"LA - tuned (Î»=$(round(unique(diag(la.prior.Pâ‚€))[1],digits=2)))\", clim=(0,1), zoom=zoom)\nplot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/prior/#Libraries","page":"A note on the prior ...","title":"Libraries","text":"","category":"section"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"using Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux, LinearAlgebra","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"note: In Progress\n","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"Â Â Â Â This documentation is still incomplete.","category":"page"},{"location":"tutorials/prior/#A-quick-note-on-the-prior","page":"A note on the prior ...","title":"A quick note on the prior","text":"","category":"section"},{"location":"tutorials/prior/#General-Effect","page":"A note on the prior ...","title":"General Effect","text":"","category":"section"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"High prior precision rightarrow only observation noise. Low prior precision rightarrow high posterior uncertainty.","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"using LaplaceRedux.Data\nn = 150       # number of observations\nÏƒtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=Ïƒtrue)\nxs = [[x] for x in x]\nX = permutedims(x)","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"data = zip(xs,y)\nn_hidden = 10\nD = size(X,1)\nÎ› = [1e5, nothing, 1e-5]\nplts = []\nnns = []\nopt=Flux.Adam(1e-3)\nfor Î» âˆˆ Î›\n    nn = Chain(\n        Dense(D, n_hidden, tanh),\n        Dense(n_hidden, 1)\n    )  \n    loss(x, y) = Flux.Losses.mse(nn(x), y)\n    # train\n    epochs = 1000\n    for epoch = 1:epochs\n        for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        Flux.update!(opt, Flux.params(nn), gs)\n        end\n    end\n    # laplace\n    if !isnothing(Î»)\n        la = Laplace(nn; likelihood=:regression, Î»=Î»)\n        fit!(la, data)  \n    else\n        la = Laplace(nn; likelihood=:regression)\n        fit!(la, data)  \n        optimize_prior!(la)\n    end\n    \n    _suffix = isnothing(Î») ? \" (optimal)\" : \"\"\n    Î» = unique(diag(la.prior.Pâ‚€))[1]\n    title = \"Î»=$(round(Î»,digits=2))$(_suffix)\"\n\n    # plot \n    plt = plot(la, X, y; title=title, zoom=-5)\n    plts = vcat(plts..., plt)\n    nns = vcat(nns..., nn)\nend\nplot(plts..., layout=(1,3), size=(1200,300))","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"(Image: )","category":"page"},{"location":"tutorials/prior/#Effect-of-Model-Size-on-Optimal-Choice","page":"A note on the prior ...","title":"Effect of Model Size on Optimal Choice","text":"","category":"section"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"For larger models, the optimal prior precision lambda as evaluated through Empirical Bayes tends to be smaller.","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"data = zip(xs,y)\nn_hiddens = [5, 10, 50]\nD = size(X,1)\nplts = []\nnns = []\nopt=Flux.Adam(1e-3)\nfor n_hidden âˆˆ n_hiddens\n    nn = Chain(\n        Dense(D, n_hidden, tanh),\n        Dense(n_hidden, 1)\n    )  \n    loss(x, y) = Flux.Losses.mse(nn(x), y)\n    # train\n    epochs = 1000\n    for epoch = 1:epochs\n        for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        Flux.update!(opt, Flux.params(nn), gs)\n        end\n    end\n    # laplace\n    la = Laplace(nn; likelihood=:regression)\n    fit!(la, data)  \n    optimize_prior!(la)\n    \n    Î» = unique(diag(la.prior.Pâ‚€))[1]\n    title = \"n_params=$(LaplaceRedux.n_params(la)),Î»=$(round(Î»,digits=2))\"\n\n    # plot \n    plt = plot(la, X, y; title=title, zoom=-5)\n    plts = vcat(plts..., plt)\n    nns = vcat(nns..., nn)\nend\nplot(plts..., layout=(1,3), size=(1200,300))","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"(Image: )","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"# Number of points to generate.\nxs, ys = LaplaceRedux.Data.toy_data_non_linear(200)\nX = hcat(xs...) # bring into tabular format\ndata = zip(xs,ys)\n\nn_hiddens = [5, 10, 50]\nD = size(X,1)\nplts = []\nnns = []\nopt=Flux.Adam(1e-3)\nfor n_hidden âˆˆ n_hiddens\n    nn = Chain(\n        Dense(D, n_hidden, Ïƒ),\n        Dense(n_hidden, 1)\n    )  \n    loss(x, y) = Flux.Losses.mse(nn(x), y)\n    # train\n    epochs = 100\n    for epoch = 1:epochs\n        for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        Flux.update!(opt, Flux.params(nn), gs)\n        end\n    end\n    # laplace\n    la = Laplace(nn; likelihood=:classification)\n    fit!(la, data)  \n    optimize_prior!(la)\n    \n    Î» = unique(diag(la.prior.Pâ‚€))[1]\n    title = \"n_params=$(LaplaceRedux.n_params(la)),Î»=$(round(Î»,digits=2))\"\n\n    # plot \n    plt = plot(la, X, ys; title=title, zoom=-1, clim=(0,1))\n    plts = vcat(plts..., plt)\n    nns = vcat(nns..., nn)\nend\nplot(plts..., layout=(1,3), size=(1200,300))","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"(Image: )","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/regression/#Libraries","page":"MLP Regression","title":"Libraries","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"Import the libraries required to run this example","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"using Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:wong)","category":"page"},{"location":"tutorials/regression/#Data","page":"MLP Regression","title":"Data","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"We first generate some synthetic data:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"using LaplaceRedux.Data\nn = 300       # number of observations\nÏƒtrue = 0.30  # true observational noise\nx, y = Data.toy_data_regression(n;noise=Ïƒtrue)\nxs = [[x] for x in x]\nX = permutedims(x)","category":"page"},{"location":"tutorials/regression/#MLP","page":"MLP Regression","title":"MLP","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"We set up a model and loss with weight regularization:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"data = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.mse(nn(x), y)","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"We train the model:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"using Flux.Optimise: update!, Adam\nopt = Adam(1e-3)\nepochs = 1000\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/regression/#Laplace-Approximation","page":"MLP Regression","title":"Laplace Approximation","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"Laplace approximation can be implemented as follows:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"subset_w = :all\nla = Laplace(nn; likelihood=:regression, subset_of_weights=subset_w)\nfit!(la, data)\nplot(la, X, y; zoom=-5, size=(400,400))","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"(Image: )","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"Next we optimize the prior precision P_0 and and observational noise sigma using Empirical Bayes:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"optimize_prior!(la; verbose=true)\nplot(la, X, y; zoom=-5, size=(400,400))","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"loss(exp.(logPâ‚€), exp.(logÏƒ)) = 104.78561546028183\nLog likelihood: -70.48742092717352\nLog det ratio: 41.1390695290454\nScatter: 27.45731953717124\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 104.9736282327825\nLog likelihood: -74.85481357633174\nLog det ratio: 46.59827618892447\nScatter: 13.639353123977058\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 84.38222356291794\nLog likelihood: -54.86985627702764\nLog det ratio: 49.92347667032635\nScatter: 9.101257901454279\n\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 84.53493863039972\nLog likelihood: -55.013137224636\nLog det ratio: 51.43622180356522\nScatter: 7.607381007962245\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 83.95921598606084\nLog likelihood: -54.41492266831395\nLog det ratio: 51.794520967146354\nScatter: 7.294065668347427\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 83.03505059021086\nLog likelihood: -53.50540374805591\nLog det ratio: 51.574749787874794\nScatter: 7.484543896435117\n\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 82.97840036025443\nLog likelihood: -53.468475394115416\nLog det ratio: 51.17273666609066\nScatter: 7.847113266187348\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 82.98550025321256\nLog likelihood: -53.48508828283467\nLog det ratio: 50.81442045868749\nScatter: 8.186403482068298\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 82.9584040552644\nLog likelihood: -53.45989630330948\nLog det ratio: 50.59063282947659\nScatter: 8.406382674433235\n\n\nloss(exp.(logPâ‚€), exp.(logÏƒ)) = 82.94465052328141\nLog likelihood: -53.44600301956443\nLog det ratio: 50.500079294094405\nScatter: 8.497215713339543","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"(Image: )","category":"page"},{"location":"tutorials/multi/#Multi-class-problem","page":"MLP Multi-Label Classifier","title":"Multi-class problem","text":"","category":"section"},{"location":"tutorials/multi/#Libraries","page":"MLP Multi-Label Classifier","title":"Libraries","text":"","category":"section"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"using Pkg; Pkg.activate(\"docs\")\n# Import libraries\nusing Flux, Plots, TaijaPlotting, Random, Statistics, LaplaceRedux\ntheme(:lime)","category":"page"},{"location":"tutorials/multi/#Data","page":"MLP Multi-Label Classifier","title":"Data","text":"","category":"section"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"using LaplaceRedux.Data\nx, y = Data.toy_data_multi()\nX = hcat(x...)\ny_train = Flux.onehotbatch(y, unique(y))\ny_train = Flux.unstack(y_train',1)","category":"page"},{"location":"tutorials/multi/#MLP","page":"MLP Multi-Label Classifier","title":"MLP","text":"","category":"section"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"We set up a model","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"data = zip(x,y_train)\nn_hidden = 3\nD = size(X,1)\nout_dim = length(unique(y))\nnn = Chain(\n    Dense(D, n_hidden, Ïƒ),\n    Dense(n_hidden, out_dim)\n)  \nloss(x, y) = Flux.Losses.logitcrossentropy(nn(x), y)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"training:","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n    for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        update!(opt, Flux.params(nn), gs)\n    end\n    if epoch % show_every == 0\n        println(\"Epoch \" * string(epoch))\n        @show avg_loss(data)\n    end\nend","category":"page"},{"location":"tutorials/multi/#Laplace-Approximation","page":"MLP Multi-Label Classifier","title":"Laplace Approximation","text":"","category":"section"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"The Laplace approximation can be implemented as follows:","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"la = Laplace(nn; likelihood=:classification)\nfit!(la, data)\noptimize_prior!(la; verbose=true, n_steps=100)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"_labels = sort(unique(y))\nplt_list = []\nfor target in _labels\n    plt = plot(la, X, y; target=target, clim=(0,1))\n    push!(plt_list, plt)\nend\nplot(plt_list...)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"(Image: )","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"_labels = sort(unique(y))\nplt_list = []\nfor target in _labels\n    plt = plot(la, X, y; target=target, clim=(0,1), link_approx=:plugin)\n    push!(plt_list, plt)\nend\nplot(plt_list...)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"(Image: )","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"reference/#All-functions-and-types","page":"Reference","title":"All functions and types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#Exported-functions","page":"Reference","title":"Exported functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [\n    LaplaceRedux,\n    LaplaceRedux.Curvature,\n    LaplaceRedux.Data,\n]\nPrivate = false","category":"page"},{"location":"reference/#LaplaceRedux.Laplace","page":"Reference","title":"LaplaceRedux.Laplace","text":"Laplace\n\nConcrete type for Laplace approximation. This type is a subtype of AbstractLaplace and is used to store all the necessary information for a Laplace approximation.\n\nFields\n\nmodel::Flux.Chain: The model to be approximated.\nlikelihood::Symbol: The likelihood function to be used.\nest_params::EstimationParams: The estimation parameters.\nprior::Prior: The parameters defining prior distribution.\nposterior::Posterior: The posterior distribution.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Laplace-Tuple{Any}","page":"Reference","title":"LaplaceRedux.Laplace","text":"Laplace(model::Any; likelihood::Symbol, kwargs...)\n\nOuter constructor for Laplace approximation. This function constructs a Laplace object from a given model and likelihood function.\n\nArguments\n\nmodel::Any: The model to be approximated (a Flux.Chain).\nlikelihood::Symbol: The likelihood function to be used. Possible values are :regression and :classification.\n\nKeyword Arguments\n\nSee LaplaceParams for a description of the keyword arguments.\n\nReturns\n\nla::Laplace: The Laplace object.\n\nExamples\n\nusing Flux, LaplaceRedux\nnn = Chain(Dense(2,1))\nla = Laplace(nn, likelihood=:regression)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.LaplaceClassification","page":"Reference","title":"LaplaceRedux.LaplaceClassification","text":"MLJBase.@mlj_model mutable struct LaplaceClassification <: MLJFlux.MLJFluxProbabilistic\n\nA mutable struct representing a Laplace Classification model that extends the MLJFluxProbabilistic abstract type. It uses Laplace approximation to estimate the posterior distribution of the weights of a neural network. \n\nThe model is defined by the following default parameters for all MLJFlux models:\n\nbuilder: a Flux model that constructs the neural network.\nfinaliser: a Flux model that processes the output of the neural network.\noptimiser: a Flux optimiser.\nloss: a loss function that takes the predicted output and the true output as arguments.\nepochs: the number of epochs.\nbatch_size: the size of a batch.\nlambda: the regularization strength.\nalpha: the regularization mix (0 for all l2, 1 for all l1).\nrng: a random number generator.\noptimiser_changes_trigger_retraining: a boolean indicating whether changes in the optimiser trigger retraining.\nacceleration: the computational resource to use.\n\nThe model also has the following parameters, which are specific to the Laplace approximation:\n\nsubset_of_weights: the subset of weights to use, either :all, :last_layer, or :subnetwork.\nsubnetwork_indices: the indices of the subnetworks.\nhessian_structure: the structure of the Hessian matrix, either :full or :diagonal.\nbackend: the backend to use, either :GGN or :EmpiricalFisher.\nÏƒ: the standard deviation of the prior distribution.\nÎ¼â‚€: the mean of the prior distribution.\nPâ‚€: the covariance matrix of the prior distribution.\nlink_approx: the link approximation to use, either :probit or :plugin.\npredict_proba: a boolean that select whether to predict probabilities or not.\nret_distr: a boolean that tells predict to either return distributions (true) objects from Distributions.jl or just the probabilities.\nfit_prior_nsteps: the number of steps used to fit the priors.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.LaplaceRegression","page":"Reference","title":"LaplaceRedux.LaplaceRegression","text":"MLJBase.@mlj_model mutable struct LaplaceRegression <: MLJFlux.MLJFluxProbabilistic\n\nA mutable struct representing a Laplace regression model that extends the MLJFlux.MLJFluxProbabilistic abstract type. It uses Laplace approximation to estimate the posterior distribution of the weights of a neural network. \n\nThe model is defined by the following default parameters for all MLJFlux models:\n\nbuilder: a Flux model that constructs the neural network.\noptimiser: a Flux optimiser.\nloss: a loss function that takes the predicted output and the true output as arguments.\nepochs: the number of epochs.\nbatch_size: the size of a batch.\nlambda: the regularization strength.\nalpha: the regularization mix (0 for all l2, 1 for all l1).\nrng: a random number generator.\noptimiser_changes_trigger_retraining: a boolean indicating whether changes in the optimiser trigger retraining.\nacceleration: the computational resource to use.\n\nThe model also has the following parameters, which are specific to the Laplace approximation:\n\nsubset_of_weights: the subset of weights to use, either :all, :last_layer, or :subnetwork.\nsubnetwork_indices: the indices of the subnetworks.\nhessian_structure: the structure of the Hessian matrix, either :full or :diagonal.\nbackend: the backend to use, either :GGN or :EmpiricalFisher.\nÏƒ: the standard deviation of the prior distribution.\nÎ¼â‚€: the mean of the prior distribution.\nPâ‚€: the covariance matrix of the prior distribution.\nret_distr: a boolean that tells predict to either return distributions (true) objects from Distributions.jl or just the probabilities.\nfit_prior_nsteps: the number of steps used to fit the priors.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.fit!-Tuple{LaplaceRedux.AbstractLaplace, Any}","page":"Reference","title":"LaplaceRedux.fit!","text":"fit!(la::AbstractLaplace,data)\n\nFits the Laplace approximation for a data set. The function returns the number of observations (n_data) that were used to update the Laplace object. It does not return the updated Laplace object itself because the function modifies the input Laplace object in place (as denoted by the use of '!' in the function's name).\n\nExamples\n\nusing Flux, LaplaceRedux\nx, y = LaplaceRedux.Data.toy_data_linear()\ndata = zip(x,y)\nnn = Chain(Dense(2,1))\nla = Laplace(nn)\nfit!(la, data)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.fit!-Tuple{LaplaceRedux.AbstractLaplace, MLUtils.DataLoader}","page":"Reference","title":"LaplaceRedux.fit!","text":"Fit the Laplace approximation, with batched data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.glm_predictive_distribution-Tuple{LaplaceRedux.AbstractLaplace, AbstractArray}","page":"Reference","title":"LaplaceRedux.glm_predictive_distribution","text":"glm_predictive_distribution(la::AbstractLaplace, X::AbstractArray)\n\nComputes the linearized GLM predictive.\n\nArguments\n\nla::AbstractLaplace: A Laplace object.\nX::AbstractArray: Input data.\n\nReturns\n\nnormal_distr A normal distribution N(fÎ¼,fvar) approximating the predictive distribution p(y|X) given the input data X.\nfÎ¼::AbstractArray: Mean of the predictive distribution. The output shape is column-major as in Flux.\nfvar::AbstractArray: Variance of the predictive distribution. The output shape is column-major as in Flux.\n\nExamples\n\n```julia-repl using Flux, LaplaceRedux using LaplaceRedux.Data: toydatalinear x, y = toydatalinear() data = zip(x,y) nn = Chain(Dense(2,1)) la = Laplace(nn; likelihood=:classification) fit!(la, data) glmpredictivedistribution(la, hcat(x...))\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.optimize_prior!-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.optimize_prior!","text":"optimize_prior!(\n    la::AbstractLaplace; \n    n_steps::Int=100, lr::Real=1e-1,\n    Î»init::Union{Nothing,Real}=nothing,\n    Ïƒinit::Union{Nothing,Real}=nothing\n)\n\nOptimize the prior precision post-hoc through Empirical Bayes (marginal log-likelihood maximization).\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.posterior_covariance","page":"Reference","title":"LaplaceRedux.posterior_covariance","text":"posterior_covariance(la::AbstractLaplace, P=la.P)\n\nComputes the posterior covariance  as the inverse of the posterior precision: Sigma=P^-1.\n\n\n\n\n\n","category":"function"},{"location":"reference/#LaplaceRedux.posterior_precision","page":"Reference","title":"LaplaceRedux.posterior_precision","text":"posterior_precision(la::AbstractLaplace, H=la.posterior.H, Pâ‚€=la.prior.Pâ‚€)\n\nComputes the posterior precision P for a fitted Laplace Approximation as follows,\n\nP = sum_n=1^Nnabla_theta^2 log p(mathcalD_ntheta)_hattheta + nabla_theta^2 log p(theta)_hattheta\n\nwhere sum_n=1^Nnabla_theta^2log p(mathcalD_ntheta)_hattheta=H is the Hessian and nabla_theta^2 log p(theta)_hattheta=P_0 is the prior precision and hattheta is the MAP estimate.\n\n\n\n\n\n","category":"function"},{"location":"reference/#LaplaceRedux.predict-Tuple{LaplaceRedux.AbstractLaplace, AbstractArray}","page":"Reference","title":"LaplaceRedux.predict","text":"predict(la::AbstractLaplace, X::AbstractArray; link_approx=:probit, predict_proba::Bool=true)\n\nComputes predictions from Bayesian neural network.\n\nArguments\n\nla::AbstractLaplace: A Laplace object.\nX::AbstractArray: Input data.\nlink_approx::Symbol=:probit: Link function approximation. Options are :probit and :plugin.\npredict_proba::Bool=true: If true (default) apply a sigmoid or a softmax function to the output of the Flux model.\nreturn_distr::Bool=false: if false (default), the function output either the direct output of the chain or pseudo-probabilities (if predict_proba= true).   if true predict return a Bernoulli distribution in binary classification tasks and a categorical distribution in multiclassification tasks.\n\nReturns\n\nFor classification tasks, LaplaceRedux provides different options: if retdistr is false:     - fÎ¼::AbstractArray: Mean of the predictive distribution if link function is set to :plugin, otherwise the probit approximation. The output shape is column-major as in Flux. if retdistr is true:     - a Bernoulli distribution in binary classification tasks and a categorical distribution in multiclassification tasks. For regression tasks:\n\nnormal_distr::Distributions.Normal:the array of Normal distributions computed by glmpredictivedistribution. \n\nExamples\n\nusing Flux, LaplaceRedux\nusing LaplaceRedux.Data: toy_data_linear\nx, y = toy_data_linear()\ndata = zip(x,y)\nnn = Chain(Dense(2,1))\nla = Laplace(nn; likelihood=:classification)\nfit!(la, data)\npredict(la, hcat(x...))\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{LaplaceClassification, Any, Any}","page":"Reference","title":"MLJModelInterface.predict","text":"predict(model::LaplaceClassification, Xnew)\n\nPredicts the class labels for new data using the LaplaceClassification model.\n\nArguments\n\nmodel::LaplaceClassification: The trained LaplaceClassification model.\nfitresult: the fitresult output produced by MLJFlux.fit!\nXnew: The new data to make predictions on.\n\nReturns\n\nAn array of predicted class labels.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{LaplaceRegression, Any, Any}","page":"Reference","title":"MLJModelInterface.predict","text":"predict(model::LaplaceRegression, Xnew)\n\nPredict the output for new input data using a Laplace regression model.\n\nArguments\n\nmodel::LaplaceRegression: The trained Laplace regression model.\nthe fitresult output produced by MLJFlux.fit!\nXnew: The new input data.\n\nReturns\n\nThe predicted output for the new input data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.CurvatureInterface","page":"Reference","title":"LaplaceRedux.Curvature.CurvatureInterface","text":"Base type for any curvature interface.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Internal-functions","page":"Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [\n    LaplaceRedux,\n    LaplaceRedux.Curvature,\n    LaplaceRedux.Data,\n]\nPublic = false","category":"page"},{"location":"reference/#LaplaceRedux.AbstractDecomposition","page":"Reference","title":"LaplaceRedux.AbstractDecomposition","text":"Abstract type of Hessian decompositions.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.AbstractLaplace","page":"Reference","title":"LaplaceRedux.AbstractLaplace","text":"Abstract base type for all Laplace approximations in this library. All subclasses implemented are parametric.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.AbstractLaplace-Tuple{AbstractArray}","page":"Reference","title":"LaplaceRedux.AbstractLaplace","text":"(la::AbstractLaplace)(X::AbstractArray)\n\nCalling a model with Laplace Approximation on an array of inputs is equivalent to explicitly calling the predict function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.EstimationParams","page":"Reference","title":"LaplaceRedux.EstimationParams","text":"EstimationParams\n\nContainer for the parameters of a Laplace approximation. \n\nFields\n\nsubset_of_weights::Symbol: the subset of weights to consider. Possible values are :all, :last_layer, and :subnetwork.\nsubnetwork_indices::Union{Nothing,Vector{Vector{Int}}}: the indices of the subnetwork. Possible values are nothing or a vector of vectors of integers.\nhessian_structure::HessianStructure: the structure of the Hessian. Possible values are :full and :kron or a concrete subtype of HessianStructure.\ncurvature::Union{Curvature.CurvatureInterface,Nothing}: the curvature interface. Possible values are nothing or a concrete subtype of CurvatureInterface.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.EstimationParams-Tuple{LaplaceRedux.LaplaceParams, Any, Symbol}","page":"Reference","title":"LaplaceRedux.EstimationParams","text":"EstimationParams(params::LaplaceParams)\n\nExtracts the estimation parameters from a LaplaceParams object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.FullHessian","page":"Reference","title":"LaplaceRedux.FullHessian","text":"Concrete type for full Hessian structure. This is the default structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.HessianStructure","page":"Reference","title":"LaplaceRedux.HessianStructure","text":"Abstract type for Hessian structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Kron","page":"Reference","title":"LaplaceRedux.Kron","text":"Kronecker-factored approximate curvature representation for a neural network model. Each element in kfacs represents two Kronecker factors (ð†, ð€), such that the full block Hessian approximation would be approximated as ð€âŠ—ð†.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.KronDecomposed","page":"Reference","title":"LaplaceRedux.KronDecomposed","text":"KronDecomposed\n\nDecomposed Kronecker-factored approximate curvature representation for a neural network model.\n\nDecomposition is required to add the prior (diagonal matrix) to the posterior (KronDecomposed). It also has the benefits of reducing the costs for computation of inverses and log-determinants.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.KronHessian","page":"Reference","title":"LaplaceRedux.KronHessian","text":"Concrete type for Kronecker-factored Hessian structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.LaplaceParams","page":"Reference","title":"LaplaceRedux.LaplaceParams","text":"LaplaceParams\n\nContainer for the parameters of a Laplace approximation.\n\nFields\n\nsubset_of_weights::Symbol: the subset of weights to consider. Possible values are :all, :last_layer, and :subnetwork.\nsubnetwork_indices::Union{Nothing,Vector{Vector{Int}}}: the indices of the subnetwork. Possible values are nothing or a vector of vectors of integers.\nhessian_structure::HessianStructure: the structure of the Hessian. Possible values are :full and :kron or a concrete subtype of HessianStructure.\nbackend::Symbol: the backend to use. Possible values are :GGN and :Fisher.\ncurvature::Union{Curvature.CurvatureInterface,Nothing}: the curvature interface. Possible values are nothing or a concrete subtype of CurvatureInterface.\nÏƒ::Real: the observation noise\nÎ¼â‚€::Real: the prior mean\nÎ»::Real: the prior precision\nPâ‚€::Union{Nothing,AbstractMatrix,UniformScaling}: the prior precision matrix\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Posterior","page":"Reference","title":"LaplaceRedux.Posterior","text":"Posterior\n\nContainer for the results of a Laplace approximation.\n\nFields\n\nÎ¼::AbstractVector: the MAP estimate of the parameters\nH::Union{AbstractArray,AbstractDecomposition,Nothing}: the Hessian matrix\nP::Union{AbstractArray,AbstractDecomposition,Nothing}: the posterior precision matrix\nÎ£::Union{AbstractArray,Nothing}: the posterior covariance matrix\nn_data::Union{Int,Nothing}: the number of data points\nn_params::Union{Int,Nothing}: the number of parameters\nn_out::Union{Int,Nothing}: the number of outputs\nloss::Real: the loss value\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Posterior-Tuple{Any, LaplaceRedux.EstimationParams}","page":"Reference","title":"LaplaceRedux.Posterior","text":"Posterior(model::Any, est_params::EstimationParams)\n\nOuter constructor for Posterior object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Prior","page":"Reference","title":"LaplaceRedux.Prior","text":"Prior\n\nContainer for the prior parameters of a Laplace approximation.\n\nFields\n\nÏƒ::Real: the observation noise\nÎ¼â‚€::Real: the prior mean\nÎ»::Real: the prior precision\nPâ‚€::Union{Nothing,AbstractMatrix,UniformScaling}: the prior precision matrix\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Prior-Tuple{LaplaceRedux.LaplaceParams, Any, Symbol}","page":"Reference","title":"LaplaceRedux.Prior","text":"Prior(params::LaplaceParams)\n\nExtracts the prior parameters from a LaplaceParams object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.:*-Tuple{LaplaceRedux.KronDecomposed, Number}","page":"Reference","title":"Base.:*","text":"Multiply by a scalar by changing the eigenvalues. Distribute the scalar along the factors of a block.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.:*-Tuple{Real, LaplaceRedux.Kron}","page":"Reference","title":"Base.:*","text":"Kronecker-factored curvature scalar scaling.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.:+-Tuple{LaplaceRedux.Kron, LaplaceRedux.Kron}","page":"Reference","title":"Base.:+","text":"Kronecker-factored curvature sum.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.:+-Tuple{LaplaceRedux.KronDecomposed, LinearAlgebra.Diagonal}","page":"Reference","title":"Base.:+","text":"Shift the factors by a diagonal (assumed uniform scaling)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.:+-Tuple{LaplaceRedux.KronDecomposed, Number}","page":"Reference","title":"Base.:+","text":"Shift the factors by a scalar across the diagonal.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.:==-Tuple{LaplaceRedux.Kron, LaplaceRedux.Kron}","page":"Reference","title":"Base.:==","text":"Kronecker-factored curvature equality.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.getindex-Tuple{LaplaceRedux.Kron, Int64}","page":"Reference","title":"Base.getindex","text":"Get Kronecker-factored block represenation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.getindex-Tuple{LaplaceRedux.KronDecomposed, Int64}","page":"Reference","title":"Base.getindex","text":"Get i-th block of a a Kronecker-factored curvature.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Base.length-Tuple{LaplaceRedux.KronDecomposed}","page":"Reference","title":"Base.length","text":"Number of blocks in a Kronecker-factored curvature.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Flux.params-Tuple{Any, LaplaceRedux.EstimationParams}","page":"Reference","title":"Flux.params","text":"Flux.params(model::Any, params::EstimationParams)\n\nExtracts the parameters of a model based on the subset of weights specified in the EstimationParams object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Flux.params-Tuple{Laplace}","page":"Reference","title":"Flux.params","text":"Flux.params(la::Laplace)\n\nOverloads the params function for a Laplace object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux._H_factor-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux._H_factor","text":"_H_factor(la::AbstractLaplace)\n\nReturns the factor Ïƒâ»Â², where Ïƒ is used in the zero-centered Gaussian prior p(Î¸) = N(Î¸;0,ÏƒÂ²I)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux._fit!-Tuple{Laplace, LaplaceRedux.FullHessian, Any}","page":"Reference","title":"LaplaceRedux._fit!","text":"_fit!(la::Laplace, hessian_structure::FullHessian, data; batched::Bool=false, batchsize::Int, override::Bool=true)\n\nFit a Laplace approximation to the posterior distribution of a model using the full Hessian.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux._fit!-Tuple{Laplace, LaplaceRedux.KronHessian, Any}","page":"Reference","title":"LaplaceRedux._fit!","text":"_fit!(la::Laplace, hessian_structure::KronHessian, data; batched::Bool=false, batchsize::Int, override::Bool=true)\n\nFit a Laplace approximation to the posterior distribution of a model using the Kronecker-factored Hessian.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux._init_H-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux._init_H","text":"_init_H(la::AbstractLaplace)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux._weight_penalty-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux._weight_penalty","text":"_weight_penalty(la::AbstractLaplace)\n\nThe weight penalty term is a regularization term used to prevent overfitting. Weight regularization methods such as weight decay introduce a penalty to the loss function when training a neural network to encourage the network to use small weights. Smaller weights in a neural network can result in a model that is more stable and less likely to overfit the training dataset, in turn having better performance when  making a prediction on new data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.approximate-Tuple{LaplaceRedux.Curvature.CurvatureInterface, LaplaceRedux.FullHessian, Tuple}","page":"Reference","title":"LaplaceRedux.approximate","text":"approximate(curvature::CurvatureInterface, hessian_structure::FullHessian, d::Tuple; batched::Bool=false)\n\nCompute the full approximation, for either a single input-output datapoint or a batch of such. \n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.approximate-Tuple{LaplaceRedux.Curvature.CurvatureInterface, LaplaceRedux.KronHessian, Any}","page":"Reference","title":"LaplaceRedux.approximate","text":"approximate(curvature::CurvatureInterface, hessian_structure::KronHessian, data; batched::Bool=false)\n\nCompute the eigendecomposed Kronecker-factored approximate curvature as the Fisher information matrix.\n\nNote, since the network predictive distribution is used in a weighted sum, and the number of backward passes is linear in the number of target classes, e.g. 100 for CIFAR-100.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.clamp-Tuple{LinearAlgebra.Eigen}","page":"Reference","title":"LaplaceRedux.clamp","text":"Clamp eigenvalues in an eigendecomposition to be non-negative.\n\nSince the Fisher information matrix is a positive-semidefinite by construction, the (near-zero) negative eigenvalues should be neglected.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.convert_subnetwork_indices-Tuple{Vector{Vector{Int64}}, AbstractArray}","page":"Reference","title":"LaplaceRedux.convert_subnetwork_indices","text":"convertsubnetworkindices(subnetwork_indices::AbstractArray)\n\nConverts the subnetwork indices from the user given format [theta, row, column] to an Int i that corresponds to the index of that weight in the flattened array of weights.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.decompose-Tuple{LaplaceRedux.Kron}","page":"Reference","title":"LaplaceRedux.decompose","text":"decompose(K::Kron)\n\nEigendecompose Kronecker factors and turn into KronDecomposed.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.functional_variance-Tuple{Any, Any}","page":"Reference","title":"LaplaceRedux.functional_variance","text":"functional_variance(la::AbstractLaplace, ð‰::AbstractArray)\n\nCompute the functional variance for the GLM predictive. Dispatches to the appropriate method based on the Hessian structure.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.functional_variance-Tuple{Laplace, LaplaceRedux.FullHessian, Any}","page":"Reference","title":"LaplaceRedux.functional_variance","text":"functional_variance(la::Laplace,ð‰)\n\nCompute the linearized GLM predictive variance as ð‰â‚™Î£ð‰â‚™' where ð‰=âˆ‡f(x;Î¸)|Î¸Ì‚ is the Jacobian evaluated at the MAP estimate and Î£ = Pâ»Â¹.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.functional_variance-Tuple{Laplace, LaplaceRedux.KronHessian, Matrix}","page":"Reference","title":"LaplaceRedux.functional_variance","text":"functionalvariance(la::Laplace, hessianstructure::KronHessian, ð‰::Matrix)\n\nCompute functional variance for the GLM predictive: as the diagonal of the KÃ—K predictive output covariance matrix ð‰ðâ»Â¹ð‰áµ€, where K is the number of outputs, ð is the posterior precision, and ð‰ is the Jacobian of model output ð‰=âˆ‡f(x;Î¸)|Î¸Ì‚.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.get_loss_fun-Tuple{Symbol, Flux.Chain}","page":"Reference","title":"LaplaceRedux.get_loss_fun","text":"get_loss_fun(likelihood::Symbol)\n\nHelper function to choose loss function based on specified model likelihood.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.get_loss_type-Tuple{Symbol, Flux.Chain}","page":"Reference","title":"LaplaceRedux.get_loss_type","text":"get_loss_type(likelihood::Symbol)\n\nChoose loss function type based on specified model likelihood.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.get_map_estimate-Tuple{Any, LaplaceRedux.EstimationParams}","page":"Reference","title":"LaplaceRedux.get_map_estimate","text":"get_map_estimate(model::Any, est_params::EstimationParams)\n\nHelper function to extract the MAP estimate of the parameters for the model based on the subset of weights specified in the EstimationParams object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.get_prior_mean-Tuple{Laplace}","page":"Reference","title":"LaplaceRedux.get_prior_mean","text":"get_prior_mean(la::Laplace)\n\nHelper function to extract the prior mean of the parameters from a Laplace approximation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.has_softmax_or_sigmoid_final_layer-Tuple{Flux.Chain}","page":"Reference","title":"LaplaceRedux.has_softmax_or_sigmoid_final_layer","text":"has_softmax_or_sigmoid_final_layer(model::Flux.Chain)\n\nCheck if the FLux model ends with a sigmoid or with a softmax layer\n\nInput:     - model: the Flux Chain object that represent the neural network. Return:     - has_finaliser: true if the check is positive, false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.hessian_approximation-Tuple{LaplaceRedux.AbstractLaplace, Any}","page":"Reference","title":"LaplaceRedux.hessian_approximation","text":"hessian_approximation(la::AbstractLaplace, d; batched::Bool=false)\n\nComputes the local Hessian approximation at a single datapoint d.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.instantiate_curvature!-Tuple{LaplaceRedux.EstimationParams, Any, Symbol, Symbol}","page":"Reference","title":"LaplaceRedux.instantiate_curvature!","text":"instantiate_curvature!(params::EstimationParams, model::Any, likelihood::Symbol, backend::Symbol)\n\nInstantiates the curvature interface for a Laplace approximation. The curvature interface is a concrete subtype of CurvatureInterface and is used to compute the Hessian matrix. The curvature interface is stored in the curvature field of the EstimationParams object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.interleave-Tuple","page":"Reference","title":"LaplaceRedux.interleave","text":"Interleave elements of multiple iterables in order provided.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.inv_square_form-Tuple{LaplaceRedux.KronDecomposed, Matrix}","page":"Reference","title":"LaplaceRedux.inv_square_form","text":"function invsquareform(K::KronDecomposed, W::Matrix)\n\nSpecial function to compute the inverse square form ð‰ðâ»Â¹ð‰áµ€ (or ð–ðŠâ»Â¹ð–áµ€)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.log_det_posterior_precision-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.log_det_posterior_precision","text":"log_det_posterior_precision(la::AbstractLaplace)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.log_det_prior_precision-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.log_det_prior_precision","text":"log_det_prior_precision(la::AbstractLaplace)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.log_det_ratio-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.log_det_ratio","text":"log_det_ratio(la::AbstractLaplace)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.log_likelihood-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.log_likelihood","text":"log_likelihood(la::AbstractLaplace)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.log_marginal_likelihood-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.log_marginal_likelihood","text":"log_marginal_likelihood(la::AbstractLaplace; Pâ‚€::Union{Nothing,UniformScaling}=nothing, Ïƒ::Union{Nothing, Real}=nothing)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.logdetblock-Tuple{Tuple{LinearAlgebra.Eigen, LinearAlgebra.Eigen}, Number}","page":"Reference","title":"LaplaceRedux.logdetblock","text":"logdetblock(block::Tuple{Eigen,Eigen}, delta::Number)\n\nLog-determinant of a block in KronDecomposed, shifted by delta by on the diagonal.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.mm-Tuple{LaplaceRedux.KronDecomposed, Any}","page":"Reference","title":"LaplaceRedux.mm","text":"Matrix-multuply for the KronDecomposed Hessian approximation K and a 2-d matrix W, applying an exponent to K and transposing W before multiplication. Return (K^x)W^T, where x is the exponent.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.n_params-Tuple{Any, LaplaceRedux.EstimationParams}","page":"Reference","title":"LaplaceRedux.n_params","text":"n_params(model::Any, params::EstimationParams)\n\nHelper function to determine the number of parameters of a Flux.Chain with Laplace approximation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.n_params-Tuple{Laplace}","page":"Reference","title":"LaplaceRedux.n_params","text":"LaplaceRedux.n_params(la::Laplace)\n\nOverloads the n_params function for a Laplace object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.outdim-Tuple{Flux.Chain}","page":"Reference","title":"LaplaceRedux.outdim","text":"outdim(model::Chain)\n\nHelper function to determine the output dimension of a Flux.Chain, corresponding to the number of neurons on the last layer of the NN.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.outdim-Tuple{LaplaceRedux.AbstractLaplace}","page":"Reference","title":"LaplaceRedux.outdim","text":"outdim(la::AbstractLaplace)\n\nHelper function to determine the output dimension, corresponding to the number of neurons  on the last layer of the NN, of a Flux.Chain with Laplace approximation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.prior_precision-Tuple{Laplace}","page":"Reference","title":"LaplaceRedux.prior_precision","text":"prior_precision(la::Laplace)\n\nHelper function to extract the prior precision matrix from a Laplace approximation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.probit-Tuple{AbstractArray, AbstractArray}","page":"Reference","title":"LaplaceRedux.probit","text":"probit(fÎ¼::AbstractArray, fvar::AbstractArray)\n\nCompute the probit approximation of the predictive distribution.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.validate_subnetwork_indices-Tuple{Union{Nothing, Vector{Vector{Int64}}}, Any}","page":"Reference","title":"LaplaceRedux.validate_subnetwork_indices","text":"validatesubnetworkindices( subnetwork_indices::Union{Nothing,Vector{Vector{Int}}}, params )\n\nDetermines whether subnetwork_indices is a valid input for specified parameters.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LinearAlgebra.det-Tuple{LaplaceRedux.KronDecomposed}","page":"Reference","title":"LinearAlgebra.det","text":"det(K::KronDecomposed)\n\nLog-determinant of the KronDecomposed block-diagonal matrix, as the exponentiated log-determinant.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LinearAlgebra.logdet-Tuple{LaplaceRedux.KronDecomposed}","page":"Reference","title":"LinearAlgebra.logdet","text":"logdet(K::KronDecomposed)\n\nLog-determinant of the KronDecomposed block-diagonal matrix, as the product of the determinants of the blocks\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.build-Tuple{LaplaceClassification, Any, Any}","page":"Reference","title":"MLJFlux.build","text":"MLJFlux.build(model::LaplaceClassification, rng, shape)\n\nBuilds an MLJFlux model for Laplace classification compatible with the dimensions of the input and output layers specified by shape.\n\nArguments\n\nmodel::LaplaceClassification: The Laplace classification model.\nrng: A random number generator to ensure reproducibility.\nshape: A tuple or array specifying the dimensions of the input and output layers.\n\nReturns\n\nThe constructed MLJFlux model, compatible with the specified input and output dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.build-Tuple{LaplaceRegression, Any, Any}","page":"Reference","title":"MLJFlux.build","text":"MLJFlux.build(model::LaplaceRegression, rng, shape)\n\nBuilds an MLJFlux model for Laplace regression compatible with the dimensions of the input and output layers specified by shape.\n\nArguments\n\nmodel::LaplaceRegression: The Laplace regression model.\nrng: A random number generator to ensure reproducibility.\nshape: A tuple or array specifying the dimensions of the input and output layers.\n\nReturns\n\nThe constructed MLJFlux model, compatible with the specified input and output dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.fitresult-Tuple{LaplaceClassification, Any, Any}","page":"Reference","title":"MLJFlux.fitresult","text":"MLJFlux.fitresult(model::LaplaceClassification, chain, y)\n\nComputes the fit result for a Laplace classification model, returning the model chain and the number of unique classes in the target data.\n\nArguments\n\nmodel::LaplaceClassification: The Laplace classification model to be evaluated.\nchain: The trained model chain.\ny: The target data, typically a vector of class labels.\n\nReturns\n\nReturns\n\nA tuple containing:\n\nThe trained Flux chain.\na deepcopy of the laplace model.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.fitresult-Tuple{LaplaceRegression, Any, Any}","page":"Reference","title":"MLJFlux.fitresult","text":"MLJFlux.fitresult(model::LaplaceRegression, chain, y)\n\nComputes the fit result for a Laplace Regression model, returning the model chain and the number of output variables in the target data.\n\nArguments\n\nmodel::LaplaceRegression: The Laplace Regression model to be evaluated.\nchain: The trained model chain.\ny: The target data, typically a vector of class labels.\n\nReturns\n\nA tuple containing:\n\nThe trained Flux chain.\na deepcopy of the laplace model.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.shape-Tuple{LaplaceRegression, Any, Any}","page":"Reference","title":"MLJFlux.shape","text":"MLJFlux.shape(model::LaplaceRegression, X, y)\n\nCompute the the number of features of the X input dataset and  the number of variables to predict from  the y  output dataset.\n\nArguments\n\nmodel::LaplaceRegression: The LaplaceRegression model to fit.\nX: The input data for training.\ny: The target labels for training one-hot encoded.\n\nReturns\n\n(input size, output size)\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.train-Tuple{LaplaceClassification, Vararg{Any, 7}}","page":"Reference","title":"MLJFlux.train","text":"MLJFlux.train(model::LaplaceClassification, chain, regularized_optimiser, optimiser_state, epochs, verbosity, X, y)\n\nFit the LaplaceRegression model using Flux.jl.\n\nArguments\n\nmodel::LaplaceClassification: The LaplaceClassification model.\nregularized_optimiser: the regularized optimiser to apply to the loss function.\noptimiser_state: thestate of the optimiser.\nepochs: The number of epochs for training.\nverbosity: The verbosity level for training.\nX: The input data for training.\ny: The target labels for training.\n\nReturns (fitresult, cache, report )\n\nwhere\n\nla: the fitted Laplace model.\noptimiser_state: the state of the optimiser.\nhistory: the training loss history.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJFlux.train-Tuple{LaplaceRegression, Vararg{Any, 7}}","page":"Reference","title":"MLJFlux.train","text":"MLJFlux.train(model::LaplaceRegression, chain, regularized_optimiser, optimiser_state, epochs, verbosity, X, y)\n\nFit the LaplaceRegression model using Flux.jl.\n\nArguments\n\nmodel::LaplaceRegression: The LaplaceRegression model.\nregularized_optimiser: the regularized optimiser to apply to the loss function.\noptimiser_state: thestate of the optimiser.\nepochs: The number of epochs for training.\nverbosity: The verbosity level for training.\nX: The input data for training.\ny: The target labels for training.\n\nReturns (la, optimiser_state, history )\n\nwhere\n\nla: the fitted Laplace model.\noptimiser_state: the state of the optimiser.\nhistory: the training loss history.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.@zb-Tuple{Any}","page":"Reference","title":"LaplaceRedux.@zb","text":"Macro for zero-based indexing. Example of usage: (@zb A[0]) = ...\n\n\n\n\n\n","category":"macro"},{"location":"reference/#LaplaceRedux.Curvature.EmpiricalFisher","page":"Reference","title":"LaplaceRedux.Curvature.EmpiricalFisher","text":"Constructor for curvature approximated by empirical Fisher.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Curvature.GGN","page":"Reference","title":"LaplaceRedux.Curvature.GGN","text":"Constructor for curvature approximated by Generalized Gauss-Newton.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Curvature.full_batched-Tuple{LaplaceRedux.Curvature.EmpiricalFisher, Tuple}","page":"Reference","title":"LaplaceRedux.Curvature.full_batched","text":"full_batched(curvature::EmpiricalFisher, d::Tuple)\n\nCompute the full empirical Fisher for batch of inputs-outputs, with the batch dimension at the end.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.full_batched-Tuple{LaplaceRedux.Curvature.GGN, Tuple}","page":"Reference","title":"LaplaceRedux.Curvature.full_batched","text":"full_batched(curvature::GGN, d::Tuple)\n\nCompute the full GGN for batch of inputs-outputs, with the batch dimension at the end.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.full_unbatched-Tuple{LaplaceRedux.Curvature.EmpiricalFisher, Tuple}","page":"Reference","title":"LaplaceRedux.Curvature.full_unbatched","text":"full_unbatched(curvature::EmpiricalFisher, d::Tuple)\n\nCompute the full empirical Fisher for a single datapoint.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.full_unbatched-Tuple{LaplaceRedux.Curvature.GGN, Tuple}","page":"Reference","title":"LaplaceRedux.Curvature.full_unbatched","text":"full_unbatched(curvature::GGN, d::Tuple)\n\nCompute the full GGN for a singular input-ouput datapoint. \n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.gradients-Tuple{LaplaceRedux.Curvature.CurvatureInterface, AbstractArray, Union{Number, AbstractArray}}","page":"Reference","title":"LaplaceRedux.Curvature.gradients","text":"gradients(curvature::CurvatureInterface, X::AbstractArray, y::Number)\n\nCompute the gradients with respect to the loss function: âˆ‡â„“(f(x;Î¸),y) where f: â„á´° â†¦ â„á´·.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.jacobians-Tuple{LaplaceRedux.Curvature.CurvatureInterface, AbstractArray}","page":"Reference","title":"LaplaceRedux.Curvature.jacobians","text":"jacobians(curvature::CurvatureInterface, X::AbstractArray; batched::Bool=false)\n\nComputes the Jacobian âˆ‡f(x;Î¸) where f: â„á´° â†¦ â„á´·.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.jacobians_batched-Tuple{LaplaceRedux.Curvature.CurvatureInterface, AbstractArray}","page":"Reference","title":"LaplaceRedux.Curvature.jacobians_batched","text":"jacobians_batched(curvature::CurvatureInterface, X::AbstractArray)\n\nCompute Jacobians of the model output w.r.t. model parameters for points in X, with batching.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.jacobians_unbatched-Tuple{LaplaceRedux.Curvature.CurvatureInterface, AbstractArray}","page":"Reference","title":"LaplaceRedux.Curvature.jacobians_unbatched","text":"jacobians_unbatched(curvature::CurvatureInterface, X::AbstractArray)\n\nCompute the Jacobian of the model output w.r.t. model parameters for the point X, without batching. Here, the nn function is wrapped in an anonymous function using the () -> syntax, which allows it to be differentiated using automatic differentiation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Data.toy_data_linear","page":"Reference","title":"LaplaceRedux.Data.toy_data_linear","text":"toy_data_linear(N=100)\n\nExamples\n\ntoy_data_linear()\n\n\n\n\n\n","category":"function"},{"location":"reference/#LaplaceRedux.Data.toy_data_multi","page":"Reference","title":"LaplaceRedux.Data.toy_data_multi","text":"toy_data_multi(N=100)\n\nExamples\n\ntoy_data_multi()\n\n\n\n\n\n","category":"function"},{"location":"reference/#LaplaceRedux.Data.toy_data_non_linear","page":"Reference","title":"LaplaceRedux.Data.toy_data_non_linear","text":"toy_data_non_linear(N=100)\n\nExamples\n\ntoy_data_non_linear()\n\n\n\n\n\n","category":"function"},{"location":"reference/#LaplaceRedux.Data.toy_data_regression","page":"Reference","title":"LaplaceRedux.Data.toy_data_regression","text":"toy_data_regression(N=25, p=1; noise=0.3, fun::Function=f(x)=sin(2 * Ï€ * x))\n\nA helper function to generate synthetic data for regression.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation for LaplaceRedux.jl.","category":"page"},{"location":"#LaplaceRedux","page":"Home","title":"LaplaceRedux","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LaplaceRedux.jl is a library written in pure Julia that can be used for effortless Bayesian Deep Learning through Laplace Approximation (LA). In the development of this package I have drawn inspiration from this Python library and its companion paper (Daxberger et al. 2021).","category":"page"},{"location":"#Installation","page":"Home","title":"ðŸš© Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The stable version of this package can be installed as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"LaplaceRedux.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"The development version can be installed like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl\")","category":"page"},{"location":"#Getting-Started","page":"Home","title":"ðŸƒ Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are new to Deep Learning in Julia or simply prefer learning through videos, check out this awesome YouTube tutorial by doggo.jl ðŸ¶. Additionally, you can also find a video of my presentation at JuliaCon 2022 on YouTube.","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"ðŸ–¥ï¸ Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LaplaceRedux.jl can be used for any neural network trained in Flux.jl. Below we show basic usage examples involving two simple models for a regression and a classification task, respectively.","category":"page"},{"location":"#Regression","page":"Home","title":"Regression","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A complete worked example for a regression model can be found in the docs. Here we jump straight to Laplace Approximation and take the pre-trained model nn as given. Then LA can be implemented as follows, where we specify the model likelihood. The plot shows the fitted values overlaid with a 95% confidence interval. As expected, predictive uncertainty quickly increases in areas that are not populated by any training data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"la = Laplace(nn; likelihood=:regression)\nfit!(la, data)\noptimize_prior!(la)\nplot(la, X, y; zoom=-5, size=(500,500))","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#Binary-Classification","page":"Home","title":"Binary Classification","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once again we jump straight to LA and refer to the docs for a complete worked example involving binary classification. In this case we need to specify likelihood=:classification. The plot below shows the resulting posterior predictive distributions as contours in the two-dimensional feature space: note how the Plugin Approximation on the left compares to the Laplace Approximation on the right.","category":"page"},{"location":"","page":"Home","title":"Home","text":"la = Laplace(nn; likelihood=:classification)\nfit!(la, data)\nla_untuned = deepcopy(la)   # saving for plotting\noptimize_prior!(la; n_steps=100)\n\n# Plot the posterior predictive distribution:\nzoom=0\np_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1))\np_untuned = plot(la_untuned, X, ys; title=\"LA - raw (Î»=$(unique(diag(la_untuned.prior.Pâ‚€))[1]))\", clim=(0,1), zoom=zoom)\np_laplace = plot(la, X, ys; title=\"LA - tuned (Î»=$(round(unique(diag(la.prior.Pâ‚€))[1],digits=2)))\", clim=(0,1), zoom=zoom)\nplot(p_plugin, p_untuned, p_laplace, layout=(1,3), size=(1700,400))","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#JuliaCon-2022","page":"Home","title":"ðŸ“¢ JuliaCon 2022","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This project was presented at JuliaCon 2022 in July 2022. See here for details.","category":"page"},{"location":"#Contribute","page":"Home","title":"ðŸ› ï¸ Contribute","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions are very much welcome! Please follow the SciML ColPrac guide. You may want to start by having a look at any open issues.","category":"page"},{"location":"#References","page":"Home","title":"ðŸŽ“ References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Daxberger, Erik, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. 2021. â€œLaplace Redux-Effortless Bayesian Deep Learning.â€ Advances in Neural Information Processing Systems 34.","category":"page"},{"location":"resources/_resources/#Additional-Resources","page":"Additional Resources","title":"Additional Resources","text":"","category":"section"},{"location":"resources/_resources/#JuliaCon-2022","page":"Additional Resources","title":"JuliaCon 2022","text":"","category":"section"},{"location":"resources/_resources/","page":"Additional Resources","title":"Additional Resources","text":"Slides: link","category":"page"},{"location":"resources/_resources/","page":"Additional Resources","title":"Additional Resources","text":"<iframe style=\"width:560px;height:315px\" src=\"https://www.paltmeyer.com/LaplaceRedux.jl/dev/resources/juliacon22/presentation.html\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"}]
}

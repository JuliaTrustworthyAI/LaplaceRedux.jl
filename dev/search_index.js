var documenterSearchIndex = {"docs":
[{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"CurrentModule = BayesLaplace","category":"page"},{"location":"logit/#Bayesian-Logistic-Regression","page":"Logistic Regression","title":"Bayesian Logistic Regression","text":"","category":"section"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"# Import libraries.\nusing Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace\ntheme(:juno)","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"We will use synthetic data with linearly separable samples:","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"# Number of points to generate.\nxs, y = toy_data_linear(100)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y);","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"Logisitic regression with weight decay can be implemented in Flux.jl as a single dense (linear) layer with binary logit crossentropy loss:","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"nn = Chain(Dense(2,1))\nŒª = 0.5\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"The code below simply trains the model. After about 50 training epochs training loss stagnates.","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"using Flux.Optimise: update!, ADAM\nopt = ADAM()\nepochs = 50\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n\nusing Plots\nanim = Animation()\nplt = plot(ylim=(0,avg_loss(data)), xlim=(0,epochs), legend=false, xlab=\"Epoch\")\navg_l = []\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\n  avg_l = vcat(avg_l,avg_loss(data))\n  plot!(plt, avg_l, color=1, title=\"Average (training) loss\")\n  frame(anim, plt)\nend\n\ngif(anim, \"www/nn_training.gif\");","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"(Image: )","category":"page"},{"location":"logit/#Laplace-appoximation","page":"Logistic Regression","title":"Laplace appoximation","text":"","category":"section"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"Laplace approximation for the posterior predictive can be implemented as follows:","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"la = laplace(nn, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data);\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin);\np_laplace = plot_contour(X',y,la;title=\"Laplace\");","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive.png\")","category":"page"},{"location":"logit/","page":"Logistic Regression","title":"Logistic Regression","text":"(Image: )","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"CurrentModule = BayesLaplace","category":"page"},{"location":"mlp/#Bayesian-MLP","page":"MLP","title":"Bayesian MLP","text":"","category":"section"},{"location":"mlp/","page":"MLP","title":"MLP","text":"# Import libraries.\nusing Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace\ntheme(:juno)","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"This time we use a synthetic dataset containing samples that are not linearly separable:","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"# Number of points to generate.\nxs, y = toy_data_non_linear(200)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y);","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"For the classification task we build a neural network with weight decay composed of a single hidden layer.","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"n_hidden = 32\nD = size(X)[1]\nnn = Chain(\n    Dense(D, n_hidden, œÉ),\n    Dense(n_hidden, 1)\n)  \nŒª = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"The model is trained for 200 epochs before the training loss stagnates.","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"using Flux.Optimise: update!, ADAM\nopt = ADAM()\nepochs = 200\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n\nusing Plots\nanim = Animation()\nplt = plot(ylim=(0,avg_loss(data)), xlim=(0,epochs), legend=false, xlab=\"Epoch\")\navg_l = []\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\n  avg_l = vcat(avg_l,avg_loss(data))\n  plot!(plt, avg_l, color=1, title=\"Average (training) loss\")\n  frame(anim, plt)\nend\n\ngif(anim, \"www/nn_training_mlp.gif\");","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"(Image: )","category":"page"},{"location":"mlp/#Laplace-appoximation","page":"MLP","title":"Laplace appoximation","text":"","category":"section"},{"location":"mlp/","page":"MLP","title":"MLP","text":"Laplace approximation can be implemented as follows:","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"la = laplace(nn, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data);\nzoom=0\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\np_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_mlp.png\")","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"(Image: )","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"Zooming out we can note that the plugin estimator produces high-confidence estimates in regions scarce of any samples. The Laplace approximation is much more conservative about these regions.","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"zoom=-50\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\np_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_mlp_zoomed.png\");","category":"page"},{"location":"mlp/","page":"MLP","title":"MLP","text":"(Image: )","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = BayesLaplace","category":"page"},{"location":"reference/#All-functions-and-types","page":"Reference","title":"All functions and types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#Exported-functions","page":"Reference","title":"Exported functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [BayesLaplace, BayesLaplace.Curvature]\nPrivate = false","category":"page"},{"location":"reference/#Internal-functions","page":"Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [BayesLaplace, BayesLaplace.Curvature]\nPublic = false","category":"page"},{"location":"reference/#BayesLaplace.Curvature.full-Tuple{BayesLaplace.Curvature.EmpiricalFisher, Tuple}","page":"Reference","title":"BayesLaplace.Curvature.full","text":"full(ùë™::EmpiricalFisher, d::Tuple)\n\nCompute the full empirical Fisher.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BayesLaplace.Curvature.jacobians-Tuple{BayesLaplace.Curvature.CurvatureInterface, AbstractArray}","page":"Reference","title":"BayesLaplace.Curvature.jacobians","text":"jacobians(ùë™::CurvatureInterface, X::AbstractArray)\n\nComputes the Jacobian ‚àáf(x;Œ∏).\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = BayesLaplace","category":"page"},{"location":"#BayesLaplace","page":"Home","title":"BayesLaplace","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for BayesLaplace.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is a small library that can be used for effortless Bayesian Deep Learning and Logisitic Regression trough Laplace Approximation. It is inspired by this Python library and its companion paper.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is not registered, but can be installed from Github as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/pat-alt/BayesLaplace.jl\")","category":"page"},{"location":"#Limitations","page":"Home","title":"Limitations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This library is pure-play and lacks any kind of unit testing. It is also limited to binary classification problems. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [BayesLaplace]","category":"page"},{"location":"#BayesLaplace.fit!-Tuple{BayesLaplace.LaplaceRedux, Any}","page":"Home","title":"BayesLaplace.fit!","text":"fit!(ùë≥::LaplaceRedux,data)\n\nFits the Laplace approximation for a data set.\n\nExamples\n\nusing Flux, BayesLaplace\nx, y = toy_data_linear()\ndata = zip(x,y)\nnn = Chain(Dense(2,1))\nla = laplace(nn)\nfit!(la, data)\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.get_params-Tuple{BayesLaplace.LaplaceRedux}","page":"Home","title":"BayesLaplace.get_params","text":"get_params(ùë≥::LaplaceRedux)\n\nRetrieves the desired (sub)set of model parameters and stores them in a list.\n\nExamples\n\nusing Flux, BayesLaplace\nnn = Chain(Dense(2,1))\nla = laplace(nn)\nBayesLaplace.get_params(la)\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.glm_predictive_distribution-Tuple{BayesLaplace.LaplaceRedux, AbstractArray}","page":"Home","title":"BayesLaplace.glm_predictive_distribution","text":"glm_predictive_distribution(ùë≥::LaplaceRedux, X::AbstractArray)\n\nComputes the linearized GLM predictive.\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.hessian_approximation-Tuple{BayesLaplace.LaplaceRedux, Any}","page":"Home","title":"BayesLaplace.hessian_approximation","text":"hessian_approximation(ùë≥::LaplaceRedux, d)\n\nComputes the local Hessian approximation at a single data d.\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.laplace-Tuple{Any}","page":"Home","title":"BayesLaplace.laplace","text":"laplace(model::Any; loss_type=:logitbinarycrossentropy, subset_of_weights=:last_layer, hessian_structure=:full,backend=:EmpiricalFisher,Œª=1)\n\nWrapper function to prepare Laplace approximation.\n\nExamples\n\nusing Flux, BayesLaplace\nnn = Chain(Dense(2,1))\nla = laplace(nn)\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.plot_contour-Tuple{Any, Any, Any}","page":"Home","title":"BayesLaplace.plot_contour","text":"plot_contour(X,y,ùë¥;clegend=true,title=\"\",length_out=50,type=:laplace,zoom=0,xlim=nothing,ylim=nothing)\n\nGenerates a contour plot for the posterior predictive surface.  \n\nExamples\n\nusing BayesLaplace, Plots\nimport BayesLaplace: predict\nusing NNlib: œÉ\nX, y = toy_data_linear(100)\nX = hcat(X...)'\nŒ≤ = [1,1]\nùë¥ =(Œ≤=Œ≤,)\npredict(ùë¥, X) = œÉ.(ùë¥.Œ≤' * X)\nplot_contour(X, y, ùë¥)\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.plot_data!-Tuple{Any, Any, Any}","page":"Home","title":"BayesLaplace.plot_data!","text":"plot_data!(plt,X,y)\n\nExamples\n\nusing BayesLaplace, Plots\nX, y = toy_data_linear(100)\nplt = plot()\nplot_data!(plt, hcat(X...)', y)\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.plugin-Tuple{BayesLaplace.LaplaceRedux, AbstractArray}","page":"Home","title":"BayesLaplace.plugin","text":"plugin(ùë≥::LaplaceRedux, X::AbstractArray)\n\nComputes the plugin estimate.\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.predict-Tuple{BayesLaplace.LaplaceRedux, AbstractArray}","page":"Home","title":"BayesLaplace.predict","text":"predict(ùë≥::LaplaceRedux, X::AbstractArray; link_approx=:probit)\n\nComputes predictions from Bayesian neural network.\n\nExamples\n\nusing Flux, BayesLaplace\nx, y = toy_data_linear()\ndata = zip(x,y)\nnn = Chain(Dense(2,1))\nla = laplace(nn)\nfit!(la, data)\npredict(la, hcat(x...))\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.predictive_variance-Tuple{BayesLaplace.LaplaceRedux, Any}","page":"Home","title":"BayesLaplace.predictive_variance","text":"predictive_variance(ùë≥::LaplaceRedux,ùêâ)\n\nCompute the linearized GLM predictive variance as ùêâ‚ÇôŒ£ÃÇùêâ‚Çô' where ùêâ=‚àáf(x;Œ∏)|Œ∏ÃÇ is the Jacobian evaluated at the MAP estimate and Œ£ÃÇ = ùêá‚Åª¬π.\n\n\n\n\n\n","category":"method"},{"location":"#BayesLaplace.toy_data_linear","page":"Home","title":"BayesLaplace.toy_data_linear","text":"toy_data_linear(N=100)\n\nExamples\n\ntoy_data_linear()\n\n\n\n\n\n","category":"function"},{"location":"#BayesLaplace.toy_data_non_linear","page":"Home","title":"BayesLaplace.toy_data_non_linear","text":"toy_data_non_linear(N=100)\n\nExamples\n\ntoy_data_non_linear()\n\n\n\n\n\n","category":"function"}]
}

var documenterSearchIndex = {"docs":
[{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/mlp/#Bayesian-MLP","page":"MLP Binary Classifier","title":"Bayesian MLP","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"This time we use a synthetic dataset containing samples that are not linearly separable:","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"# Number of points to generate.\nxs, ys = LaplaceRedux.Data.toy_data_non_linear(200)\nX = hcat(xs...) # bring into tabular format\ndata = zip(xs,ys)","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"For the classification task we build a neural network with weight decay composed of a single hidden layer.","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"n_hidden = 32\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, œÉ),\n    Dense(n_hidden, 1)\n)  \nŒª = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"The model is trained for 200 epochs before the training loss stagnates.","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 200\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/mlp/#Laplace-Approximation","page":"MLP Binary Classifier","title":"Laplace Approximation","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"Laplace approximation can be implemented as follows:","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"la = Laplace(nn; likelihood=:classification, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data)","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"# Plot the posterior distribution with a contour plot.\nzoom=0\np_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1), zoom=zoom)\np_laplace = plot(la, X, ys; title=\"Laplace\", clim=(0,1), zoom=zoom)\nplot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"(Image: )","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"Zooming out we can note that the plugin estimator produces high-confidence estimates in regions scarce of any samples. The Laplace approximation is much more conservative about these regions.","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"zoom=-50\np_plugin = plot(la, X, ys; title=\"Plugin\", link_approx=:plugin, clim=(0,1), zoom=zoom)\np_laplace = plot(la, X, ys; title=\"Laplace\", clim=(0,1), zoom=zoom)\nplot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))","category":"page"},{"location":"tutorials/mlp/","page":"MLP Binary Classifier","title":"MLP Binary Classifier","text":"(Image: )","category":"page"},{"location":"resources/resources/#Additional-Resources","page":"Additional Resources","title":"Additional Resources","text":"","category":"section"},{"location":"resources/resources/#JuliaCon-2022","page":"Additional Resources","title":"JuliaCon 2022","text":"","category":"section"},{"location":"resources/resources/","page":"Additional Resources","title":"Additional Resources","text":"Slides: link","category":"page"},{"location":"resources/resources/","page":"Additional Resources","title":"Additional Resources","text":"<iframe style=\"width:560px;height:315px\" src=\"https://www.paltmeyer.com/LaplaceRedux.jl/dev/resources/juliacon22/presentation.html\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/logit/#Bayesian-Logisitic-Regression","page":"Logistic Regression","title":"Bayesian Logisitic Regression","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"We will use synthetic data with linearly separable samples:","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"# Number of points to generate.\nxs, ys = LaplaceRedux.Data.toy_data_linear(100)\nX = hcat(xs...) # bring into tabular format\ndata = zip(xs,ys)","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"Logistic regression with weight decay can be implemented in Flux.jl as a single dense (linear) layer with binary logit crossentropy loss:","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"nn = Chain(Dense(2,1))\nŒª = 0.5\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"The code below simply trains the model. After about 50 training epochs training loss stagnates.","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 50\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/logit/#Laplace-appoximation","page":"Logistic Regression","title":"Laplace appoximation","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"Laplace approximation for the posterior predictive can be implemented as follows:","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"la = Laplace(nn; likelihood=:classification, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data)","category":"page"},{"location":"tutorials/logit/","page":"Logistic Regression","title":"Logistic Regression","text":"The plot below shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right).","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/prior/#A-quick-note-on-the-prior","page":"A note on the prior ...","title":"A quick note on the prior","text":"","category":"section"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"Low prior uncertainty ‚Üí posterior dominated by prior. High prior uncertainty ‚Üí posterior approaches MLE.","category":"page"},{"location":"tutorials/prior/","page":"A note on the prior ...","title":"A note on the prior ...","text":"# Number of points to generate:\nxs, y = LaplaceRedux.Data.toy_data_non_linear(200)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y)","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/regression/#Data","page":"MLP Regression","title":"Data","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"We first generate some synthetic data:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"using LaplaceRedux.Data\nfun(x) = sin(2*œÄ*x)\nn = 100     # number of observations\nœÉtrue = 0.3       # true observational noise\nx, y = Data.toy_data_regression(100;noise=œÉtrue,fun=fun)\nxs = [[x] for x in x]\nX = permutedims(x)","category":"page"},{"location":"tutorials/regression/#MLP","page":"MLP Regression","title":"MLP","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"We set up a model and loss with weight regularization:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"data = zip(xs,y)\nn_hidden = 50\nD = size(X,1)\nnn = Chain(\n    Dense(D, n_hidden, tanh_fast),\n    Dense(n_hidden, n_hidden, tanh_fast),\n    Dense(n_hidden, 1)\n)  \nŒª = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.mse(nn(x), y) + weight_regularization();","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"We train the model:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/regression/#Laplace-Approximation","page":"MLP Regression","title":"Laplace Approximation","text":"","category":"section"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"Laplace approximation can be implemented as follows:","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"la = Laplace(nn; likelihood=:regression, Œª=Œª, subset_of_weights=:last_layer, œÉ=œÉtrue)\nfit!(la, data)\nplot(la, X, y)","category":"page"},{"location":"tutorials/regression/","page":"MLP Regression","title":"MLP Regression","text":"(Image: )","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"tutorials/multi/#Multi-class-problem","page":"MLP Multi-Label Classifier","title":"Multi-class problem","text":"","category":"section"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"using LaplaceRedux.Data\nx, y = Data.toy_data_multi()\nX = hcat(x...)\ny_train = Flux.onehotbatch(y, unique(y))\ny_train = Flux.unstack(y_train',1)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"data = zip(x,y_train)\nn_hidden = 32\nD = size(X,1)\nout_dim = length(unique(y))\nnn = Chain(\n    Dense(D, n_hidden, œÉ),\n    Dense(n_hidden, out_dim)\n)  \nŒª = 0.1\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitcrossentropy(nn(x), y) + weight_regularization();","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 200\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n    for d in data\n        gs = gradient(Flux.params(nn)) do\n            l = loss(d...)\n        end\n        update!(opt, Flux.params(nn), gs)\n    end\n    if epoch % show_every == 0\n        println(\"Epoch \" * string(epoch))\n        @show avg_loss(data)\n    end\nend","category":"page"},{"location":"tutorials/multi/#Laplace-Approximation","page":"MLP Multi-Label Classifier","title":"Laplace Approximation","text":"","category":"section"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"la = Laplace(nn; likelihood=:classification, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"_labels = sort(unique(y))\nplt_list = []\nfor target in _labels\n    plt = plot(la, X, y; target=target, clim=(0,1))\n    push!(plt_list, plt)\nend\nplot(plt_list...)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"(Image: )","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"_labels = sort(unique(y))\nplt_list = []\nfor target in _labels\n    plt = plot(la, X, y; target=target, clim=(0,1), link_approx=:plugin)\n    push!(plt_list, plt)\nend\nplot(plt_list...)","category":"page"},{"location":"tutorials/multi/","page":"MLP Multi-Label Classifier","title":"MLP Multi-Label Classifier","text":"(Image: )","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"reference/#All-functions-and-types","page":"Reference","title":"All functions and types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#Exported-functions","page":"Reference","title":"Exported functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [\n    LaplaceRedux,\n    LaplaceRedux.Curvature\n]\nPrivate = false","category":"page"},{"location":"reference/#LaplaceRedux.Laplace-Tuple{AbstractArray}","page":"Reference","title":"LaplaceRedux.Laplace","text":"(la::Laplace)(X::AbstractArray; kwrgs...)\n\nCalling a model with Laplace Approximation on an array of inputs is equivalent to explicitly calling the predict function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Laplace-Tuple{Any}","page":"Reference","title":"LaplaceRedux.Laplace","text":"Laplace(model::Any; loss_fun::Union{Symbol, Function}, kwargs...)\n\nWrapper function to prepare Laplace approximation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.fit!-Tuple{Laplace, Any}","page":"Reference","title":"LaplaceRedux.fit!","text":"fit!(la::Laplace,data)\n\nFits the Laplace approximation for a data set.\n\nExamples\n\nusing Flux, LaplaceRedux\nx, y = LaplaceRedux.Data.toy_data_linear()\ndata = zip(x,y)\nnn = Chain(Dense(2,1))\nla = Laplace(nn)\nfit!(la, data)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.predict-Tuple{Laplace, AbstractArray}","page":"Reference","title":"LaplaceRedux.predict","text":"predict(la::Laplace, X::AbstractArray; link_approx=:probit)\n\nComputes predictions from Bayesian neural network.\n\nExamples\n\nusing Flux, LaplaceRedux\nx, y = toy_data_linear()\ndata = zip(x,y)\nnn = Chain(Dense(2,1))\nla = Laplace(nn)\nfit!(la, data)\npredict(la, hcat(x...))\n\n\n\n\n\n","category":"method"},{"location":"reference/#Internal-functions","page":"Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [\n    LaplaceRedux,\n    LaplaceRedux.Curvature\n]\nPublic = false","category":"page"},{"location":"reference/#LaplaceRedux.get_loss_fun-Tuple{Symbol, Flux.Chain}","page":"Reference","title":"LaplaceRedux.get_loss_fun","text":"get_loss_fun(likelihood::Symbol)\n\nHelper function to choose loss function based on specified model likelihood.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.get_params-Tuple{Laplace}","page":"Reference","title":"LaplaceRedux.get_params","text":"get_params(la::Laplace)\n\nRetrieves the desired (sub)set of model parameters and stores them in a list.\n\nExamples\n\nusing Flux, LaplaceRedux\nnn = Chain(Dense(2,1))\nla = Laplace(nn)\nLaplaceRedux.get_params(la)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.glm_predictive_distribution-Tuple{Laplace, AbstractArray}","page":"Reference","title":"LaplaceRedux.glm_predictive_distribution","text":"glm_predictive_distribution(la::Laplace, X::AbstractArray)\n\nComputes the linearized GLM predictive.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.hessian_approximation-Tuple{Laplace, Any}","page":"Reference","title":"LaplaceRedux.hessian_approximation","text":"hessian_approximation(la::Laplace, d)\n\nComputes the local Hessian approximation at a single data d.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.optimize_prior_precision-Tuple{Laplace}","page":"Reference","title":"LaplaceRedux.optimize_prior_precision","text":"optimize_prior_precision(la::Laplace; n_steps=100, lr=1e-1, init_prior_prec=1.)\n\nOptimize the prior precision post-hoc through empirical Bayes (marginal log-likelihood maximization).\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.outdim-Tuple{Flux.Chain}","page":"Reference","title":"LaplaceRedux.outdim","text":"outdim(model::Chain)\n\nHelper function to determine the output dimension of a Flux.Chain\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.outdim-Tuple{Laplace}","page":"Reference","title":"LaplaceRedux.outdim","text":"outdim(la::Laplace)\n\nHelper function to determine the output dimension of a Flux.Chain with Laplace approximation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.predictive_variance-Tuple{Laplace, Any}","page":"Reference","title":"LaplaceRedux.predictive_variance","text":"predictive_variance(la::Laplace,ùêâ)\n\nCompute the linearized GLM predictive variance as ùêâ‚ÇôŒ£ùêâ‚Çô' where ùêâ=‚àáf(x;Œ∏)|Œ∏ÃÇ is the Jacobian evaluated at the MAP estimate and Œ£ = H‚Åª¬π.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.CurvatureInterface","page":"Reference","title":"LaplaceRedux.Curvature.CurvatureInterface","text":"Basetype for any curvature interface.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Curvature.EmpiricalFisher","page":"Reference","title":"LaplaceRedux.Curvature.EmpiricalFisher","text":"Constructor for Empirical Fisher.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LaplaceRedux.Curvature.full-Tuple{LaplaceRedux.Curvature.EmpiricalFisher, Tuple}","page":"Reference","title":"LaplaceRedux.Curvature.full","text":"full(curvature::EmpiricalFisher, d::Union{Tuple,NamedTuple})\n\nCompute the full empirical Fisher.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.gradients-Tuple{LaplaceRedux.Curvature.CurvatureInterface, AbstractArray, Union{Number, AbstractArray}}","page":"Reference","title":"LaplaceRedux.Curvature.gradients","text":"gradients(curvature::CurvatureInterface, X::AbstractArray, y::Number)\n\nCompute the gradients with respect to the loss function: ‚àá‚Ñì(f(x;Œ∏),y) where f: ‚Ñù·¥∞ ‚Ü¶ ‚Ñù·¥∑.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LaplaceRedux.Curvature.jacobians-Tuple{LaplaceRedux.Curvature.CurvatureInterface, AbstractArray}","page":"Reference","title":"LaplaceRedux.Curvature.jacobians","text":"jacobians(curvature::CurvatureInterface, X::AbstractArray)\n\nComputes the Jacobian ‚àáf(x;Œ∏) where f: ‚Ñù·¥∞ ‚Ü¶ ‚Ñù·¥∑.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LaplaceRedux","category":"page"},{"location":"#LaplaceRedux","page":"Home","title":"LaplaceRedux","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LaplaceRedux.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"LaplaceRedux.jl (formerly BayesLaplace.jl) is a small package that can be used for effortless Bayesian Deep Learning and Logistic Regression trough Laplace Approximation. It is inspired by this Python library and its companion paper.","category":"page"},{"location":"#News","page":"Home","title":"News üì£","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JuliaCon 2022: This project was presented at JuliaCon 2022 in July 2022. See here for details.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This first minor version of the package has been registered and can be installed as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"LaplaceRedux.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"The development version can be installed like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/pat-alt/LaplaceRedux.jl\")","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Laplace approximation can be used post-hoc for any trained neural network. This library should be compatible with any pre-trained Flux.jl model. Let nn be one such model trained on dataset data. Then implementing Laplace approximation is easy as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"la = Laplace(nn)\nfit!(la, data)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Calling predict(nn,X) for some features X will produce posterior predictions. The plot below has been lifted from the documentation, which provides more detail. It shows the resulting posterior predictive surface for the plugin estimator (left) and the Laplace approximation (right) for a toy data set.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#Limitations","page":"Home","title":"Limitations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This library currently offers native support only for models composed and trained in Flux. It is also limited to binary classification problems and still lacks support for hyperparameter tuning. Finally, it also requires additional testing.","category":"page"}]
}

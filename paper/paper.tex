
% JuliaCon proceedings template
\documentclass{juliacon}
\setcounter{page}{1}

\begin{document}

\input{header}

\maketitle

\begin{abstract}

Treating deep neural networks probabilistically comes with numerous advantages including improved robustness and greater interpretability. These factors are key to building artificial intelligence (AI) that is trustworthy. A drawback commonly associated with existing Bayesian methods is that they increase computational costs. Recent work has shown that Bayesian deep learning can be effortless through Laplace approximation. We propose a small Julia package, `LaplaceRedux.jl` that implements this new approach for deep neural network trained in `Flux.jl`.

\end{abstract}

\hypertarget{sec-intro}{%
\section{Background}\label{sec-intro}}

Over the past decade Deep Learning (DL) has arguably been one of the
dominating subdisciplines of Artificial Intelligence. Despite the
tremendous success of deep neural networks, practitioners and
researchers have also pointed to a vast number of pitfalls that have so
far inhibited the use of DL in safety-critical applications. Among other
things these pitfalls include a lack adversarial robustness
\cite{goodfellow2014explaining} and an inherent opaqueness of deep
neural networks, often described as the Black-Box problem. The number of
parameters relative to the size of the available data is generally huge:

\begin{quote}
{[}\ldots{]} deep neural networks are typically very underspecified by
the available data, and {[}\ldots{]} parameters {[}therefore{]}
correspond to a diverse variety of compelling explanations for the data.
\cite{wilson2020case}
\end{quote}

A scenario like this very much calls for treating predictions from deep
learning models probabilistically \cite{wilson2020case}. It is therefore
not surprising that interest in Bayesian Deep Learning has grown in
recent years as researchers have tackled the problem from a wide range
of angles including: MCMC (see
\href{https://turing.ml/dev/tutorials/03-bayesian-neural-network/}{\texttt{Turing}}),
Variational Inference \cite{blundell2015weight}, Monte Carlo Dropout
\cite{gal2016dropout} and Deep Ensembles
\cite{lakshminarayanan2016simple}. Laplace Redux
(\cite{immer2020improving},\cite{daxberger2021laplace}) is one of the
most recent and promising approaches to Bayesian neural networks (BNN).

\hypertarget{sec-body}{%
\section{Laplace Approximation for Deep Learning}\label{sec-body}}

Let \(\mathcal{D}=\{x,y\}_{n=1}^N\) denote our feature-label pairs and
let \(f(x;\theta)=y\) denote some deep neural network specified by its
parameters \(\theta\). We are interested in eventually estimating the
posterior predictive distribution given by the following Bayesian model
average (BMA):

\begin{equation}\protect\hypertarget{eq-bma}{}{
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
}\label{eq-bma}\end{equation}

To do so we first need to compute the weight posterior
\(p(\theta|\mathcal{D})\). Laplace Approximation (LA) relies on the fact
that the second-order Taylor expansion of this posteriour amounts to a
multivariate Gaussian centered around the maximum a posteriori (MAP)
estimate \(\hat{\theta}=\arg\max_{\theta}p(\theta|\mathcal{D})\) with
covariance equal to the inverse Hessian of our loss function evaluated
at the mode \(\hat{\Sigma}=(\mathbf{H}(\mathbf{\hat{\theta}}))^{-1}\).
In other words, we can train our deep neural network in the usual way by
minimizing the negative log likelihood
\(\ell(\mathbf{w})=-\log p(y|x,\mathcal{D})\). To obtain Gaussian LA
weight posterior we then only need to compute the Hessian evaluated at
the obtained MAP estimate.

Laplace Approximation itself actually dates back to the 18th century,
but despite its simplicity it has long been neglected by the deep
learning community. One reason for this may be that for large neural
networks with many parameters the exact Hessian computation is
prohibitive. On can rely on linearised approximations of the Hessian,
but those still scale quandratically in the number of parameters.
Fortunately, recent work has shown that block-diagonal factorizations
can be successfully applied in this context \cite{daxberger2021laplace}.

Another reason for why LA has been neglected in the past, is that early
attempts at using LA for deep learning actually failed: simply sampling
from the LA weight posterior to compute the exact BNN posterior
predictive distribution in Equation~\ref{eq-bma} does not work when
using approximations for the Hessian \cite{lawrence2001variational}.
Instead we rely on a linear expansion of predictive around mode as
demonstrated by Immer et al.~(2020) \cite{immer2020improving}. Formally,
we use a locally linearized version of our BNN:

\hypertarget{sec-con}{%
\section{Conclusions}\label{sec-con}}

Bleh

\hypertarget{sec-ack}{%
\section{Acknowledgements}\label{sec-ack}}

\input{bib.tex}

\end{document}

% Inspired by the International Journal of Computer Applications template

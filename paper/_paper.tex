% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, frame hidden, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, breakable, sharp corners, enhanced]}{\end{tcolorbox}}\fi

\hypertarget{sec-intro}{%
\section{Background}\label{sec-intro}}

Over the past decade Deep Learning (DL) has arguably been one of the
dominating subdisciplines of Artificial Intelligence. Despite the
tremendous success of deep neural networks, practitioners and
researchers have also pointed to a vast number of pitfalls that have so
far inhibited the use of DL in safety-critical applications. Among other
things these pitfalls include a lack adversarial robustness
\cite{goodfellow2014explaining} and an inherent opaqueness of deep
neural networks, often described as the Black-Box problem. The number of
parameters relative to the size of the available data is generally huge:

\begin{quote}
{[}\ldots{]} deep neural networks are typically very underspecified by
the available data, and {[}\ldots{]} parameters {[}therefore{]}
correspond to a diverse variety of compelling explanations for the data.
\cite{wilson2020case}
\end{quote}

A scenario like this very much calls for treating predictions from deep
learning models probabilistically \cite{wilson2020case}. It is therefore
not surprising that interest in Bayesian Deep Learning has grown in
recent years as researchers have tackled the problem from a wide range
of angles including: MCMC (see
\href{https://turing.ml/dev/tutorials/03-bayesian-neural-network/}{\texttt{Turing}}),
Variational Inference \cite{blundell2015weight}, Monte Carlo Dropout
\cite{gal2016dropout} and Deep Ensembles
\cite{lakshminarayanan2016simple}. Laplace Redux
(\cite{immer2020improving},\cite{daxberger2021laplace}) is one of the
most recent and promising approaches to Bayesian neural networks (BNN).

\hypertarget{sec-body}{%
\section{Laplace Approximation for Deep Learning}\label{sec-body}}

Let \(\mathcal{D}=\{x,y\}_{n=1}^N\) denote our feature-label pairs and
let \(f(x;\theta)=y\) denote some deep neural network specified by its
parameters \(\theta\). We are interested in eventually estimating the
posterior predictive distribution given by the following Bayesian model
average (BMA):

\begin{equation}\protect\hypertarget{eq-bma}{}{
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
}\label{eq-bma}\end{equation}

To do so we first need to compute the weight posterior
\(p(\theta|\mathcal{D})\). Laplace Approximation (LA) relies on the fact
that the second-order Taylor expansion of this posteriour amounts to a
multivariate Gaussian centered around the maximum a posteriori (MAP)
estimate \(\hat{\theta}=\arg\max_{\theta}p(\theta|\mathcal{D})\) with
covariance equal to the inverse Hessian of our loss function evaluated
at the mode \(\hat{\Sigma}=(\mathbf{H}(\mathbf{\hat{\theta}}))^{-1}\).
In other words, we can train our deep neural network in the usual way by
minimizing the negative log likelihood
\(\ell(\mathbf{w})=-\log p(y|x,\mathcal{D})\). To obtain Gaussian LA
weight posterior we then only need to compute the Hessian evaluated at
the obtained MAP estimate.

Laplace Approximation itself actually dates back to the 18th century,
but despite its simplicity it has long been neglected by the deep
learning community. One reason for this may be that for large neural
networks with many parameters the exact Hessian computation is
prohibitive. On can rely on linearised approximations of the Hessian,
but those still scale quandratically in the number of parameters.
Fortunately, recent work has shown that block-diagonal factorizations
can be successfully applied in this context \cite{daxberger2021laplace}.

Another reason for why LA has been neglected in the past, is that early
attempts at using LA for deep learning actually failed: simply sampling
from the LA weight posterior to compute the exact BNN posterior
predictive distribution in Equation~\ref{eq-bma} does not work when
using approximations for the Hessian \cite{lawrence2001variational}.
Instead we rely on a linear expansion of predictive around mode as
demonstrated by Immer et al.~(2020) \cite{immer2020improving}. Formally,
we use a locally linearized version of our BNN:

\hypertarget{sec-con}{%
\section{Conclusions}\label{sec-con}}

Bleh

\hypertarget{sec-ack}{%
\section{Acknowledgements}\label{sec-ack}}



\end{document}

---
format: latex
execute:
    eval: false
    echo: false
jupyter: julia-1.7
---

```{julia}
using Pkg; Pkg.activate("paper")
using LaplaceRedux, Plots, PlotThemes, Flux
theme(:wong)
default(size=(500, 375))
www_path = "paper/www"
include("paper/utils.jl")
```

# Background {#sec-intro}

Over the past decade Deep Learning (DL) has arguably been one of the dominating subdisciplines of Artificial Intelligence. Despite the tremendous success of deep neural networks, practitioners and researchers have also pointed to a vast number of pitfalls that have so far inhibited the use of DL in safety-critical applications. Among other things these pitfalls include a lack adversarial robustness \cite{goodfellow2014explaining} and an inherent opaqueness of deep neural networks, often described as the black-box problem. 

In deep learning, the number of parameters relative to the size of the available data is generally huge: 

> [...] deep neural networks are typically very underspecified by the available data, and [...] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. 
> \cite{wilson2020case}

A scenario like this very much calls for treating model predictions probabilistically \cite{wilson2020case}. It is therefore not surprising that interest in Bayesian deep learning has grown in recent years as researchers have tackled the problem from a wide range of angles including: MCMC (see [`Turing`](https://turing.ml/dev/tutorials/03-bayesian-neural-network/)), Mean Field Variational Inference \cite{blundell2015weight}, Monte Carlo Dropout \cite{gal2016dropout} and Deep Ensembles \cite{lakshminarayanan2016simple}. Laplace Redux (\cite{immer2020improving},\cite{daxberger2021laplace}) is one of the most recent and promising approaches to Bayesian neural networks (BNN). 

# Laplace Approximation for Deep Learning {#sec-body}

Let $\mathcal{D}=\{x,y\}_{n=1}^N$ denote our feature-label pairs and let $f(x;\theta)=y$ denote some deep neural network specified by its parameters $\theta$. We are interested in estimating the posterior predictive distribution given by the following Bayesian model average (BMA):

$$
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
$$ {#eq-bma}

To do so we first need to compute the weight posterior $p(\theta|\mathcal{D})$. Laplace Approximation (LA) relies on the fact that the second-order Taylor expansion of this posteriour amounts to a multivariate Gaussian $q(\theta)=\mathcal{N}(\hat\mu,\hat\Sigma)$ centered around the maximum a posteriori (MAP) estimate $\hat\mu=\hat{\theta}=\arg\max_{\theta}p(\theta|\mathcal{D})$ with covariance equal to the inverse Hessian of our loss function evaluated at the mode $\hat{\Sigma}=-(\hat{\mathcal{H}}|_{\hat{\theta}})^{-1}$. 

To apply Laplace in the context of deep learning, we can train our network in the standard way by minimizing the negative log likelihood $\ell(\theta)=-\log p(y|x,\mathcal{D})$. To obtain Gaussian LA weight posterior we then only need to compute the Hessian evaluated at the obtained MAP estimate. 

Laplace Approximation itself dates back to the 18th century, but despite its simplicity it has not been widely used or studied by the deep learning community until recently. One reason for this may be that for large neural networks with many parameters the exact Hessian computation is prohibitive. One can rely on linearized approximations of the Hessian, but those still scale quadratically in the number of parameters. Fortunately, recent work has shown that block-diagonal factorizations can be successfully applied in this context \cite{martens2015optimizing}. 

Another reason for why LA may have been neglected in the past, is that early attempts at using it for deep learning actually failed: simply sampling from the Laplace posterior to compute the exact BNN posterior predictive distribution in @eq-bma does not work when using approximations for the Hessian \cite{lawrence2001variational}. Instead we can use a linear expansion of the predictive around the mode as demonstrated by Immer et al. (2020) \cite{immer2020improving}. Formally, we locally linearize our network,

$$
f^{\hat{\theta}}_{\mbox{lin}}(x;\theta)=f(x;\hat{\theta}) + \mathcal{J}_{\theta}(\theta-\hat{\theta})
$$ {#eq-glm}
 
which turns the BNN into a Bayesian generalized linear model (GLM) where $\hat{\theta}$ corresponds to the MAP estimate as before. The corresponding GLM predictive, 

$$
p(y|x,\mathcal{D}) = \mathbb{E} \left[ p(y|f^{\hat{\theta}}_{\mbox{lin}}(x;\theta_n)) \right], \ \ \ \theta_n \sim q(\theta)
$$ {#eq-glm-predictive}

has a closed-form solution for regression problems and for classification problems can be approximated using (extended) probit approximation \cite{daxberger2021laplace}. 

Immer et al. (2020) \cite{immer2020improving} provide a much more detailed exposition of the above with a focus on theoretical underpinnings and intuition. Daxberger et el. (2021) \cite{daxberger2021laplace} introduce Laplace Redux from more of an applied perspective and present a comprehensive Pyhon implementation: [laplace](https://aleximmer.github.io/Laplace/).

# `LaplaceRedux.jl` --- a Julia implementation

The `LaplaceRedux.jl` package is intended to make this new methodological framework available to the Julia community. It is interfaced to the popular deep learning library, [`Flux.jl`](https://fluxml.ai/). 

```{julia}
#| echo: false
#| 
# Data:
xs, y = LaplaceRedux.Data.toy_data_non_linear(200)
X = hcat(xs...); # bring into tabular format
data = zip(xs,y)

# Build MLP:
n_hidden = 32
D = size(X)[1]
nn = Chain(
    Dense(
      randn(n_hidden,D)./10,
      zeros(n_hidden), σ
    ),
    Dense(
      randn(1,n_hidden)./10,
      zeros(1)
    )
)  

# Loss function:
λ = 0.01
sqnorm(x) = sum(abs2, x)
weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))
loss(x, y; λ=λ) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization(λ)

# Training:
using Flux.Optimise: update!, ADAM
opt = ADAM()
epochs = 200
for epoch = 1:epochs
  for d in data
    gs = gradient(Flux.params(nn)) do
      l = loss(d...)
    end
    update!(opt, Flux.params(nn), gs)
  end
end
```

Using just a few lines of code the package enables users to compute apply Laplace Redux to their pre-trained neural networks. A basic usage example is shown in listing \ref{lst:laplace}: the `Laplace` function simply wraps the Flux neural network `nn`. Here we have also provided two of the optional key arguments that determine the prior precision $\lambda$ and the subset of network layers to be used. The returned instance can then be trained on data using the generic `fit!` method. Calling the generic `predict` method on the fitted instance will generate GLM predictions according to @eq-glm-predictive.

```{julia}
la = Laplace(
  nn;
  λ=λ, subset_of_weights=:last_layer
)
fit!(la, data)
predict(la, X)
```

\begin{lstlisting}[language=Julia, escapechar=@, numbers=left, label={lst:laplace}, caption={}]
la = Laplace(
  nn;
  @$\lambda=\lambda$@, subset_of_weights=:last_layer
)
fit!(la, data)
\end{lstlisting}

```{julia}
#| echo: false
p_plugin = plot_contour(X',y,la;title="Plugin",type=:plugin);
p_laplace = plot_contour(X',y,la;title="Laplace")
# Plot the posterior distribution with a contour plot.
plt = plot(p_plugin, p_laplace, layout=(1,2), size=(500,220))
savefig(plt, joinpath(www_path,"posterior_predictive_mlp.png"))
```

@fig-pred-mlp shows an example involving a synthetic data set consisting of two classes. Contours indicate the predicted probabilities using the plugin estimator (left) and Laplace approximation (right). Relying solely on the MAP estimate, the plugin estimator produces overly confident predictions. Conversely, the GLM predictions account for predictive uncertainty as captured by the Laplace posterior.

![Posterior predictive distribution of a simple neural network in the 2D feature space using the plugin estimator (left) and Laplace approximation (right).](www/posterior_predictive_mlp.png){#fig-pred-mlp width="20pc" height="8.5pc"}

The package is still in its infancy and its functionality limited at the time of writing. For example, it currently lacks support for regression and multi-class problems. It also still works with full Hessian approximations, as opposed to the less expensive (block-) diagonal variants. That being said, choices regarding the package architecture were made with these future development opportunities in mind. This should hopefully make the package attractive to other Julia developers interested in the topic.

# Conclusions {#sec-con}

Laplace Redux is arguably one of the most exciting and promising recent developments in Bayesian deep learning. The goal of this project is to bring this framework to the attention of the Julia machine learning community. The package `LaplaceRedux.jl` offers a useful starting ground for a full-fledged implementation in pure Julia. Future developments are planned and contributions are very much welcome.

# Acknowledgements {#sec-ack}

I am grateful to my PhD supervisors Cynthia C. S. Liem and Arie van Deursen for being so supportive of me working on open-source developments. I am also grateful to the Julia community for being so kind, welcoming and helpful. 